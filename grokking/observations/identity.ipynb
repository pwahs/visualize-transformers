{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obervations on training identity mapping:\n",
    "\n",
    "Vocabulary: 0, 1, 2, =\n",
    "\n",
    "Output Vocab: 0, 1, 2\n",
    "\n",
    "Task: Complete 0=, 1=, 2=\n",
    "\n",
    "* Higher accuracy depends on long vectors, not just correct direction\n",
    "* Un-embedding vectors are not strictly along \"right\" vectors, but roughly 120 degrees separate from each other (+-10 degrees)\n",
    "    * With higher vocabulary, the vectors are sometimes even on the other side of a different vector, but that goes along with a large magnitude difference to still predict the correct token\n",
    "* size of parameters are limited by training that is meant to guarantee numeric stability. There would be better solutions with arbitrarily large weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open questions:\n",
    "\n",
    "* Are 120 degrees optimal? Given any other weights, or\n",
    "    * Answer: No, with vocab 0...6, there sometimes was a >180 degree gap. Varying magnitude in combination with varying direction yields low loss \n",
    "* In combination with those other weights?\n",
    "* What happens for larger vocabulary?\n",
    "    * Same angles?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
