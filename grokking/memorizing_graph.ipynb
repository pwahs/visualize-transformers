{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpreting Identity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBvIU6zdIZpa"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ssx2pnTOIZpa"
      },
      "outputs": [],
      "source": [
        "TRAIN_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Jfz-lAWIZpb",
        "outputId": "48df1c71-5bc3-4a14-9a34-1af5c16786a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (25.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_219706/80073063.py:12: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"load_ext autoreload\")\n",
            "/tmp/ipykernel_219706/80073063.py:13: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"autoreload 2\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipympl in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (0.9.6)\n",
            "Requirement already satisfied: ipython<9 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (8.31.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (8.1.5)\n",
            "Requirement already satisfied: matplotlib<4,>=3.5.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (3.10.0)\n",
            "Requirement already satisfied: numpy in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (2.2.1)\n",
            "Requirement already satisfied: pillow in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (11.1.0)\n",
            "Requirement already satisfied: traitlets<6 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (5.14.3)\n",
            "Requirement already satisfied: decorator in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (3.0.48)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (2.19.1)\n",
            "Requirement already satisfied: stack_data in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (0.6.3)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.2)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (2.9.0.post0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython<9->ipympl) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib<4,>=3.5.0->ipympl) (1.17.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (2.1.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (3.0.0)\n",
            "Requirement already satisfied: pure_eval in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (0.2.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scipy in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (1.15.0)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from scipy) (2.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: manim in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (0.18.1)\n",
            "Requirement already satisfied: Pillow>=9.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (11.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (2.19.1)\n",
            "Requirement already satisfied: click>=8.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (8.1.8)\n",
            "Requirement already satisfied: cloup>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.0.5)\n",
            "Requirement already satisfied: decorator>=4.3.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (5.1.1)\n",
            "Requirement already satisfied: isosurfaces>=0.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.1.2)\n",
            "Requirement already satisfied: manimpango<1.0.0,>=0.5.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.6.0)\n",
            "Requirement already satisfied: mapbox-earcut>=1.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.0.3)\n",
            "Requirement already satisfied: moderngl<6.0.0,>=5.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (5.12.0)\n",
            "Requirement already satisfied: moderngl-window>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.0.3)\n",
            "Requirement already satisfied: networkx>=2.6 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.26 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (2.2.1)\n",
            "Requirement already satisfied: pycairo<2.0.0,>=1.13 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.27.0)\n",
            "Requirement already satisfied: pydub>=0.20.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.25.1)\n",
            "Requirement already satisfied: rich>=12.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (13.9.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.15.0)\n",
            "Requirement already satisfied: screeninfo>=0.7 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.8.1)\n",
            "Requirement already satisfied: skia-pathops>=0.7.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.8.0.post2)\n",
            "Requirement already satisfied: srt>=3.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.5.3)\n",
            "Requirement already satisfied: svgelements>=1.8.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.9.6)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (4.12.2)\n",
            "Requirement already satisfied: watchdog>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (6.0.0)\n",
            "Requirement already satisfied: glcontext>=3.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from moderngl<6.0.0,>=5.0.0->manim) (3.0.0)\n",
            "Requirement already satisfied: pyglet>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from moderngl-window>=2.0.0->manim) (2.0.20)\n",
            "Requirement already satisfied: pyglm<3,>=2.7.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from moderngl-window>=2.0.0->manim) (2.7.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from rich>=12.0.0->manim) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->manim) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torch in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "/bin/bash: Zeile 1: 2: Datei oder Verzeichnis nicht gefunden\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: einops in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (0.8.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformer_lens in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (2.11.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (1.2.1)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (3.2.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.8.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.2.36)\n",
            "Requirement already satisfied: numpy>=1.26 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.2.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.10 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.5.1)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.66.5)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.47.1)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.4.1)\n",
            "Requirement already satisfied: typing-extensions in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.12.2)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.1.1)\n",
            "Requirement already satisfied: pyyaml in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.5.1)\n",
            "Requirement already satisfied: filelock in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.11.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (2.19.1)\n",
            "Requirement already satisfied: networkx in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (0.21.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.10.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (5.29.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.10.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.10->transformer_lens) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEVELOPMENT_MODE = True\n",
        "# Upgrade pip\n",
        "%pip install --upgrade pip\n",
        "\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "# Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "ipython.magic(\"load_ext autoreload\")\n",
        "ipython.magic(\"autoreload 2\")\n",
        "ipython.run_line_magic(\"pip\", \"install ipympl\")\n",
        "ipython.run_line_magic(\"pip\", \"install scipy\")\n",
        "ipython.run_line_magic(\"pip\", \"install manim\")\n",
        "ipython.run_line_magic(\"pip\", \"install torch\")\n",
        "ipython.run_line_magic(\"pip\", \"install numpy<2\")\n",
        "ipython.run_line_magic(\"pip\", \"install einops\")\n",
        "ipython.run_line_magic(\"pip\", \"install transformer_lens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i8GCNEdpIZpc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import copy\n",
        "\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, ActivationCache\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "# Define the location to save the model, using a relative path\n",
        "PTH_LOCATION = \"workspace/_scratch/identity.pth\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M2Nzu7EIZpe"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOckI2-GIZpe"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_2OXlPG-IZpe"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = 3\n",
        "OUTPUT_DIM = 3\n",
        "frac_train = 1\n",
        "\n",
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1e-2\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "num_epochs = 10000\n",
        "checkpoint_every = 500\n",
        "\n",
        "DATA_SEED = 599"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT7xY9WsIZpe"
      },
      "source": [
        "## Define Task\n",
        "* Define random function\n",
        "* Define the dataset & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOJMbA26IZpe"
      },
      "source": [
        "Input format:\n",
        "|a|b|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I8Af2zLZIZpf",
        "outputId": "5075c607-e865-4b8c-90df-d2685d17ebb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2]], device='cuda:0')\n",
            "tensor([2, 2, 1, 0, 2, 2, 0, 0, 2], device='cuda:0')\n",
            "torch.Size([9, 2])\n",
            "tensor([], device='cuda:0', size=(0, 2), dtype=torch.int64)\n",
            "tensor([], device='cuda:0', dtype=torch.int64)\n",
            "torch.Size([0, 2])\n"
          ]
        }
      ],
      "source": [
        "def get_training_data(input_dim = INPUT_DIM, output_dim = OUTPUT_DIM, data_seed = DATA_SEED):\n",
        "    torch.manual_seed(data_seed)\n",
        "    a_vector = torch.arange(input_dim)\n",
        "    dataset = torch.cartesian_prod(a_vector, a_vector).to(device)\n",
        "\n",
        "    labels = torch.randint(0, output_dim, (dataset.shape[0],), device=device)\n",
        "    train_data = dataset\n",
        "    train_labels = labels\n",
        "    # For now no test data\n",
        "    test_data = dataset[0:0]\n",
        "    test_labels = labels[0:0]\n",
        "    print(train_data)\n",
        "    print(train_labels)\n",
        "    print(train_data.shape)\n",
        "    print(test_data[:5])\n",
        "    print(test_labels[:5])\n",
        "    print(test_data.shape)\n",
        "    return train_data, train_labels, test_data, test_labels\n",
        "\n",
        "train_data, train_labels, test_data, test_labels = get_training_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_83i1bUkIZpf"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UL7gVZ9WIZpf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_seeded_model(seed = 999, input_dim = INPUT_DIM, output_dim = OUTPUT_DIM):\n",
        "    cfg = HookedTransformerConfig(\n",
        "        n_layers = 1,\n",
        "        n_heads = 1,\n",
        "        d_model = 2,\n",
        "        d_head = 2,\n",
        "        d_mlp = None,\n",
        "        attn_only=True,\n",
        "        act_fn = \"relu\",\n",
        "        normalization_type=None,\n",
        "        d_vocab=input_dim,\n",
        "        d_vocab_out=output_dim,\n",
        "        n_ctx=2,\n",
        "        init_weights=True,\n",
        "        device=device,\n",
        "        seed = seed,\n",
        "    )\n",
        "    model = HookedTransformer(cfg)\n",
        "    # Biases are enabled by default\n",
        "    # for name, param in model.named_parameters():\n",
        "    #     if \"b_\" in name:\n",
        "    #         param.requires_grad = False\n",
        "    return model\n",
        "\n",
        "model = get_seeded_model(seed = 993)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lctx1jAtIZpg"
      },
      "source": [
        "## Define Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2]], device='cuda:0')\n",
            "tensor([[[-0.0232, -0.3831,  0.0734],\n",
            "         [ 0.3973,  0.0077,  0.2572]],\n",
            "\n",
            "        [[-0.0232, -0.3831,  0.0734],\n",
            "         [ 0.1674, -0.1676,  0.1479]],\n",
            "\n",
            "        [[-0.0232, -0.3831,  0.0734],\n",
            "         [ 0.1926,  0.8592, -0.0728]],\n",
            "\n",
            "        [[-0.3110, -0.5506, -0.0756],\n",
            "         [ 0.3419,  0.0151,  0.2194]],\n",
            "\n",
            "        [[-0.3110, -0.5506, -0.0756],\n",
            "         [ 0.1117, -0.1603,  0.1098]],\n",
            "\n",
            "        [[-0.3110, -0.5506, -0.0756],\n",
            "         [ 0.1047,  0.8789, -0.1347]],\n",
            "\n",
            "        [[-0.0824,  0.2363, -0.1083],\n",
            "         [ 0.4788, -0.1707,  0.3516]],\n",
            "\n",
            "        [[-0.0824,  0.2363, -0.1083],\n",
            "         [ 0.2537, -0.3527,  0.2469]],\n",
            "\n",
            "        [[-0.0824,  0.2363, -0.1083],\n",
            "         [ 0.3217,  0.6311,  0.0640]]], device='cuda:0',\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(train_data)\n",
        "print(model(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dJORAU_PIZpg",
        "outputId": "1c2b9f25-056e-4e0d-defc-0bd11586136d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.0589, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "tensor(nan, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "Uniform loss:\n",
            "2.1972245773362196\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)\n",
        "def loss_fn(logits, labels):\n",
        "    if len(logits.shape)==3:\n",
        "        logits = logits[:, -1]\n",
        "    logits = logits.to(torch.float64)\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
        "    return -correct_log_probs.mean()\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(train_loss)\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(test_loss)\n",
        "print(\"Uniform loss:\")\n",
        "print(np.log(INPUT_DIM * INPUT_DIM))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9Q4bIcIZpg"
      },
      "source": [
        "## Actually Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj4h2LIRIZpg"
      },
      "source": [
        "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_stats(model):\n",
        "    # Extract the p 2-dimensional tensors, vector i is vec[:, i]\n",
        "    vec = model.W_U.data\n",
        "\n",
        "    # Function to compute the angle between two vectors\n",
        "    def compute_angle(v1, v2):\n",
        "        cos_theta = torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))\n",
        "        angle = torch.acos(cos_theta) * (180.0 / np.pi)\n",
        "        return angle\n",
        "\n",
        "    # Compute pairwise angles\n",
        "    # for i in range(vec.shape[1]):\n",
        "    #     for j in range(i+1, vec.shape[1]):\n",
        "    #         angle = compute_angle(vec[:, i], vec[:, j])\n",
        "    #         print(f\"Angle between {i} and {j}: {angle.item():.2f}Â°\")\n",
        "    #     print(f\"Norm of vector {i}: {torch.norm(vec[:, i]):.2f}\")\n",
        "\n",
        "print_stats(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c477d3a6463646deb7cf1de99bb0199a"
          ]
        },
        "id": "UsZLeCMeIZph",
        "outputId": "133f3502-ca28-490c-c030-f60b751dcd7b"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_data, train_labels, test_data, test_labels, num_epochs = num_epochs, loss_target = None):\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=lr, weight_decay=wd, betas=betas\n",
        "    )\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    model_checkpoints = []\n",
        "    checkpoint_epochs = []\n",
        "    if TRAIN_MODEL:\n",
        "        for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "            train_logits = model(train_data)\n",
        "            train_loss = loss_fn(train_logits, train_labels)\n",
        "            train_loss.backward()\n",
        "            train_losses.append(train_loss.item())\n",
        "            if loss_target is not None and train_loss.item() < loss_target:\n",
        "                print(f\"Loss target {loss_target} reached with loss {train_loss.item()} at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                test_logits = model(test_data)\n",
        "                test_loss = loss_fn(test_logits, test_labels)\n",
        "                test_losses.append(test_loss.item())\n",
        "\n",
        "            if ((epoch+1)%checkpoint_every)==0:\n",
        "                checkpoint_epochs.append(epoch)\n",
        "                model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "                print_stats(model)\n",
        "                print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")\n",
        "    if TRAIN_MODEL:\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\":model.state_dict(),\n",
        "                \"config\": model.cfg,\n",
        "                \"checkpoints\": model_checkpoints,\n",
        "                \"checkpoint_epochs\": checkpoint_epochs,\n",
        "                \"test_losses\": test_losses,\n",
        "                \"train_losses\": train_losses,\n",
        "            },\n",
        "            PTH_LOCATION)\n",
        "\n",
        "train_model(\n",
        "    model, train_data, train_labels, test_data, test_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EzRKi7J7IZph"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_219706/999483974.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  cached_data = torch.load(PTH_LOCATION)\n"
          ]
        }
      ],
      "source": [
        "if not TRAIN_MODEL:\n",
        "    cached_data = torch.load(PTH_LOCATION)\n",
        "    model.load_state_dict(cached_data['model'])\n",
        "    model_checkpoints = cached_data[\"checkpoints\"]\n",
        "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
        "    test_losses = cached_data['test_losses']\n",
        "    train_losses = cached_data['train_losses']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transposed Input:\n",
            " tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
            "        [0, 1, 2, 0, 1, 2, 0, 1, 2]], device='cuda:0')\n",
            "Labels:  tensor([2, 2, 1, 0, 2, 2, 0, 0, 2], device='cuda:0')\n",
            "Logits of last token:\n",
            " tensor([[ -6.4230,  -1.9307,  10.7630],\n",
            "        [ -4.4916,  -8.5375,  12.1585],\n",
            "        [ -1.7896,   9.3899,  -3.5204],\n",
            "        [  6.9543,  -4.1992,  -7.7939],\n",
            "        [ -4.3770,  -8.4497,  11.9299],\n",
            "        [ -1.8799, -12.0370,  10.5191],\n",
            "        [ 10.5641, -12.7535,  -7.6477],\n",
            "        [  7.4779,  -9.8636,  -4.9022],\n",
            "        [ -2.8386, -14.7257,  13.6992]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Unembed:\n",
            " tensor([[-1.0812,  2.0407,  0.2946],\n",
            "        [ 0.7589,  0.2098, -1.2724]], device='cuda:0')\n",
            "Last layer before unembed:\n",
            " tensor([[ -0.0272,  -8.3789],\n",
            "        [ -3.0794, -10.1822],\n",
            "        [  4.2643,   3.8404],\n",
            "        [ -2.5773,   5.6153],\n",
            "        [ -3.0554,  -9.9970],\n",
            "        [ -4.8838,  -9.3115],\n",
            "        [ -6.6602,   4.5551],\n",
            "        [ -5.0603,   2.7677],\n",
            "        [ -5.9198, -12.0507]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "def create_cache(model):\n",
        "    input = train_data\n",
        "    print(\"Transposed Input:\\n\", input.transpose(0, 1))\n",
        "    logits, cache = model.run_with_cache(input)\n",
        "    print(\"Labels: \", train_labels)\n",
        "    print(\"Logits of last token:\\n\", logits[:, -1, :])\n",
        "    print(\"Unembed:\\n\", model.W_U.data)\n",
        "    print(\"Last layer before unembed:\\n\", cache.cache_dict[\"blocks.0.hook_resid_post\"][:, -1, :])\n",
        "    return cache\n",
        "\n",
        "cache = create_cache(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hook_embed': tensor([[[-1.1449,  0.6601],\n",
              "          [-1.1449,  0.6601]],\n",
              " \n",
              "         [[-1.1449,  0.6601],\n",
              "          [ 0.3006, -0.1175]],\n",
              " \n",
              "         [[-1.1449,  0.6601],\n",
              "          [ 1.5386, -0.9611]],\n",
              " \n",
              "         [[ 0.3006, -0.1175],\n",
              "          [-1.1449,  0.6601]],\n",
              " \n",
              "         [[ 0.3006, -0.1175],\n",
              "          [ 0.3006, -0.1175]],\n",
              " \n",
              "         [[ 0.3006, -0.1175],\n",
              "          [ 1.5386, -0.9611]],\n",
              " \n",
              "         [[ 1.5386, -0.9611],\n",
              "          [-1.1449,  0.6601]],\n",
              " \n",
              "         [[ 1.5386, -0.9611],\n",
              "          [ 0.3006, -0.1175]],\n",
              " \n",
              "         [[ 1.5386, -0.9611],\n",
              "          [ 1.5386, -0.9611]]], device='cuda:0'),\n",
              " 'hook_pos_embed': tensor([[[ 0.5616,  0.9027],\n",
              "          [-0.6252, -1.5300]],\n",
              " \n",
              "         [[ 0.5616,  0.9027],\n",
              "          [-0.6252, -1.5300]],\n",
              " \n",
              "         [[ 0.5616,  0.9027],\n",
              "          [-0.6252, -1.5300]],\n",
              " \n",
              "         [[ 0.5616,  0.9027],\n",
              "          [-0.6252, -1.5300]],\n",
              " \n",
              "         [[ 0.5616,  0.9027],\n",
              "          [-0.6252, -1.5300]],\n",
              " \n",
              "         [[ 0.5616,  0.9027],\n",
              "          [-0.6252, -1.5300]],\n",
              " \n",
              "         [[ 0.5616,  0.9027],\n",
              "          [-0.6252, -1.5300]],\n",
              " \n",
              "         [[ 0.5616,  0.9027],\n",
              "          [-0.6252, -1.5300]],\n",
              " \n",
              "         [[ 0.5616,  0.9027],\n",
              "          [-0.6252, -1.5300]]], device='cuda:0'),\n",
              " 'blocks.0.hook_resid_pre': tensor([[[-0.5833,  1.5629],\n",
              "          [-1.7701, -0.8698]],\n",
              " \n",
              "         [[-0.5833,  1.5629],\n",
              "          [-0.3246, -1.6475]],\n",
              " \n",
              "         [[-0.5833,  1.5629],\n",
              "          [ 0.9134, -2.4911]],\n",
              " \n",
              "         [[ 0.8621,  0.7852],\n",
              "          [-1.7701, -0.8698]],\n",
              " \n",
              "         [[ 0.8621,  0.7852],\n",
              "          [-0.3246, -1.6475]],\n",
              " \n",
              "         [[ 0.8621,  0.7852],\n",
              "          [ 0.9134, -2.4911]],\n",
              " \n",
              "         [[ 2.1002, -0.0584],\n",
              "          [-1.7701, -0.8698]],\n",
              " \n",
              "         [[ 2.1002, -0.0584],\n",
              "          [-0.3246, -1.6475]],\n",
              " \n",
              "         [[ 2.1002, -0.0584],\n",
              "          [ 0.9134, -2.4911]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_q': tensor([[[[-2.3254,  0.4960]],\n",
              " \n",
              "          [[-3.5381,  4.8472]]],\n",
              " \n",
              " \n",
              "         [[[-2.3254,  0.4960]],\n",
              " \n",
              "          [[-0.0740,  3.0141]]],\n",
              " \n",
              " \n",
              "         [[[-2.3254,  0.4960]],\n",
              " \n",
              "          [[ 2.9873,  1.6085]]],\n",
              " \n",
              " \n",
              "         [[[ 1.1386, -1.3372]],\n",
              " \n",
              "          [[-3.5381,  4.8472]]],\n",
              " \n",
              " \n",
              "         [[[ 1.1386, -1.3372]],\n",
              " \n",
              "          [[-0.0740,  3.0141]]],\n",
              " \n",
              " \n",
              "         [[[ 1.1386, -1.3372]],\n",
              " \n",
              "          [[ 2.9873,  1.6085]]],\n",
              " \n",
              " \n",
              "         [[[ 4.1999, -2.7428]],\n",
              " \n",
              "          [[-3.5381,  4.8472]]],\n",
              " \n",
              " \n",
              "         [[[ 4.1999, -2.7428]],\n",
              " \n",
              "          [[-0.0740,  3.0141]]],\n",
              " \n",
              " \n",
              "         [[[ 4.1999, -2.7428]],\n",
              " \n",
              "          [[ 2.9873,  1.6085]]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_k': tensor([[[[ 2.4980, -3.3361]],\n",
              " \n",
              "          [[ 4.2302, -1.2372]]],\n",
              " \n",
              " \n",
              "         [[[ 2.4980, -3.3361]],\n",
              " \n",
              "          [[-0.1080,  2.1239]]],\n",
              " \n",
              " \n",
              "         [[[ 2.4980, -3.3361]],\n",
              " \n",
              "          [[-3.9294,  5.2835]]],\n",
              " \n",
              " \n",
              "         [[[-1.8402,  0.0250]],\n",
              " \n",
              "          [[ 4.2302, -1.2372]]],\n",
              " \n",
              " \n",
              "         [[[-1.8402,  0.0250]],\n",
              " \n",
              "          [[-0.1080,  2.1239]]],\n",
              " \n",
              " \n",
              "         [[[-1.8402,  0.0250]],\n",
              " \n",
              "          [[-3.9294,  5.2835]]],\n",
              " \n",
              " \n",
              "         [[[-5.6616,  3.1846]],\n",
              " \n",
              "          [[ 4.2302, -1.2372]]],\n",
              " \n",
              " \n",
              "         [[[-5.6616,  3.1846]],\n",
              " \n",
              "          [[-0.1080,  2.1239]]],\n",
              " \n",
              " \n",
              "         [[[-5.6616,  3.1846]],\n",
              " \n",
              "          [[-3.9294,  5.2835]]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_v': tensor([[[[-3.5159,  0.5056]],\n",
              " \n",
              "          [[ 2.2176,  2.6192]]],\n",
              " \n",
              " \n",
              "         [[[-3.5159,  0.5056]],\n",
              " \n",
              "          [[ 3.8176,  0.4706]]],\n",
              " \n",
              " \n",
              "         [[[-3.5159,  0.5056]],\n",
              " \n",
              "          [[ 5.5954, -1.3495]]],\n",
              " \n",
              " \n",
              "         [[[-1.9158, -1.6430]],\n",
              " \n",
              "          [[ 2.2176,  2.6192]]],\n",
              " \n",
              " \n",
              "         [[[-1.9158, -1.6430]],\n",
              " \n",
              "          [[ 3.8176,  0.4706]]],\n",
              " \n",
              " \n",
              "         [[[-1.9158, -1.6430]],\n",
              " \n",
              "          [[ 5.5954, -1.3495]]],\n",
              " \n",
              " \n",
              "         [[[-0.1380, -3.4631]],\n",
              " \n",
              "          [[ 2.2176,  2.6192]]],\n",
              " \n",
              " \n",
              "         [[[-0.1380, -3.4631]],\n",
              " \n",
              "          [[ 3.8176,  0.4706]]],\n",
              " \n",
              " \n",
              "         [[[-0.1380, -3.4631]],\n",
              " \n",
              "          [[ 5.5954, -1.3495]]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_attn_scores': tensor([[[[ -5.2775,     -inf],\n",
              "           [-17.6840, -14.8238]]],\n",
              " \n",
              " \n",
              "         [[[ -5.2775,     -inf],\n",
              "           [ -7.2409,   4.5322]]],\n",
              " \n",
              " \n",
              "         [[[ -5.2775,     -inf],\n",
              "           [  1.4822,  -2.2910]]],\n",
              " \n",
              " \n",
              "         [[[ -1.5052,     -inf],\n",
              "           [  4.6895, -14.8238]]],\n",
              " \n",
              " \n",
              "         [[[ -1.5052,     -inf],\n",
              "           [  0.1496,   4.5322]]],\n",
              " \n",
              " \n",
              "         [[[ -1.5052,     -inf],\n",
              "           [ -3.8586,  -2.2910]]],\n",
              " \n",
              " \n",
              "         [[[-22.9902,     -inf],\n",
              "           [ 25.0795, -14.8238]]],\n",
              " \n",
              " \n",
              "         [[[-22.9902,     -inf],\n",
              "           [  7.0836,   4.5322]]],\n",
              " \n",
              " \n",
              "         [[[-22.9902,     -inf],\n",
              "           [ -8.3370,  -2.2910]]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_pattern': tensor([[[[1.0000e+00, 0.0000e+00],\n",
              "           [5.4154e-02, 9.4585e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [7.7098e-06, 9.9999e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [9.7754e-01, 2.2462e-02]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.0000e+00, 3.3536e-09]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.2339e-02, 9.8766e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.7255e-01, 8.2745e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.0000e+00, 4.6796e-18]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [9.2767e-01, 7.2330e-02]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [2.3615e-03, 9.9764e-01]]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_z': tensor([[[[-3.5159,  0.5056]],\n",
              " \n",
              "          [[ 1.9071,  2.5048]]],\n",
              " \n",
              " \n",
              "         [[[-3.5159,  0.5056]],\n",
              " \n",
              "          [[ 3.8176,  0.4706]]],\n",
              " \n",
              " \n",
              "         [[[-3.5159,  0.5056]],\n",
              " \n",
              "          [[-3.3112,  0.4639]]],\n",
              " \n",
              " \n",
              "         [[[-1.9158, -1.6430]],\n",
              " \n",
              "          [[-1.9158, -1.6430]]],\n",
              " \n",
              " \n",
              "         [[[-1.9158, -1.6430]],\n",
              " \n",
              "          [[ 3.7469,  0.4445]]],\n",
              " \n",
              " \n",
              "         [[[-1.9158, -1.6430]],\n",
              " \n",
              "          [[ 4.2993, -1.4002]]],\n",
              " \n",
              " \n",
              "         [[[-0.1380, -3.4631]],\n",
              " \n",
              "          [[-0.1380, -3.4631]]],\n",
              " \n",
              " \n",
              "         [[[-0.1380, -3.4631]],\n",
              " \n",
              "          [[ 0.1481, -3.1786]]],\n",
              " \n",
              " \n",
              "         [[[-0.1380, -3.4631]],\n",
              " \n",
              "          [[ 5.5819, -1.3545]]]], device='cuda:0'),\n",
              " 'blocks.0.hook_attn_out': tensor([[[ 3.5850,  6.6975],\n",
              "          [ 1.7428, -7.5090]],\n",
              " \n",
              "         [[ 3.5850,  6.6975],\n",
              "          [-2.7548, -8.5347]],\n",
              " \n",
              "         [[ 3.5850,  6.6975],\n",
              "          [ 3.3509,  6.3315]],\n",
              " \n",
              "         [[-0.8072,  6.4851],\n",
              "          [-0.8072,  6.4851]],\n",
              " \n",
              "         [[-0.8072,  6.4851],\n",
              "          [-2.7308, -8.3495]],\n",
              " \n",
              "         [[-0.8072,  6.4851],\n",
              "          [-5.7972, -6.8204]],\n",
              " \n",
              "         [[-4.8902,  5.4249],\n",
              "          [-4.8902,  5.4249]],\n",
              " \n",
              "         [[-4.8902,  5.4249],\n",
              "          [-4.7357,  4.4152]],\n",
              " \n",
              "         [[-4.8902,  5.4249],\n",
              "          [-6.8332, -9.5596]]], device='cuda:0'),\n",
              " 'blocks.0.hook_resid_post': tensor([[[  3.0017,   8.2604],\n",
              "          [ -0.0272,  -8.3789]],\n",
              " \n",
              "         [[  3.0017,   8.2604],\n",
              "          [ -3.0794, -10.1822]],\n",
              " \n",
              "         [[  3.0017,   8.2604],\n",
              "          [  4.2643,   3.8404]],\n",
              " \n",
              "         [[  0.0549,   7.2703],\n",
              "          [ -2.5773,   5.6153]],\n",
              " \n",
              "         [[  0.0549,   7.2703],\n",
              "          [ -3.0554,  -9.9970]],\n",
              " \n",
              "         [[  0.0549,   7.2703],\n",
              "          [ -4.8838,  -9.3115]],\n",
              " \n",
              "         [[ -2.7900,   5.3665],\n",
              "          [ -6.6602,   4.5551]],\n",
              " \n",
              "         [[ -2.7900,   5.3665],\n",
              "          [ -5.0603,   2.7677]],\n",
              " \n",
              "         [[ -2.7900,   5.3665],\n",
              "          [ -5.9198, -12.0507]]], device='cuda:0')}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a list of all activations stored in the cache, especially their names\n",
        "cache.cache_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Animations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from manim import *\n",
        "\n",
        "config.media_width = \"80%\"\n",
        "config.verbosity = \"WARNING\"\n",
        "config.preview = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[tensor([-1.1449,  0.6601])(#FFFFFF)], [tensor([-0.5833,  1.5629])(#FFFFFF)], [tensor([3.0017, 8.2604])(#888888)]]\n"
          ]
        }
      ],
      "source": [
        "class VectorParams:\n",
        "    def __init__(self, values = [], color = WHITE, label = \"\"):\n",
        "        self.values = values\n",
        "        self.color = color\n",
        "        self.label = label\n",
        "    def __repr__(self) -> str:\n",
        "        return str(self.values) + \"(\" + str(self.color) + \")\"\n",
        "\n",
        "class Data:\n",
        "    def __init__(self):\n",
        "        self.vectors: list[list[VectorParams]] = [[]]\n",
        "        self.steps = 0\n",
        "        self.current_labels = set()\n",
        "\n",
        "    def add_vector(self, vector, color = WHITE, label = \"\"):\n",
        "        if label not in self.current_labels:\n",
        "            self.current_labels.add(label)\n",
        "            self.vectors[self.steps].append(VectorParams(values = vector, color = color, label = label))\n",
        "\n",
        "    def next_step(self):\n",
        "        self.steps += 1\n",
        "        self.vectors.append([])\n",
        "        self.current_labels = set()\n",
        "\n",
        "    def add_vectors_at_hook(self, c: ActivationCache, hook: str, color0 = WHITE, color1 = WHITE, input_labels = None, input_colors = None):\n",
        "        if input_labels is None:\n",
        "            input_labels = [\"\" for i in range(c.cache_dict[hook].shape[0])]\n",
        "        for i in range(c.cache_dict[hook].shape[0]):\n",
        "            self.add_vector(c.cache_dict[hook][i][0].cpu(), color = color0, label = input_labels[i][:1])\n",
        "            self.add_vector(\n",
        "                c.cache_dict[hook][i][1].cpu(), color=color1 if input_colors is None else input_colors[i], label=input_labels[i][:2]\n",
        "            )\n",
        "\n",
        "\n",
        "def compile_data_vectors(cache, input_labels=None, input_colors=None):\n",
        "    # Set default value as list of empty strings\n",
        "    vectors = Data()\n",
        "    vectors.add_vectors_at_hook(cache, \"hook_embed\", color1 = GRAY, input_labels=input_labels, input_colors=input_colors)\n",
        "    vectors.next_step()\n",
        "    vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_pre\", input_labels=input_labels, input_colors=input_colors)\n",
        "    vectors.next_step()\n",
        "    vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_post\", color0 = GRAY, input_labels=input_labels, input_colors=input_colors)\n",
        "    # vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_mid\")\n",
        "\n",
        "    print(vectors.vectors)\n",
        "    return vectors\n",
        "\n",
        "\n",
        "vectors = compile_data_vectors(cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def change_font_size(labeled_arrow: LabeledArrow, new_size):\n",
        "    # print(labeled_arrow, labeled_arrow.submobjects)\n",
        "    # print(labeled_arrow.submobjects[-1].font_size)\n",
        "    if not isinstance(labeled_arrow, LabeledArrow):\n",
        "        return\n",
        "    label = labeled_arrow.submobjects[-1]\n",
        "    box = labeled_arrow.submobjects[-2]\n",
        "    if not isinstance(box, BackgroundRectangle):\n",
        "        box = labeled_arrow.submobjects[-3]\n",
        "    coords = label.get_center()\n",
        "    # print(new_size)\n",
        "    labeled_arrow.submobjects[-1] = MathTex(\n",
        "        label.get_tex_string(), color=label.color, font_size=new_size\n",
        "    )\n",
        "    # print(\"size=\", labeled_arrow.submobjects[-1].font_size)\n",
        "    label = labeled_arrow.submobjects[-1]\n",
        "    label.move_to(coords)\n",
        "    box.width = label.width + 2 * box.buff\n",
        "    box.height = label.height + 2 * box.buff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "DOT_SCALE = 0.05\n",
        "class VisualizeTransformer(MovingCameraScene):\n",
        "    def construct(self):\n",
        "        print(\"v=\", vectors.vectors)\n",
        "        axes = Axes(\n",
        "            x_range = [-20, 20, 1],\n",
        "            y_range = [-20, 20, 1],\n",
        "            x_axis_config={\n",
        "                \"numbers_to_include\": np.arange(-18, 18.1, 3),\n",
        "                \"font_size\": 24\n",
        "            },\n",
        "            y_axis_config={\n",
        "                \"numbers_to_include\": np.arange(-18, 18.1, 3), \n",
        "                \"font_size\": 24            \n",
        "            },\n",
        "            x_length = 40,\n",
        "            y_length = 40,\n",
        "            axis_config={\"color\": GREEN}\n",
        "        )\n",
        "\n",
        "        scale = ValueTracker(2)\n",
        "\n",
        "        dots = VGroup()\n",
        "        def update_scale(self):\n",
        "            return\n",
        "            # TODO: Make the scaling nicer\n",
        "            self.stroke_width = 6 * scale.get_value()\n",
        "            change_font_size(self, 48 * scale.get_value())\n",
        "            # print(\"New font size: \", self.font_size)\n",
        "\n",
        "        # Embedding arrows\n",
        "        for i, t in enumerate(vectors.vectors[0]):\n",
        "            # print(t, t.numpy())\n",
        "            # arrow = LabeledArrow(\n",
        "            #     start=ORIGIN,\n",
        "            #     end=np.append(t.values.numpy(), 0),\n",
        "            #     buff = 0,\n",
        "            #     label = t.label,\n",
        "            #     label_frame = False,\n",
        "            #     label_color=YELLOW,\n",
        "            #     color = t.color,\n",
        "            #     max_stroke_width_to_length_ratio = 100,\n",
        "            # )\n",
        "\n",
        "            # arrow.add_updater(update_scale)\n",
        "            # arrows.add(arrow)\n",
        "            dot = LabeledDot(\n",
        "                point=np.append(t.values.numpy(), 0),\n",
        "                label=t.label,\n",
        "                color=t.color,\n",
        "                radius=DOT_SCALE * (len(vectors.vectors[0]) - i) + 0.2,\n",
        "            )\n",
        "\n",
        "            dot.add_updater(update_scale)\n",
        "            dots.add(dot)\n",
        "\n",
        "        # Transitioning the arrows through the model\n",
        "        self.add(axes, axes.get_axis_labels(), dots)\n",
        "        for step in range(1, len(vectors.vectors)):\n",
        "            new_dots = VGroup()\n",
        "            transition_arrows = VGroup()\n",
        "            for i, t in enumerate(vectors.vectors[step]):\n",
        "                # print(t, t.numpy())\n",
        "                # new_arrow = LabeledArrow(\n",
        "                #     start=ORIGIN,\n",
        "                #     end=np.append(t.values.numpy(), 0),\n",
        "                #     buff=0,\n",
        "                #     label=t.label,\n",
        "                #     label_frame=False,\n",
        "                #     label_color=YELLOW,\n",
        "                #     color=t.color,\n",
        "                #     max_stroke_width_to_length_ratio=100,\n",
        "                # )\n",
        "                # new_arrow.add_updater(update_scale)\n",
        "                # new_arrows.add(new_arrow)\n",
        "                new_dot = LabeledDot(\n",
        "                    point=np.append(t.values.numpy(), 0),\n",
        "                    label=t.label,\n",
        "                    color=t.color,\n",
        "                    radius = DOT_SCALE * (len(vectors.vectors[step]) - i) + 0.2,\n",
        "                )\n",
        "                new_dot.add_updater(update_scale)\n",
        "                new_dots.add(new_dot)\n",
        "\n",
        "                transition_arrow = Arrow(\n",
        "                    start=dots[i].arc_center,\n",
        "                    end=new_dots[i].arc_center,\n",
        "                    buff=0,\n",
        "                    color=RED,\n",
        "                )\n",
        "                transition_arrow.add_updater(update_scale)\n",
        "                transition_arrows.add(transition_arrow)\n",
        "\n",
        "            view = SurroundingRectangle(new_dots)\n",
        "            factor = max(\n",
        "                view.width / self.camera.frame_width,\n",
        "                view.height / self.camera.frame_height,\n",
        "            )\n",
        "            print(\n",
        "                factor,\n",
        "                self.camera.frame_width, view.width,\n",
        "                self.camera.frame_height, view.height,\n",
        "            )\n",
        "            self.wait()\n",
        "            self.play(FadeIn(transition_arrows), self.camera.auto_zoom(view, margin = 2), scale.animate.set_value(scale.get_value() * factor))\n",
        "            self.wait()\n",
        "            self.play(\n",
        "                ReplacementTransform(dots, new_dots)\n",
        "            )\n",
        "            self.wait()\n",
        "            self.play(FadeOut(transition_arrows))\n",
        "            self.wait()\n",
        "            dots = new_dots\n",
        "\n",
        "        # Unembedding Arrows\n",
        "        embedding_arrows = VGroup()\n",
        "        data = model.W_U.data\n",
        "        print(\"unembed: \", data)\n",
        "        for i in range(model.W_U.data.size()[1]):\n",
        "            embedding_arrow = LabeledArrow(\n",
        "                start=ORIGIN,\n",
        "                end=[data[0, i].item(), data[1, i].item(), 0],\n",
        "                label=str(i),\n",
        "                color=LIGHT_PINK,\n",
        "                buff=0,\n",
        "                max_stroke_width_to_length_ratio=100,\n",
        "            )\n",
        "            embedding_arrows.add(embedding_arrow)\n",
        "        self.play(FadeIn(embedding_arrows))\n",
        "        self.wait()\n",
        "\n",
        "# v = VisualizeTransformer()\n",
        "# v.construct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n",
            "env: TORCH_USE_CUDA_DSA=1\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "%env TORCH_USE_CUDA_DSA=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [0, 3],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [1, 3],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2],\n",
            "        [2, 3],\n",
            "        [3, 0],\n",
            "        [3, 1],\n",
            "        [3, 2],\n",
            "        [3, 3]], device='cuda:0')\n",
            "tensor([0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1], device='cuda:0')\n",
            "torch.Size([16, 2])\n",
            "tensor([], device='cuda:0', size=(0, 2), dtype=torch.int64)\n",
            "tensor([], device='cuda:0', dtype=torch.int64)\n",
            "torch.Size([0, 2])\n",
            "Transposed Input:\n",
            " tensor([[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3],\n",
            "        [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]], device='cuda:0')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [14,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [15,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [30,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [31,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [46,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [47,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [48,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [49,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [52,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [53,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [56,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [57,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [60,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [61,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [62,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n",
            "../aten/src/ATen/native/cuda/IndexKernel.cu:93: operator(): block: [0,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && \"index out of bounds\"` failed.\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 0 transpose_mat2 0 m 2 n 32 k 2 mat1_ld 2 mat2_ld 2 result_ld 2 abcType 0 computeType 68 scaleType 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmanim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-qh Video\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mimport os\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mos.environ[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mCUDA_LAUNCH_BLOCKING\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mos.environ[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43mTORCH_USE_CUDA_DSA\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] = \u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43minput_dim = 4\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43moutput_dim = 2\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrain_data, train_labels, test_data, test_labels = get_training_data(input_dim, output_dim, data_seed=999)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# model = get_seeded_model(998, input_dim, output_dim)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# train_model(model, train_data, train_labels, test_data, test_labels, num_epochs = 10000, loss_target = 1/(input_dim ** 2 * 4))\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcache = create_cache(model)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43marrow_labels = [\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.join([str(d.item()) for d in v]) for v in train_data]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcolors = [BLUE, YELLOW, GREEN, RED]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43marrow_colors = [colors[l] for l in train_labels]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mvectors = compile_data_vectors(cache, input_labels=arrow_labels, input_colors = arrow_colors)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLabels: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, arrow_labels)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mclass Video(VisualizeTransformer):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def construct(self):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        VisualizeTransformer.construct(self)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/manim/utils/ipython_magic.py:122\u001b[0m, in \u001b[0;36mManimMagic.manim\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Render Manim scenes contained in IPython cells.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mWorks as a line or cell magic.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cell:\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m args \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-h\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--help\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--version\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args:\n",
            "File \u001b[0;32m<string>:13\u001b[0m\n",
            "Cell \u001b[0;32mIn[12], line 4\u001b[0m, in \u001b[0;36mcreate_cache\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m train_data\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransposed Input:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m logits, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabels: \u001b[39m\u001b[38;5;124m\"\u001b[39m, train_labels)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits of last token:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:657\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    642\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    649\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    650\u001b[0m ]:\n\u001b[1;32m    651\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \n\u001b[1;32m    653\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 657\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    661\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim)\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/transformer_lens/hook_points.py:568\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, pos_slice, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    555\u001b[0m     names_filter,\n\u001b[1;32m    556\u001b[0m     incl_bwd,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    559\u001b[0m     pos_slice\u001b[38;5;241m=\u001b[39mpos_slice,\n\u001b[1;32m    560\u001b[0m )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(\n\u001b[1;32m    563\u001b[0m     fwd_hooks\u001b[38;5;241m=\u001b[39mfwd,\n\u001b[1;32m    564\u001b[0m     bwd_hooks\u001b[38;5;241m=\u001b[39mbwd,\n\u001b[1;32m    565\u001b[0m     reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    566\u001b[0m     clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts,\n\u001b[1;32m    567\u001b[0m ):\n\u001b[0;32m--> 568\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    570\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/transformer_lens/HookedTransformer.py:575\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    571\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    572\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    573\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/transformer_lens/components/transformer_block.py:160\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    153\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    154\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    156\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/transformer_lens/components/abstract_attention.py:282\u001b[0m, in \u001b[0;36mAbstractAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         w \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange(\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_O, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_index d_head d_model -> d_model (head_index d_head)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         )\n\u001b[0;32m--> 282\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_head\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_O\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;66;03m# Explicitly calculate the attention result so it can be accessed by a hook\u001b[39;00m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;66;03m# This is off by default because it can easily eat through your GPU memory.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mload_in_4bit:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling cublasLtMatmul with transpose_mat1 0 transpose_mat2 0 m 2 n 32 k 2 mat1_ld 2 mat2_ld 2 result_ld 2 abcType 0 computeType 68 scaleType 0"
          ]
        }
      ],
      "source": [
        "%%manim -qh Video\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "\n",
        "input_dim = 4\n",
        "output_dim = 2\n",
        "train_data, train_labels, test_data, test_labels = get_training_data(input_dim, output_dim, data_seed=999)\n",
        "# model = get_seeded_model(998, input_dim, output_dim)\n",
        "# train_model(model, train_data, train_labels, test_data, test_labels, num_epochs = 10000, loss_target = 1/(input_dim ** 2 * 4))\n",
        "cache = create_cache(model)\n",
        "arrow_labels = [\"\".join([str(d.item()) for d in v]) for v in train_data]\n",
        "colors = [BLUE, YELLOW, GREEN, RED]\n",
        "arrow_colors = [colors[l] for l in train_labels]\n",
        "vectors = compile_data_vectors(cache, input_labels=arrow_labels, input_colors = arrow_colors)\n",
        "print(\"Labels: \", arrow_labels)\n",
        "\n",
        "class Video(VisualizeTransformer):\n",
        "    def construct(self):\n",
        "        VisualizeTransformer.construct(self)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8859a5491331dba93123a91c2831400aced845b502848170e05fcb48b2c144be"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
