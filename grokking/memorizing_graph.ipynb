{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpreting Identity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBvIU6zdIZpa"
      },
      "source": [
        "# Setup\n",
        "(No need to read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "ssx2pnTOIZpa"
      },
      "outputs": [],
      "source": [
        "TRAIN_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "7Jfz-lAWIZpb",
        "outputId": "48df1c71-5bc3-4a14-9a34-1af5c16786a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (24.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Running as a Jupyter notebook - intended for development only!\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_132870/2175630693.py:24: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"load_ext autoreload\")\n",
            "/tmp/ipykernel_132870/2175630693.py:25: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"autoreload 2\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipympl in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (0.9.6)\n",
            "Requirement already satisfied: ipython<9 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (8.31.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (8.1.5)\n",
            "Requirement already satisfied: matplotlib<4,>=3.5.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (3.10.0)\n",
            "Requirement already satisfied: numpy in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (2.2.1)\n",
            "Requirement already satisfied: pillow in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (11.1.0)\n",
            "Requirement already satisfied: traitlets<6 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (5.14.3)\n",
            "Requirement already satisfied: decorator in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (3.0.48)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (2.19.1)\n",
            "Requirement already satisfied: stack_data in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (0.6.3)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.2)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (2.9.0.post0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython<9->ipympl) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib<4,>=3.5.0->ipympl) (1.17.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (2.1.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (3.0.0)\n",
            "Requirement already satisfied: pure_eval in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (0.2.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scipy in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (1.15.0)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from scipy) (2.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: manim in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (0.18.1)\n",
            "Requirement already satisfied: Pillow>=9.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (11.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (2.19.1)\n",
            "Requirement already satisfied: click>=8.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (8.1.8)\n",
            "Requirement already satisfied: cloup>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.0.5)\n",
            "Requirement already satisfied: decorator>=4.3.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (5.1.1)\n",
            "Requirement already satisfied: isosurfaces>=0.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.1.2)\n",
            "Requirement already satisfied: manimpango<1.0.0,>=0.5.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.6.0)\n",
            "Requirement already satisfied: mapbox-earcut>=1.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.0.3)\n",
            "Requirement already satisfied: moderngl<6.0.0,>=5.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (5.12.0)\n",
            "Requirement already satisfied: moderngl-window>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.0.3)\n",
            "Requirement already satisfied: networkx>=2.6 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.26 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (2.2.1)\n",
            "Requirement already satisfied: pycairo<2.0.0,>=1.13 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.27.0)\n",
            "Requirement already satisfied: pydub>=0.20.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.25.1)\n",
            "Requirement already satisfied: rich>=12.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (13.9.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.15.0)\n",
            "Requirement already satisfied: screeninfo>=0.7 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.8.1)\n",
            "Requirement already satisfied: skia-pathops>=0.7.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.8.0.post2)\n",
            "Requirement already satisfied: srt>=3.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.5.3)\n",
            "Requirement already satisfied: svgelements>=1.8.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.9.6)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (4.12.2)\n",
            "Requirement already satisfied: watchdog>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (6.0.0)\n",
            "Requirement already satisfied: glcontext>=3.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from moderngl<6.0.0,>=5.0.0->manim) (3.0.0)\n",
            "Requirement already satisfied: pyglet>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from moderngl-window>=2.0.0->manim) (2.0.20)\n",
            "Requirement already satisfied: pyglm<3,>=2.7.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from moderngl-window>=2.0.0->manim) (2.7.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from rich>=12.0.0->manim) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->manim) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torch in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "/bin/bash: Zeile 1: 2: Datei oder Verzeichnis nicht gefunden\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: einops in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (0.8.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformer_lens in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (2.11.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (1.2.1)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (3.2.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.8.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.2.36)\n",
            "Requirement already satisfied: numpy>=1.26 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.2.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.10 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.5.1)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.66.5)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.47.1)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.4.1)\n",
            "Requirement already satisfied: typing-extensions in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.12.2)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.1.1)\n",
            "Requirement already satisfied: pyyaml in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.5.1)\n",
            "Requirement already satisfied: filelock in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.11.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (2.19.1)\n",
            "Requirement already satisfied: networkx in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (0.21.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.10.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (5.29.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.10.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.10->transformer_lens) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEVELOPMENT_MODE = True\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "# Upgrade pip\n",
        "%pip install --upgrade pip\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
        "    # # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")\n",
        "    ipython.run_line_magic(\"pip\", \"install ipympl\")\n",
        "    ipython.run_line_magic(\"pip\", \"install scipy\")\n",
        "    ipython.run_line_magic(\"pip\", \"install manim\")\n",
        "    ipython.run_line_magic(\"pip\", \"install torch\")\n",
        "    ipython.run_line_magic(\"pip\", \"install numpy<2\")\n",
        "    ipython.run_line_magic(\"pip\", \"install einops\")\n",
        "    ipython.run_line_magic(\"pip\", \"install transformer_lens\")\n",
        "\n",
        "if IN_COLAB or IN_GITHUB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis\n",
        "    %pip install ipympl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "i8GCNEdpIZpc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import numpy as np\n",
        "import einops\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import copy\n",
        "\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, ActivationCache\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "# Define the location to save the model, using a relative path\n",
        "PTH_LOCATION = \"workspace/_scratch/identity.pth\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M2Nzu7EIZpe"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOckI2-GIZpe"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "_2OXlPG-IZpe"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = 3\n",
        "OUTPUT_DIM = 3\n",
        "frac_train = 1\n",
        "\n",
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1e-2\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "num_epochs = 10000\n",
        "checkpoint_every = 500\n",
        "\n",
        "DATA_SEED = 599"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT7xY9WsIZpe"
      },
      "source": [
        "## Define Task\n",
        "* Define random function\n",
        "* Define the dataset & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOJMbA26IZpe"
      },
      "source": [
        "Input format:\n",
        "|a|b|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Ad6m8BIZpf"
      },
      "source": [
        "Convert this to a train + test set - 30% in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "I8Af2zLZIZpf",
        "outputId": "5075c607-e865-4b8c-90df-d2685d17ebb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2]])\n",
            "tensor([0, 1, 2, 1, 1, 0, 2, 2, 0])\n",
            "torch.Size([9, 2])\n",
            "tensor([], size=(0, 2), dtype=torch.int64)\n",
            "tensor([], dtype=torch.int64)\n",
            "torch.Size([0, 2])\n"
          ]
        }
      ],
      "source": [
        "def get_training_data(input_dim = INPUT_DIM, output_dim = OUTPUT_DIM):\n",
        "    # TODO: Define new data set as random two-parameter function\n",
        "\n",
        "    torch.manual_seed(DATA_SEED)\n",
        "    a_vector = torch.arange(input_dim)\n",
        "    dataset = torch.cartesian_prod(a_vector, a_vector).to(device)\n",
        "\n",
        "    labels = torch.randint(0, output_dim, (dataset.shape[0],), device=device)\n",
        "    torch.manual_seed(DATA_SEED)\n",
        "\n",
        "    train_data = dataset\n",
        "    train_labels = labels\n",
        "    # For now no test data\n",
        "    test_data = dataset[0:0]\n",
        "    test_labels = labels[0:0]\n",
        "    print(train_data)\n",
        "    print(train_labels)\n",
        "    print(train_data.shape)\n",
        "    print(test_data[:5])\n",
        "    print(test_labels[:5])\n",
        "    print(test_data.shape)\n",
        "    return train_data, train_labels, test_data, test_labels\n",
        "\n",
        "train_data, train_labels, test_data, test_labels = get_training_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_83i1bUkIZpf"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "UL7gVZ9WIZpf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_seeded_model(seed = 999, input_dim = INPUT_DIM, output_dim = OUTPUT_DIM):\n",
        "    cfg = HookedTransformerConfig(\n",
        "        n_layers = 1,\n",
        "        n_heads = 1,\n",
        "        d_model = 2,\n",
        "        d_head = 2,\n",
        "        d_mlp = None,\n",
        "        act_fn = \"relu\",\n",
        "        normalization_type=None,\n",
        "        d_vocab=input_dim,\n",
        "        d_vocab_out=output_dim,\n",
        "        n_ctx=2,\n",
        "        init_weights=True,\n",
        "        device=device,\n",
        "        seed = seed,\n",
        "        attn_only=True\n",
        "    )\n",
        "    model = HookedTransformer(cfg)\n",
        "    # Biases are enabled by default\n",
        "    # for name, param in model.named_parameters():\n",
        "    #     if \"b_\" in name:\n",
        "    #         param.requires_grad = False\n",
        "    return model\n",
        "\n",
        "model = get_seeded_model(seed = 993)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H61FscDoIZpg"
      },
      "source": [
        "Disable the biases, as we don't need them for this task and it makes things easier to interpret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lctx1jAtIZpg"
      },
      "source": [
        "## Define Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2]])\n",
            "tensor([[[-0.0232, -0.3831,  0.0734],\n",
            "         [ 0.3973,  0.0077,  0.2572]],\n",
            "\n",
            "        [[-0.0232, -0.3831,  0.0734],\n",
            "         [ 0.1674, -0.1676,  0.1479]],\n",
            "\n",
            "        [[-0.0232, -0.3831,  0.0734],\n",
            "         [ 0.1926,  0.8592, -0.0728]],\n",
            "\n",
            "        [[-0.3110, -0.5506, -0.0756],\n",
            "         [ 0.3419,  0.0151,  0.2194]],\n",
            "\n",
            "        [[-0.3110, -0.5506, -0.0756],\n",
            "         [ 0.1117, -0.1603,  0.1098]],\n",
            "\n",
            "        [[-0.3110, -0.5506, -0.0756],\n",
            "         [ 0.1047,  0.8789, -0.1347]],\n",
            "\n",
            "        [[-0.0824,  0.2363, -0.1083],\n",
            "         [ 0.4788, -0.1707,  0.3516]],\n",
            "\n",
            "        [[-0.0824,  0.2363, -0.1083],\n",
            "         [ 0.2537, -0.3527,  0.2469]],\n",
            "\n",
            "        [[-0.0824,  0.2363, -0.1083],\n",
            "         [ 0.3217,  0.6311,  0.0640]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(train_data)\n",
        "print(model(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "dJORAU_PIZpg",
        "outputId": "1c2b9f25-056e-4e0d-defc-0bd11586136d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.2079, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "Uniform loss:\n",
            "2.1972245773362196\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)\n",
        "def loss_fn(logits, labels):\n",
        "    if len(logits.shape)==3:\n",
        "        logits = logits[:, -1]\n",
        "    logits = logits.to(torch.float64)\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
        "    return -correct_log_probs.mean()\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(train_loss)\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(test_loss)\n",
        "print(\"Uniform loss:\")\n",
        "print(np.log(INPUT_DIM * INPUT_DIM))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9Q4bIcIZpg"
      },
      "source": [
        "## Actually Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj4h2LIRIZpg"
      },
      "source": [
        "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_stats(model):\n",
        "    # Extract the p 2-dimensional tensors, vector i is vec[:, i]\n",
        "    vec = model.W_U.data\n",
        "\n",
        "    # Function to compute the angle between two vectors\n",
        "    def compute_angle(v1, v2):\n",
        "        cos_theta = torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))\n",
        "        angle = torch.acos(cos_theta) * (180.0 / np.pi)\n",
        "        return angle\n",
        "\n",
        "    # Compute pairwise angles\n",
        "    # for i in range(vec.shape[1]):\n",
        "    #     for j in range(i+1, vec.shape[1]):\n",
        "    #         angle = compute_angle(vec[:, i], vec[:, j])\n",
        "    #         print(f\"Angle between {i} and {j}: {angle.item():.2f}Â°\")\n",
        "    #     print(f\"Norm of vector {i}: {torch.norm(vec[:, i]):.2f}\")\n",
        "\n",
        "print_stats(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c477d3a6463646deb7cf1de99bb0199a"
          ]
        },
        "id": "UsZLeCMeIZph",
        "outputId": "133f3502-ca28-490c-c030-f60b751dcd7b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "960ef159d0154daca30112af038b416f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 499 Train Loss 0.38312511649032743 Test Loss nan\n",
            "Epoch 999 Train Loss 0.059466672574566455 Test Loss nan\n",
            "Epoch 1499 Train Loss 0.017834876413487258 Test Loss nan\n",
            "Epoch 1999 Train Loss 0.008198291490770912 Test Loss nan\n",
            "Epoch 2499 Train Loss 0.0045474221205888754 Test Loss nan\n",
            "Epoch 2999 Train Loss 0.002792479299131615 Test Loss nan\n",
            "Epoch 3499 Train Loss 0.0018253827444404303 Test Loss nan\n",
            "Epoch 3999 Train Loss 0.001243454240354598 Test Loss nan\n",
            "Epoch 4499 Train Loss 0.0008715261628919556 Test Loss nan\n",
            "Epoch 4999 Train Loss 0.000623363184824029 Test Loss nan\n",
            "Epoch 5499 Train Loss 0.00045251579865528753 Test Loss nan\n",
            "Epoch 5999 Train Loss 0.0003321229238846405 Test Loss nan\n",
            "Epoch 6499 Train Loss 0.0002457877433733275 Test Loss nan\n",
            "Epoch 6999 Train Loss 0.00018305811923903094 Test Loss nan\n",
            "Epoch 7499 Train Loss 0.00013701582525633196 Test Loss nan\n",
            "Epoch 7999 Train Loss 0.00010295625678972151 Test Loss nan\n",
            "Epoch 8499 Train Loss 7.760370500183763e-05 Test Loss nan\n",
            "Epoch 8999 Train Loss 5.8645065525021103e-05 Test Loss nan\n",
            "Epoch 9499 Train Loss 4.4407799890347315e-05 Test Loss nan\n",
            "Epoch 9999 Train Loss 3.36845194394233e-05 Test Loss nan\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_data, train_labels, test_data, test_labels, num_epochs = num_epochs):\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=lr, weight_decay=wd, betas=betas\n",
        "    )\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    model_checkpoints = []\n",
        "    checkpoint_epochs = []\n",
        "    if TRAIN_MODEL:\n",
        "        for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "            train_logits = model(train_data)\n",
        "            train_loss = loss_fn(train_logits, train_labels)\n",
        "            train_loss.backward()\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                test_logits = model(test_data)\n",
        "                test_loss = loss_fn(test_logits, test_labels)\n",
        "                test_losses.append(test_loss.item())\n",
        "\n",
        "            if ((epoch+1)%checkpoint_every)==0:\n",
        "                checkpoint_epochs.append(epoch)\n",
        "                model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "                print_stats(model)\n",
        "                print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")\n",
        "    if TRAIN_MODEL:\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\":model.state_dict(),\n",
        "                \"config\": model.cfg,\n",
        "                \"checkpoints\": model_checkpoints,\n",
        "                \"checkpoint_epochs\": checkpoint_epochs,\n",
        "                \"test_losses\": test_losses,\n",
        "                \"train_losses\": train_losses,\n",
        "            },\n",
        "            PTH_LOCATION)\n",
        "\n",
        "train_model(\n",
        "    model, train_data, train_labels, test_data, test_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "EzRKi7J7IZph"
      },
      "outputs": [],
      "source": [
        "if not TRAIN_MODEL:\n",
        "    cached_data = torch.load(PTH_LOCATION)\n",
        "    model.load_state_dict(cached_data['model'])\n",
        "    model_checkpoints = cached_data[\"checkpoints\"]\n",
        "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
        "    test_losses = cached_data['test_losses']\n",
        "    train_losses = cached_data['train_losses']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transposed Input:\n",
            " tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
            "        [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
            "Labels:  tensor([0, 1, 2, 1, 1, 0, 2, 2, 0])\n",
            "Logits of last token:\n",
            " tensor([[ 10.4829,  -0.3025,  -2.2474],\n",
            "        [  6.5879,  17.4972, -25.4383],\n",
            "        [ 13.5779, -21.1048,  24.9873],\n",
            "        [ 11.3726,  29.8343, -42.1891],\n",
            "        [  6.9325,  32.8404, -45.7635],\n",
            "        [ 15.0433,  -1.8849,  -0.5669],\n",
            "        [  7.1881, -14.3199,  16.5910],\n",
            "        [  2.7490, -11.2691,  12.9574],\n",
            "        [ 13.9139,  -5.9930,   4.9690]], grad_fn=<SliceBackward0>)\n",
            "Unembed:\n",
            " tensor([[ 1.8803,  1.9334, -2.7273],\n",
            "        [-1.8391,  0.6233, -0.6581]])\n",
            "Last layer before unembed:\n",
            " tensor([[  1.4772,  -3.7749],\n",
            "        [  7.8878,   4.8970],\n",
            "        [ -6.2070, -13.3137],\n",
            "        [ 13.3178,   7.8469],\n",
            "        [ 13.9018,  10.8582],\n",
            "        [  1.4628,  -6.2691],\n",
            "        [ -4.4100,  -8.0023],\n",
            "        [ -3.8085,  -4.9736],\n",
            "        [ -0.2841,  -7.4411]])\n"
          ]
        }
      ],
      "source": [
        "def create_cache(model):\n",
        "    input = train_data\n",
        "    print(\"Transposed Input:\\n\", input.transpose(0, 1))\n",
        "    logits, cache = model.run_with_cache(input)\n",
        "    print(\"Labels: \", train_labels)\n",
        "    print(\"Logits of last token:\\n\", logits[:, -1, :])\n",
        "    print(\"Unembed:\\n\", model.W_U.data)\n",
        "    print(\"Last layer before unembed:\\n\", cache.cache_dict[\"blocks.0.hook_resid_post\"][:, -1, :])\n",
        "    return cache\n",
        "\n",
        "cache = create_cache(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9.074821799999999"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "-1.3451 * -3.4785 + 0.7905 * 5.5609"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hook_embed': tensor([[[ 0.7787, -1.3847],\n",
              "          [ 0.7787, -1.3847]],\n",
              " \n",
              "         [[ 0.7787, -1.3847],\n",
              "          [-1.5724, -2.0352]],\n",
              " \n",
              "         [[ 0.7787, -1.3847],\n",
              "          [ 1.0328,  2.4861]],\n",
              " \n",
              "         [[-1.5724, -2.0352],\n",
              "          [ 0.7787, -1.3847]],\n",
              " \n",
              "         [[-1.5724, -2.0352],\n",
              "          [-1.5724, -2.0352]],\n",
              " \n",
              "         [[-1.5724, -2.0352],\n",
              "          [ 1.0328,  2.4861]],\n",
              " \n",
              "         [[ 1.0328,  2.4861],\n",
              "          [ 0.7787, -1.3847]],\n",
              " \n",
              "         [[ 1.0328,  2.4861],\n",
              "          [-1.5724, -2.0352]],\n",
              " \n",
              "         [[ 1.0328,  2.4861],\n",
              "          [ 1.0328,  2.4861]]]),\n",
              " 'hook_pos_embed': tensor([[[ 2.0124,  1.3483],\n",
              "          [-1.7071, -0.9040]],\n",
              " \n",
              "         [[ 2.0124,  1.3483],\n",
              "          [-1.7071, -0.9040]],\n",
              " \n",
              "         [[ 2.0124,  1.3483],\n",
              "          [-1.7071, -0.9040]],\n",
              " \n",
              "         [[ 2.0124,  1.3483],\n",
              "          [-1.7071, -0.9040]],\n",
              " \n",
              "         [[ 2.0124,  1.3483],\n",
              "          [-1.7071, -0.9040]],\n",
              " \n",
              "         [[ 2.0124,  1.3483],\n",
              "          [-1.7071, -0.9040]],\n",
              " \n",
              "         [[ 2.0124,  1.3483],\n",
              "          [-1.7071, -0.9040]],\n",
              " \n",
              "         [[ 2.0124,  1.3483],\n",
              "          [-1.7071, -0.9040]],\n",
              " \n",
              "         [[ 2.0124,  1.3483],\n",
              "          [-1.7071, -0.9040]]]),\n",
              " 'blocks.0.hook_resid_pre': tensor([[[ 2.7911, -0.0365],\n",
              "          [-0.9284, -2.2887]],\n",
              " \n",
              "         [[ 2.7911, -0.0365],\n",
              "          [-3.2795, -2.9392]],\n",
              " \n",
              "         [[ 2.7911, -0.0365],\n",
              "          [-0.6743,  1.5821]],\n",
              " \n",
              "         [[ 0.4400, -0.6869],\n",
              "          [-0.9284, -2.2887]],\n",
              " \n",
              "         [[ 0.4400, -0.6869],\n",
              "          [-3.2795, -2.9392]],\n",
              " \n",
              "         [[ 0.4400, -0.6869],\n",
              "          [-0.6743,  1.5821]],\n",
              " \n",
              "         [[ 3.0452,  3.8343],\n",
              "          [-0.9284, -2.2887]],\n",
              " \n",
              "         [[ 3.0452,  3.8343],\n",
              "          [-3.2795, -2.9392]],\n",
              " \n",
              "         [[ 3.0452,  3.8343],\n",
              "          [-0.6743,  1.5821]]]),\n",
              " 'blocks.0.attn.hook_q': tensor([[[[-0.7350, -1.2924]],\n",
              " \n",
              "          [[ 0.2595,  0.3381]]],\n",
              " \n",
              " \n",
              "         [[[-0.7350, -1.2924]],\n",
              " \n",
              "          [[ 1.3038,  1.0154]]],\n",
              " \n",
              " \n",
              "         [[[-0.7350, -1.2924]],\n",
              " \n",
              "          [[ 2.1900, -1.4717]]],\n",
              " \n",
              " \n",
              "         [[[ 0.3093, -0.6151]],\n",
              " \n",
              "          [[ 0.2595,  0.3381]]],\n",
              " \n",
              " \n",
              "         [[[ 0.3093, -0.6151]],\n",
              " \n",
              "          [[ 1.3038,  1.0154]]],\n",
              " \n",
              " \n",
              "         [[[ 0.3093, -0.6151]],\n",
              " \n",
              "          [[ 2.1900, -1.4717]]],\n",
              " \n",
              " \n",
              "         [[[ 1.1956, -3.1022]],\n",
              " \n",
              "          [[ 0.2595,  0.3381]]],\n",
              " \n",
              " \n",
              "         [[[ 1.1956, -3.1022]],\n",
              " \n",
              "          [[ 1.3038,  1.0154]]],\n",
              " \n",
              " \n",
              "         [[[ 1.1956, -3.1022]],\n",
              " \n",
              "          [[ 2.1900, -1.4717]]]]),\n",
              " 'blocks.0.attn.hook_k': tensor([[[[-0.7378,  0.1270]],\n",
              " \n",
              "          [[ 2.6125, -4.7611]]],\n",
              " \n",
              " \n",
              "         [[[-0.7378,  0.1270]],\n",
              " \n",
              "          [[ 3.9041, -6.2346]]],\n",
              " \n",
              " \n",
              "         [[[-0.7378,  0.1270]],\n",
              " \n",
              "          [[-1.5878,  3.3427]]],\n",
              " \n",
              " \n",
              "         [[[ 0.5538, -1.3465]],\n",
              " \n",
              "          [[ 2.6125, -4.7611]]],\n",
              " \n",
              " \n",
              "         [[[ 0.5538, -1.3465]],\n",
              " \n",
              "          [[ 3.9041, -6.2346]]],\n",
              " \n",
              " \n",
              "         [[[ 0.5538, -1.3465]],\n",
              " \n",
              "          [[-1.5878,  3.3427]]],\n",
              " \n",
              " \n",
              "         [[[-4.9380,  8.2308]],\n",
              " \n",
              "          [[ 2.6125, -4.7611]]],\n",
              " \n",
              " \n",
              "         [[[-4.9380,  8.2308]],\n",
              " \n",
              "          [[ 3.9041, -6.2346]]],\n",
              " \n",
              " \n",
              "         [[[-4.9380,  8.2308]],\n",
              " \n",
              "          [[-1.5878,  3.3427]]]]),\n",
              " 'blocks.0.attn.hook_v': tensor([[[[ 1.0454,  4.2635]],\n",
              " \n",
              "          [[-3.8350, -3.9464]]],\n",
              " \n",
              " \n",
              "         [[[ 1.0454,  4.2635]],\n",
              " \n",
              "          [[-6.5853, -8.3255]]],\n",
              " \n",
              " \n",
              "         [[[ 1.0454,  4.2635]],\n",
              " \n",
              "          [[-1.8930,  0.5097]]],\n",
              " \n",
              " \n",
              "         [[[-1.7049, -0.1156]],\n",
              " \n",
              "          [[-3.8350, -3.9464]]],\n",
              " \n",
              " \n",
              "         [[[-1.7049, -0.1156]],\n",
              " \n",
              "          [[-6.5853, -8.3255]]],\n",
              " \n",
              " \n",
              "         [[[-1.7049, -0.1156]],\n",
              " \n",
              "          [[-1.8930,  0.5097]]],\n",
              " \n",
              " \n",
              "         [[[ 2.9874,  8.7195]],\n",
              " \n",
              "          [[-3.8350, -3.9464]]],\n",
              " \n",
              " \n",
              "         [[[ 2.9874,  8.7195]],\n",
              " \n",
              "          [[-6.5853, -8.3255]]],\n",
              " \n",
              " \n",
              "         [[[ 2.9874,  8.7195]],\n",
              " \n",
              "          [[-1.8930,  0.5097]]]]),\n",
              " 'blocks.0.attn.hook_attn_scores': tensor([[[[  0.2674,     -inf],\n",
              "           [ -0.1050,  -0.6589]]],\n",
              " \n",
              " \n",
              "         [[[  0.2674,     -inf],\n",
              "           [ -0.5890,  -0.8772]]],\n",
              " \n",
              " \n",
              "         [[[  0.2674,     -inf],\n",
              "           [ -1.2747,  -5.9375]]],\n",
              " \n",
              " \n",
              "         [[[  0.7068,     -inf],\n",
              "           [ -0.2203,  -0.6589]]],\n",
              " \n",
              " \n",
              "         [[[  0.7068,     -inf],\n",
              "           [ -0.4562,  -0.8772]]],\n",
              " \n",
              " \n",
              "         [[[  0.7068,     -inf],\n",
              "           [  2.2589,  -5.9375]]],\n",
              " \n",
              " \n",
              "         [[[-22.2299,     -inf],\n",
              "           [  1.0616,  -0.6589]]],\n",
              " \n",
              " \n",
              "         [[[-22.2299,     -inf],\n",
              "           [  1.3573,  -0.8772]]],\n",
              " \n",
              " \n",
              "         [[[-22.2299,     -inf],\n",
              "           [-16.2127,  -5.9375]]]]),\n",
              " 'blocks.0.attn.hook_pattern': tensor([[[[1.0000e+00, 0.0000e+00],\n",
              "           [6.3503e-01, 3.6497e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [5.7156e-01, 4.2844e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [9.9065e-01, 9.3509e-03]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [6.0792e-01, 3.9208e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [6.0372e-01, 3.9628e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [9.9972e-01, 2.7556e-04]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [8.4819e-01, 1.5181e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [9.0330e-01, 9.6697e-02]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [3.4478e-05, 9.9997e-01]]]]),\n",
              " 'blocks.0.attn.hook_z': tensor([[[[ 1.0454,  4.2635]],\n",
              " \n",
              "          [[-0.7358,  1.2671]]],\n",
              " \n",
              " \n",
              "         [[[ 1.0454,  4.2635]],\n",
              " \n",
              "          [[-2.2239, -1.1302]]],\n",
              " \n",
              " \n",
              "         [[[ 1.0454,  4.2635]],\n",
              " \n",
              "          [[ 1.0179,  4.2284]]],\n",
              " \n",
              " \n",
              "         [[[-1.7049, -0.1156]],\n",
              " \n",
              "          [[-2.5401, -1.6176]]],\n",
              " \n",
              " \n",
              "         [[[-1.7049, -0.1156]],\n",
              " \n",
              "          [[-3.6389, -3.3691]]],\n",
              " \n",
              " \n",
              "         [[[-1.7049, -0.1156]],\n",
              " \n",
              "          [[-1.7049, -0.1155]]],\n",
              " \n",
              " \n",
              "         [[[ 2.9874,  8.7195]],\n",
              " \n",
              "          [[ 1.9517,  6.7967]]],\n",
              " \n",
              " \n",
              "         [[[ 2.9874,  8.7195]],\n",
              " \n",
              "          [[ 2.0618,  7.0713]]],\n",
              " \n",
              " \n",
              "         [[[ 2.9874,  8.7195]],\n",
              " \n",
              "          [[-1.8929,  0.5099]]]]),\n",
              " 'blocks.0.hook_attn_out': tensor([[[  5.6039,   8.8927],\n",
              "          [  4.3398,   0.8483]],\n",
              " \n",
              "         [[  5.6039,   8.8927],\n",
              "          [  3.4431,  -5.6662]],\n",
              " \n",
              "         [[  5.6039,   8.8927],\n",
              "          [  5.6011,   8.7902]],\n",
              " \n",
              "         [[  4.0242,  -3.0473],\n",
              "          [  3.2856,  -7.0077]],\n",
              " \n",
              "         [[  4.0242,  -3.0473],\n",
              "          [  2.6517, -11.7817]],\n",
              " \n",
              "         [[  4.0242,  -3.0473],\n",
              "          [  4.0246,  -3.0470]],\n",
              " \n",
              "         [[  8.7712,  19.9743],\n",
              "          [  7.7645,  14.9460]],\n",
              " \n",
              "         [[  8.7712,  19.9743],\n",
              "          [  7.9772,  15.6169]],\n",
              " \n",
              "         [[  8.7712,  19.9743],\n",
              "          [  5.3077,  -2.0660]]]),\n",
              " 'blocks.0.hook_resid_post': tensor([[[  8.3950,   8.8562],\n",
              "          [  3.4114,  -1.4404]],\n",
              " \n",
              "         [[  8.3950,   8.8562],\n",
              "          [  0.1637,  -8.6054]],\n",
              " \n",
              "         [[  8.3950,   8.8562],\n",
              "          [  4.9268,  10.3723]],\n",
              " \n",
              "         [[  4.4643,  -3.7342],\n",
              "          [  2.3572,  -9.2964]],\n",
              " \n",
              "         [[  4.4643,  -3.7342],\n",
              "          [ -0.6278, -14.7209]],\n",
              " \n",
              "         [[  4.4643,  -3.7342],\n",
              "          [  3.3503,  -1.4649]],\n",
              " \n",
              "         [[ 11.8164,  23.8086],\n",
              "          [  6.8361,  12.6572]],\n",
              " \n",
              "         [[ 11.8164,  23.8086],\n",
              "          [  4.6978,  12.6777]],\n",
              " \n",
              "         [[ 11.8164,  23.8086],\n",
              "          [  4.6334,  -0.4839]]])}"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a list of all activations stored in the cache, especially their names\n",
        "cache.cache_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Animations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "from manim import *\n",
        "\n",
        "config.media_width = \"80%\"\n",
        "config.verbosity = \"WARNING\"\n",
        "config.preview = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'blocks.0.hook_resid_mid'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[150], line 45\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mprint\u001b[39m(vectors\u001b[38;5;241m.\u001b[39mvectors)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m vectors\n\u001b[0;32m---> 45\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_data_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[150], line 38\u001b[0m, in \u001b[0;36mcompile_data_vectors\u001b[0;34m(cache, input_labels, input_colors)\u001b[0m\n\u001b[1;32m     36\u001b[0m vectors\u001b[38;5;241m.\u001b[39madd_vectors_at_hook(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.0.hook_resid_pre\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_labels\u001b[38;5;241m=\u001b[39minput_labels, input_colors\u001b[38;5;241m=\u001b[39minput_colors)\n\u001b[1;32m     37\u001b[0m vectors\u001b[38;5;241m.\u001b[39mnext_step()\n\u001b[0;32m---> 38\u001b[0m \u001b[43mvectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_vectors_at_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks.0.hook_resid_mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGRAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_colors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_colors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_post\")\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectors\u001b[38;5;241m.\u001b[39mvectors)\n",
            "Cell \u001b[0;32mIn[150], line 23\u001b[0m, in \u001b[0;36mData.add_vectors_at_hook\u001b[0;34m(self, c, hook, color0, color1, input_labels, input_colors)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21madd_vectors_at_hook\u001b[39m(\u001b[38;5;28mself\u001b[39m, c: ActivationCache, hook: \u001b[38;5;28mstr\u001b[39m, color0 \u001b[38;5;241m=\u001b[39m WHITE, color1 \u001b[38;5;241m=\u001b[39m WHITE, input_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, input_colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m         input_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(c\u001b[38;5;241m.\u001b[39mcache_dict[hook]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_vector(c\u001b[38;5;241m.\u001b[39mcache_dict[hook][i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), color \u001b[38;5;241m=\u001b[39m color0, label \u001b[38;5;241m=\u001b[39m input_labels[i][:\u001b[38;5;241m1\u001b[39m])\n",
            "\u001b[0;31mKeyError\u001b[0m: 'blocks.0.hook_resid_mid'"
          ]
        }
      ],
      "source": [
        "class VectorParams:\n",
        "    def __init__(self, values = [], color = WHITE, label = \"\"):\n",
        "        self.values = values\n",
        "        self.color = color\n",
        "        self.label = label\n",
        "    def __repr__(self) -> str:\n",
        "        return str(self.values) + \"(\" + str(self.color) + \")\"\n",
        "\n",
        "class Data:\n",
        "    def __init__(self):\n",
        "        self.vectors: list[list[VectorParams]] = [[]]\n",
        "        self.steps = 0\n",
        "\n",
        "    def add_vector(self, vector, color = WHITE, label = \"\"):\n",
        "        self.vectors[self.steps].append(VectorParams(values = vector, color = color, label = label))\n",
        "\n",
        "    def next_step(self):\n",
        "        self.steps += 1\n",
        "        self.vectors.append([])\n",
        "\n",
        "    def add_vectors_at_hook(self, c: ActivationCache, hook: str, color0 = WHITE, color1 = WHITE, input_labels = None, input_colors = None):\n",
        "        if input_labels is None:\n",
        "            input_labels = [\"\" for i in range(c.cache_dict[hook].shape[0])]\n",
        "        for i in range(c.cache_dict[hook].shape[0]):\n",
        "            self.add_vector(c.cache_dict[hook][i][0].cpu(), color = color0, label = input_labels[i][:1])\n",
        "            self.add_vector(\n",
        "                c.cache_dict[hook][i][1].cpu(), color=color1 if input_colors is None else input_colors[i], label=input_labels[i][:2]\n",
        "            )\n",
        "\n",
        "\n",
        "def compile_data_vectors(cache, input_labels=None, input_colors=None):\n",
        "    # Set default value as list of empty strings\n",
        "    vectors = Data()\n",
        "    vectors.add_vectors_at_hook(cache, \"hook_embed\", color1 = GRAY, input_labels=input_labels, input_colors=input_colors)\n",
        "    vectors.next_step()\n",
        "    vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_pre\", input_labels=input_labels, input_colors=input_colors)\n",
        "    vectors.next_step()\n",
        "    vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_post\", color0 = GRAY, input_labels=input_labels, input_colors=input_colors)\n",
        "    # vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_mid\")\n",
        "\n",
        "    print(vectors.vectors)\n",
        "    return vectors\n",
        "\n",
        "\n",
        "vectors = compile_data_vectors(cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "def change_font_size(labeled_arrow: LabeledArrow, new_size):\n",
        "    # print(labeled_arrow, labeled_arrow.submobjects)\n",
        "    # print(labeled_arrow.submobjects[-1].font_size)\n",
        "    if not isinstance(labeled_arrow, LabeledArrow):\n",
        "        return\n",
        "    label = labeled_arrow.submobjects[-1]\n",
        "    box = labeled_arrow.submobjects[-2]\n",
        "    if not isinstance(box, BackgroundRectangle):\n",
        "        box = labeled_arrow.submobjects[-3]\n",
        "    coords = label.get_center()\n",
        "    # print(new_size)\n",
        "    labeled_arrow.submobjects[-1] = MathTex(\n",
        "        label.get_tex_string(), color=label.color, font_size=new_size\n",
        "    )\n",
        "    # print(\"size=\", labeled_arrow.submobjects[-1].font_size)\n",
        "    label = labeled_arrow.submobjects[-1]\n",
        "    label.move_to(coords)\n",
        "    box.width = label.width + 2 * box.buff\n",
        "    box.height = label.height + 2 * box.buff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VisualizeTransformer(MovingCameraScene):\n",
        "    def construct(self):\n",
        "        print(\"v=\", vectors.vectors)\n",
        "        axes = Axes(\n",
        "            x_range = [-20, 20, 1],\n",
        "            y_range = [-20, 20, 1],\n",
        "            x_axis_config={\n",
        "                \"numbers_to_include\": np.arange(-18, 18.1, 3),\n",
        "                \"font_size\": 24\n",
        "            },\n",
        "            y_axis_config={\n",
        "                \"numbers_to_include\": np.arange(-18, 18.1, 3), \n",
        "                \"font_size\": 24            \n",
        "            },\n",
        "            x_length = 40,\n",
        "            y_length = 40,\n",
        "            axis_config={\"color\": GREEN}\n",
        "        )\n",
        "\n",
        "        scale = ValueTracker(2)\n",
        "\n",
        "        arrows = VGroup()\n",
        "        def update_scale(self):\n",
        "            return\n",
        "            # TODO: Make the scaling nicer\n",
        "            self.stroke_width = 6 * scale.get_value()\n",
        "            change_font_size(self, 48 * scale.get_value())\n",
        "            # print(\"New font size: \", self.font_size)\n",
        "\n",
        "        # Embedding arrows\n",
        "        for i, t in enumerate(vectors.vectors[0]):\n",
        "            # print(t, t.numpy())\n",
        "            arrow = LabeledArrow(\n",
        "                start=ORIGIN,\n",
        "                end=np.append(t.values.numpy(), 0),\n",
        "                buff = 0,\n",
        "                label = t.label,\n",
        "                label_frame = False,\n",
        "                label_color=YELLOW,\n",
        "                color = t.color,\n",
        "                max_stroke_width_to_length_ratio = 100,\n",
        "            )\n",
        "\n",
        "            arrow.add_updater(update_scale)\n",
        "            arrows.add(arrow)\n",
        "\n",
        "        # Transitioning the arrows through the model\n",
        "        self.add(axes, axes.get_axis_labels(), arrows)\n",
        "        for step in range(1, len(vectors.vectors)):\n",
        "            new_arrows = VGroup()\n",
        "            transition_arrows = VGroup()\n",
        "            for i, t in enumerate(vectors.vectors[step]):\n",
        "                # print(t, t.numpy())\n",
        "                new_arrow = LabeledArrow(\n",
        "                    start=ORIGIN,\n",
        "                    end=np.append(t.values.numpy(), 0),\n",
        "                    buff=0,\n",
        "                    label=t.label,\n",
        "                    label_frame=False,\n",
        "                    label_color=YELLOW,\n",
        "                    color=t.color,\n",
        "                    max_stroke_width_to_length_ratio=100,\n",
        "                )\n",
        "                new_arrow.add_updater(update_scale)\n",
        "                new_arrows.add(new_arrow)\n",
        "                transition_arrow = Arrow(\n",
        "                    start=arrows[i].end, end=new_arrows[i].end, buff=0, color=RED\n",
        "                )\n",
        "                transition_arrow.add_updater(update_scale)\n",
        "                transition_arrows.add(transition_arrow)\n",
        "\n",
        "            view = SurroundingRectangle(new_arrows)\n",
        "            factor = max(\n",
        "                view.width / self.camera.frame_width,\n",
        "                view.height / self.camera.frame_height,\n",
        "            )\n",
        "            print(\n",
        "                factor,\n",
        "                self.camera.frame_width, view.width,\n",
        "                self.camera.frame_height, view.height,\n",
        "            )\n",
        "            self.wait()\n",
        "            self.play(FadeIn(transition_arrows), self.camera.auto_zoom(view, margin = 2), scale.animate.set_value(scale.get_value() * factor))\n",
        "            self.wait()\n",
        "            self.play(\n",
        "                ReplacementTransform(arrows, new_arrows)\n",
        "            )\n",
        "            self.wait()\n",
        "            self.play(FadeOut(transition_arrows))\n",
        "            self.wait()\n",
        "            arrows = new_arrows\n",
        "\n",
        "        # Unembedding Arrows\n",
        "        embedding_arrows = VGroup()\n",
        "        data = model.W_U.data\n",
        "        print(\"unembed: \", data)\n",
        "        for i in range(model.W_U.data.size()[1]):\n",
        "            embedding_arrow = LabeledArrow(\n",
        "                start=ORIGIN,\n",
        "                end=[data[0, i].item(), data[1, i].item(), 0],\n",
        "                label=str(i),\n",
        "                color=LIGHT_PINK,\n",
        "                buff=0,\n",
        "                max_stroke_width_to_length_ratio=100,\n",
        "            )\n",
        "            embedding_arrows.add(embedding_arrow)\n",
        "        self.play(FadeIn(embedding_arrows))\n",
        "        self.wait()\n",
        "\n",
        "# v = VisualizeTransformer()\n",
        "# v.construct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2]])\n",
            "tensor([0, 1, 2, 1, 1, 0, 2, 2, 0])\n",
            "torch.Size([9, 2])\n",
            "tensor([], size=(0, 2), dtype=torch.int64)\n",
            "tensor([], dtype=torch.int64)\n",
            "torch.Size([0, 2])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78b1404691ff49fe9a6afabc371f851d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 499 Train Loss 0.6421390828313496 Test Loss nan\n",
            "Epoch 999 Train Loss 0.1928687304109461 Test Loss nan\n",
            "Epoch 1499 Train Loss 0.06768903053940167 Test Loss nan\n",
            "Epoch 1999 Train Loss 0.0301804806828507 Test Loss nan\n",
            "Epoch 2499 Train Loss 0.0158383624467111 Test Loss nan\n",
            "Epoch 2999 Train Loss 0.009204395812378592 Test Loss nan\n",
            "Epoch 3499 Train Loss 0.005723434807426177 Test Loss nan\n",
            "Epoch 3999 Train Loss 0.003722629364008125 Test Loss nan\n",
            "Epoch 4499 Train Loss 0.002459580057104153 Test Loss nan\n",
            "Epoch 4999 Train Loss 0.000194753486736811 Test Loss nan\n",
            "Epoch 5499 Train Loss 9.725542117432578e-05 Test Loss nan\n",
            "Epoch 5999 Train Loss 6.207135603489843e-05 Test Loss nan\n",
            "Epoch 6499 Train Loss 4.395756090837954e-05 Test Loss nan\n",
            "Epoch 6999 Train Loss 3.2805621649162914e-05 Test Loss nan\n",
            "Epoch 7499 Train Loss 2.5187485393298896e-05 Test Loss nan\n",
            "Epoch 7999 Train Loss 1.9617942014220933e-05 Test Loss nan\n",
            "Epoch 8499 Train Loss 1.5358451373570808e-05 Test Loss nan\n",
            "Epoch 8999 Train Loss 1.2021419593190775e-05 Test Loss nan\n",
            "Epoch 9499 Train Loss 9.380231982155694e-06 Test Loss nan\n",
            "Epoch 9999 Train Loss 7.290923043476372e-06 Test Loss nan\n",
            "Transposed Input:\n",
            " tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
            "        [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
            "Labels:  tensor([0, 1, 2, 1, 1, 0, 2, 2, 0])\n",
            "Logits of last token:\n",
            " tensor([[  8.0202,  -3.9290,  -3.5731],\n",
            "        [  0.8057,  19.8774, -20.0623],\n",
            "        [ 11.6703, -34.8556,  23.2990],\n",
            "        [  5.4931,  16.7392, -21.4481],\n",
            "        [ -1.0989,  35.9034, -33.9748],\n",
            "        [  7.8882,  -3.7391,  -3.6336],\n",
            "        [ 15.8473, -44.3415,  28.6287],\n",
            "        [ 11.2557, -39.7536,  28.5005],\n",
            "        [ 10.6767,  -8.8165,  -1.3074]], grad_fn=<SliceBackward0>)\n",
            "Unembed:\n",
            " tensor([[ 2.1475, -2.1679,  0.0816],\n",
            "        [ 0.0335, -2.3400,  2.2644]])\n",
            "Last layer before unembed:\n",
            " tensor([[  3.4114,  -1.4404],\n",
            "        [  0.1637,  -8.6054],\n",
            "        [  4.9268,  10.3723],\n",
            "        [  2.3572,  -9.2964],\n",
            "        [ -0.6278, -14.7209],\n",
            "        [  3.3503,  -1.4649],\n",
            "        [  6.8361,  12.6572],\n",
            "        [  4.6978,  12.6777],\n",
            "        [  4.6334,  -0.4839]])\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'blocks.0.hook_resid_mid'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[152], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmanim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-ql Video\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43minput_dim = 3\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43moutput_dim = 3\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrain_data, train_labels, test_data, test_labels = get_training_data(input_dim, output_dim)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel = get_seeded_model(999, input_dim, output_dim)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrain_model(model, train_data, train_labels, test_data, test_labels, num_epochs = 10000)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcache = create_cache(model)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43marrow_labels = [\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.join([str(d.item()) for d in v]) for v in train_data]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mcolors = [BLUE, YELLOW, GREEN, RED]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43marrow_colors = [colors[l] for l in train_labels]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mvectors = compile_data_vectors(cache, input_labels=arrow_labels, input_colors = arrow_colors)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mprint(\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLabels: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m, arrow_labels)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mclass Video(VisualizeTransformer):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    def construct(self):\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m        VisualizeTransformer.construct(self)\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2541\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2539\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2540\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2541\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2543\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2544\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m~/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/manim/utils/ipython_magic.py:122\u001b[0m, in \u001b[0;36mManimMagic.manim\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Render Manim scenes contained in IPython cells.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03mWorks as a line or cell magic.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cell:\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_ns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m args \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-h\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--help\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--version\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args:\n",
            "File \u001b[0;32m<string>:11\u001b[0m\n",
            "Cell \u001b[0;32mIn[150], line 38\u001b[0m, in \u001b[0;36mcompile_data_vectors\u001b[0;34m(cache, input_labels, input_colors)\u001b[0m\n\u001b[1;32m     36\u001b[0m vectors\u001b[38;5;241m.\u001b[39madd_vectors_at_hook(cache, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblocks.0.hook_resid_pre\u001b[39m\u001b[38;5;124m\"\u001b[39m, input_labels\u001b[38;5;241m=\u001b[39minput_labels, input_colors\u001b[38;5;241m=\u001b[39minput_colors)\n\u001b[1;32m     37\u001b[0m vectors\u001b[38;5;241m.\u001b[39mnext_step()\n\u001b[0;32m---> 38\u001b[0m \u001b[43mvectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_vectors_at_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblocks.0.hook_resid_mid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mGRAY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_colors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_colors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_post\")\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(vectors\u001b[38;5;241m.\u001b[39mvectors)\n",
            "Cell \u001b[0;32mIn[150], line 24\u001b[0m, in \u001b[0;36mData.add_vectors_at_hook\u001b[0;34m(self, c, hook, color0, color1, input_labels, input_colors)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     input_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(c\u001b[38;5;241m.\u001b[39mcache_dict[hook]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])]\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_vector(c\u001b[38;5;241m.\u001b[39mcache_dict[hook][i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), color \u001b[38;5;241m=\u001b[39m color0, label \u001b[38;5;241m=\u001b[39m input_labels[i][:\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_vector(\n\u001b[1;32m     27\u001b[0m         c\u001b[38;5;241m.\u001b[39mcache_dict[hook][i][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu(), color\u001b[38;5;241m=\u001b[39mcolor1 \u001b[38;5;28;01mif\u001b[39;00m input_colors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m input_colors[i], label\u001b[38;5;241m=\u001b[39minput_labels[i][:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     28\u001b[0m     )\n",
            "\u001b[0;31mKeyError\u001b[0m: 'blocks.0.hook_resid_mid'"
          ]
        }
      ],
      "source": [
        "%%manim -ql Video\n",
        "\n",
        "input_dim = 3\n",
        "output_dim = 3\n",
        "train_data, train_labels, test_data, test_labels = get_training_data(input_dim, output_dim)\n",
        "model = get_seeded_model(999, input_dim, output_dim)\n",
        "train_model(model, train_data, train_labels, test_data, test_labels, num_epochs = 10000)\n",
        "cache = create_cache(model)\n",
        "arrow_labels = [\"\".join([str(d.item()) for d in v]) for v in train_data]\n",
        "colors = [BLUE, YELLOW, GREEN, RED]\n",
        "arrow_colors = [colors[l] for l in train_labels]\n",
        "vectors = compile_data_vectors(cache, input_labels=arrow_labels, input_colors = arrow_colors)\n",
        "print(\"Labels: \", arrow_labels)\n",
        "\n",
        "class Video(VisualizeTransformer):\n",
        "    def construct(self):\n",
        "        VisualizeTransformer.construct(self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.2470, -1.5699,  1.6658],\n",
              "        [ 0.6676, -1.5448,  0.6091]])"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.W_U.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[tensor([-0.9791, -0.5990])(#FFFFFF),\n",
              "  tensor([-0.9791, -0.5990])(#888888),\n",
              "  tensor([-0.9791, -0.5990])(#FFFFFF),\n",
              "  tensor([ 0.9609, -0.1159])(#888888),\n",
              "  tensor([-0.9791, -0.5990])(#FFFFFF),\n",
              "  tensor([-0.4902,  1.0463])(#888888),\n",
              "  tensor([ 0.9609, -0.1159])(#FFFFFF),\n",
              "  tensor([-0.9791, -0.5990])(#888888),\n",
              "  tensor([ 0.9609, -0.1159])(#FFFFFF),\n",
              "  tensor([ 0.9609, -0.1159])(#888888),\n",
              "  tensor([ 0.9609, -0.1159])(#FFFFFF),\n",
              "  tensor([-0.4902,  1.0463])(#888888),\n",
              "  tensor([-0.4902,  1.0463])(#FFFFFF),\n",
              "  tensor([-0.9791, -0.5990])(#888888),\n",
              "  tensor([-0.4902,  1.0463])(#FFFFFF),\n",
              "  tensor([ 0.9609, -0.1159])(#888888),\n",
              "  tensor([-0.4902,  1.0463])(#FFFFFF),\n",
              "  tensor([-0.4902,  1.0463])(#888888)],\n",
              " [tensor([-0.6081, -0.2109])(#FFFFFF),\n",
              "  tensor([-1.7593, -0.4604])(#FFFFFF),\n",
              "  tensor([-0.6081, -0.2109])(#FFFFFF),\n",
              "  tensor([0.1807, 0.0228])(#FFFFFF),\n",
              "  tensor([-0.6081, -0.2109])(#FFFFFF),\n",
              "  tensor([-1.2703,  1.1849])(#FFFFFF),\n",
              "  tensor([1.3319, 0.2723])(#FFFFFF),\n",
              "  tensor([-1.7593, -0.4604])(#FFFFFF),\n",
              "  tensor([1.3319, 0.2723])(#FFFFFF),\n",
              "  tensor([0.1807, 0.0228])(#FFFFFF),\n",
              "  tensor([1.3319, 0.2723])(#FFFFFF),\n",
              "  tensor([-1.2703,  1.1849])(#FFFFFF),\n",
              "  tensor([-0.1191,  1.4344])(#FFFFFF),\n",
              "  tensor([-1.7593, -0.4604])(#FFFFFF),\n",
              "  tensor([-0.1191,  1.4344])(#FFFFFF),\n",
              "  tensor([0.1807, 0.0228])(#FFFFFF),\n",
              "  tensor([-0.1191,  1.4344])(#FFFFFF),\n",
              "  tensor([-1.2703,  1.1849])(#FFFFFF)],\n",
              " [tensor([0.2620, 1.2962])(#888888),\n",
              "  tensor([-0.6774,  1.5111])(#FFFFFF),\n",
              "  tensor([0.2620, 1.2962])(#888888),\n",
              "  tensor([0.4618, 0.2362])(#FFFFFF),\n",
              "  tensor([0.2620, 1.2962])(#888888),\n",
              "  tensor([-0.0236,  3.4465])(#FFFFFF),\n",
              "  tensor([-0.4044, -3.9398])(#888888),\n",
              "  tensor([-3.4601, -4.5947])(#FFFFFF),\n",
              "  tensor([-0.4044, -3.9398])(#888888),\n",
              "  tensor([-0.9012, -2.7543])(#FFFFFF),\n",
              "  tensor([-0.4044, -3.9398])(#888888),\n",
              "  tensor([-1.3179,  0.6244])(#FFFFFF),\n",
              "  tensor([-0.2629,  0.6190])(#888888),\n",
              "  tensor([0.2298, 3.4840])(#FFFFFF),\n",
              "  tensor([-0.2629,  0.6190])(#888888),\n",
              "  tensor([ 0.0086, -0.8053])(#FFFFFF),\n",
              "  tensor([-0.2629,  0.6190])(#888888),\n",
              "  tensor([-0.8094,  1.6954])(#FFFFFF)]]"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[0000706e946097a0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
            "[0000706e946097a0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
            "[0000706ec4c0b110] avcodec decoder: Using NVIDIA VDPAU Driver Shared Library  550.127.08  Fri Oct 25 21:40:15 UTC 2024 for hardware decoding\n",
            "[0000706e946097a0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
            "[0000706e946097a0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
            "[0000706ec4c12850] avcodec decoder: Using NVIDIA VDPAU Driver Shared Library  550.127.08  Fri Oct 25 21:40:15 UTC 2024 for hardware decoding\n",
            "[0000706e946097a0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
            "[0000706e946097a0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
            "[0000706ec00eeab0] avcodec decoder: Using NVIDIA VDPAU Driver Shared Library  550.127.08  Fri Oct 25 21:40:15 UTC 2024 for hardware decoding\n",
            "[0000706e945c03e0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
            "[0000706e945c03e0] gl gl: Initialized libplacebo v4.192.1 (API v192)\n",
            "[0000706ec0005150] avcodec decoder: Using NVIDIA VDPAU Driver Shared Library  550.127.08  Fri Oct 25 21:40:15 UTC 2024 for hardware decoding\n"
          ]
        }
      ],
      "source": [
        "vectors.vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8859a5491331dba93123a91c2831400aced845b502848170e05fcb48b2c144be"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
