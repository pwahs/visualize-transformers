{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpreting Identity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBvIU6zdIZpa"
      },
      "source": [
        "# Setup\n",
        "(No need to read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ssx2pnTOIZpa"
      },
      "outputs": [],
      "source": [
        "TRAIN_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Jfz-lAWIZpb",
        "outputId": "48df1c71-5bc3-4a14-9a34-1af5c16786a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (24.3.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Running as a Jupyter notebook - intended for development only!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_642462/2175630693.py:24: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"load_ext autoreload\")\n",
            "/tmp/ipykernel_642462/2175630693.py:25: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"autoreload 2\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipympl in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (0.9.6)\n",
            "Requirement already satisfied: ipython<9 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (8.31.0)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (8.1.5)\n",
            "Requirement already satisfied: matplotlib<4,>=3.5.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (3.10.0)\n",
            "Requirement already satisfied: numpy in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (2.2.1)\n",
            "Requirement already satisfied: pillow in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (11.1.0)\n",
            "Requirement already satisfied: traitlets<6 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipympl) (5.14.3)\n",
            "Requirement already satisfied: decorator in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (3.0.48)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (2.19.1)\n",
            "Requirement already satisfied: stack_data in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipython<9->ipympl) (0.6.3)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.2)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from matplotlib<4,>=3.5.0->ipympl) (2.9.0.post0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython<9->ipympl) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib<4,>=3.5.0->ipympl) (1.17.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (2.1.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (3.0.0)\n",
            "Requirement already satisfied: pure_eval in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from stack_data->ipython<9->ipympl) (0.2.3)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scipy in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (1.15.0)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from scipy) (2.2.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: manim in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (0.18.1)\n",
            "Requirement already satisfied: Pillow>=9.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (11.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (2.19.1)\n",
            "Requirement already satisfied: click>=8.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (8.1.8)\n",
            "Requirement already satisfied: cloup>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.0.5)\n",
            "Requirement already satisfied: decorator>=4.3.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (5.1.1)\n",
            "Requirement already satisfied: isosurfaces>=0.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.1.2)\n",
            "Requirement already satisfied: manimpango<1.0.0,>=0.5.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.6.0)\n",
            "Requirement already satisfied: mapbox-earcut>=1.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.0.3)\n",
            "Requirement already satisfied: moderngl<6.0.0,>=5.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (5.12.0)\n",
            "Requirement already satisfied: moderngl-window>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.0.3)\n",
            "Requirement already satisfied: networkx>=2.6 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.26 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (2.2.1)\n",
            "Requirement already satisfied: pycairo<2.0.0,>=1.13 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.27.0)\n",
            "Requirement already satisfied: pydub>=0.20.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.25.1)\n",
            "Requirement already satisfied: rich>=12.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (13.9.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.15.0)\n",
            "Requirement already satisfied: screeninfo>=0.7 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.8.1)\n",
            "Requirement already satisfied: skia-pathops>=0.7.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (0.8.0.post2)\n",
            "Requirement already satisfied: srt>=3.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (3.5.3)\n",
            "Requirement already satisfied: svgelements>=1.8.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (1.9.6)\n",
            "Requirement already satisfied: tqdm>=4.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (4.12.2)\n",
            "Requirement already satisfied: watchdog>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from manim) (6.0.0)\n",
            "Requirement already satisfied: glcontext>=3.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from moderngl<6.0.0,>=5.0.0->manim) (3.0.0)\n",
            "Requirement already satisfied: pyglet>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from moderngl-window>=2.0.0->manim) (2.0.20)\n",
            "Requirement already satisfied: pyglm<3,>=2.7.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from moderngl-window>=2.0.0->manim) (2.7.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from rich>=12.0.0->manim) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->manim) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: torch in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "/bin/bash: Zeile 1: 2: Datei oder Verzeichnis nicht gefunden\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: einops in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (0.8.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformer_lens in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (2.11.0)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (1.2.1)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (3.2.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.8.0)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.2.36)\n",
            "Requirement already satisfied: numpy>=1.26 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.2.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.2.0)\n",
            "Requirement already satisfied: torch>=1.10 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (2.5.1)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.66.5)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.47.1)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.4.1)\n",
            "Requirement already satisfied: typing-extensions in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (4.12.2)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformer_lens) (0.19.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (24.1)\n",
            "Requirement already satisfied: psutil in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.1.1)\n",
            "Requirement already satisfied: pyyaml in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.27.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from accelerate>=0.23.0->transformer_lens) (0.5.1)\n",
            "Requirement already satisfied: filelock in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from datasets>=2.7.1->transformer_lens) (3.11.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pandas>=1.1.5->transformer_lens) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from rich>=12.6.0->transformer_lens) (2.19.1)\n",
            "Requirement already satisfied: networkx in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (75.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from torch>=1.10->transformer_lens) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.10->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from transformers>=4.37.2->transformer_lens) (0.21.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (3.10.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (5.29.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.10.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from wandb>=0.13.5->transformer_lens) (1.3.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets>=2.7.1->transformer_lens) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb>=0.13.5->transformer_lens) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from jinja2->torch>=1.10->transformer_lens) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEVELOPMENT_MODE = True\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "# Upgrade pip\n",
        "%pip install --upgrade pip\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
        "    # # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")\n",
        "    ipython.run_line_magic(\"pip\", \"install ipympl\")\n",
        "    ipython.run_line_magic(\"pip\", \"install scipy\")\n",
        "    ipython.run_line_magic(\"pip\", \"install manim\")\n",
        "    ipython.run_line_magic(\"pip\", \"install torch\")\n",
        "    ipython.run_line_magic(\"pip\", \"install numpy<2\")\n",
        "    ipython.run_line_magic(\"pip\", \"install einops\")\n",
        "    ipython.run_line_magic(\"pip\", \"install transformer_lens\")\n",
        "\n",
        "if IN_COLAB or IN_GITHUB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis\n",
        "    %pip install ipympl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "i8GCNEdpIZpc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/philippw/Stuff/programming/math/miniconda3/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "  return torch._C._cuda_getDeviceCount() > 0\n"
          ]
        }
      ],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import numpy as np\n",
        "import einops\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import copy\n",
        "\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, ActivationCache\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "# Define the location to save the model, using a relative path\n",
        "PTH_LOCATION = \"workspace/_scratch/identity.pth\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M2Nzu7EIZpe"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOckI2-GIZpe"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_2OXlPG-IZpe"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = 3\n",
        "OUTPUT_DIM = 3\n",
        "frac_train = 1\n",
        "\n",
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1e-2\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "num_epochs = 10000\n",
        "checkpoint_every = 500\n",
        "\n",
        "DATA_SEED = 599"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT7xY9WsIZpe"
      },
      "source": [
        "## Define Task\n",
        "* Define random function\n",
        "* Define the dataset & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOJMbA26IZpe"
      },
      "source": [
        "Input format:\n",
        "|a|b|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Ad6m8BIZpf"
      },
      "source": [
        "Convert this to a train + test set - 30% in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "I8Af2zLZIZpf",
        "outputId": "5075c607-e865-4b8c-90df-d2685d17ebb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2]])\n",
            "tensor([0, 1, 2, 1, 1, 0, 2, 2, 0])\n",
            "torch.Size([9, 2])\n",
            "tensor([], size=(0, 2), dtype=torch.int64)\n",
            "tensor([], dtype=torch.int64)\n",
            "torch.Size([0, 2])\n"
          ]
        }
      ],
      "source": [
        "def get_training_data(input_dim = INPUT_DIM, output_dim = OUTPUT_DIM, data_seed = DATA_SEED):\n",
        "    # TODO: Define new data set as random two-parameter function\n",
        "\n",
        "    torch.manual_seed(data_seed)\n",
        "    a_vector = torch.arange(input_dim)\n",
        "    dataset = torch.cartesian_prod(a_vector, a_vector).to(device)\n",
        "\n",
        "    labels = torch.randint(0, output_dim, (dataset.shape[0],), device=device)\n",
        "    train_data = dataset\n",
        "    train_labels = labels\n",
        "    # For now no test data\n",
        "    test_data = dataset[0:0]\n",
        "    test_labels = labels[0:0]\n",
        "    print(train_data)\n",
        "    print(train_labels)\n",
        "    print(train_data.shape)\n",
        "    print(test_data[:5])\n",
        "    print(test_labels[:5])\n",
        "    print(test_data.shape)\n",
        "    return train_data, train_labels, test_data, test_labels\n",
        "\n",
        "train_data, train_labels, test_data, test_labels = get_training_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_83i1bUkIZpf"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UL7gVZ9WIZpf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_seeded_model(seed = 999, input_dim = INPUT_DIM, output_dim = OUTPUT_DIM):\n",
        "    cfg = HookedTransformerConfig(\n",
        "        n_layers = 1,\n",
        "        n_heads = 1,\n",
        "        d_model = 2,\n",
        "        d_head = 2,\n",
        "        d_mlp = None,\n",
        "        act_fn = \"relu\",\n",
        "        normalization_type=None,\n",
        "        d_vocab=input_dim,\n",
        "        d_vocab_out=output_dim,\n",
        "        n_ctx=2,\n",
        "        init_weights=True,\n",
        "        device=device,\n",
        "        seed = seed,\n",
        "        attn_only=True\n",
        "    )\n",
        "    model = HookedTransformer(cfg)\n",
        "    # Biases are enabled by default\n",
        "    # for name, param in model.named_parameters():\n",
        "    #     if \"b_\" in name:\n",
        "    #         param.requires_grad = False\n",
        "    return model\n",
        "\n",
        "model = get_seeded_model(seed = 993)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H61FscDoIZpg"
      },
      "source": [
        "Disable the biases, as we don't need them for this task and it makes things easier to interpret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lctx1jAtIZpg"
      },
      "source": [
        "## Define Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2]])\n",
            "tensor([[[-0.0232, -0.3831,  0.0734],\n",
            "         [ 0.3973,  0.0077,  0.2572]],\n",
            "\n",
            "        [[-0.0232, -0.3831,  0.0734],\n",
            "         [ 0.1674, -0.1676,  0.1479]],\n",
            "\n",
            "        [[-0.0232, -0.3831,  0.0734],\n",
            "         [ 0.1926,  0.8592, -0.0728]],\n",
            "\n",
            "        [[-0.3110, -0.5506, -0.0756],\n",
            "         [ 0.3419,  0.0151,  0.2194]],\n",
            "\n",
            "        [[-0.3110, -0.5506, -0.0756],\n",
            "         [ 0.1117, -0.1603,  0.1098]],\n",
            "\n",
            "        [[-0.3110, -0.5506, -0.0756],\n",
            "         [ 0.1047,  0.8789, -0.1347]],\n",
            "\n",
            "        [[-0.0824,  0.2363, -0.1083],\n",
            "         [ 0.4788, -0.1707,  0.3516]],\n",
            "\n",
            "        [[-0.0824,  0.2363, -0.1083],\n",
            "         [ 0.2537, -0.3527,  0.2469]],\n",
            "\n",
            "        [[-0.0824,  0.2363, -0.1083],\n",
            "         [ 0.3217,  0.6311,  0.0640]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(train_data)\n",
        "print(model(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dJORAU_PIZpg",
        "outputId": "1c2b9f25-056e-4e0d-defc-0bd11586136d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.2079, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "Uniform loss:\n",
            "2.1972245773362196\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)\n",
        "def loss_fn(logits, labels):\n",
        "    if len(logits.shape)==3:\n",
        "        logits = logits[:, -1]\n",
        "    logits = logits.to(torch.float64)\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
        "    return -correct_log_probs.mean()\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(train_loss)\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(test_loss)\n",
        "print(\"Uniform loss:\")\n",
        "print(np.log(INPUT_DIM * INPUT_DIM))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9Q4bIcIZpg"
      },
      "source": [
        "## Actually Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj4h2LIRIZpg"
      },
      "source": [
        "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_stats(model):\n",
        "    # Extract the p 2-dimensional tensors, vector i is vec[:, i]\n",
        "    vec = model.W_U.data\n",
        "\n",
        "    # Function to compute the angle between two vectors\n",
        "    def compute_angle(v1, v2):\n",
        "        cos_theta = torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))\n",
        "        angle = torch.acos(cos_theta) * (180.0 / np.pi)\n",
        "        return angle\n",
        "\n",
        "    # Compute pairwise angles\n",
        "    # for i in range(vec.shape[1]):\n",
        "    #     for j in range(i+1, vec.shape[1]):\n",
        "    #         angle = compute_angle(vec[:, i], vec[:, j])\n",
        "    #         print(f\"Angle between {i} and {j}: {angle.item():.2f}°\")\n",
        "    #     print(f\"Norm of vector {i}: {torch.norm(vec[:, i]):.2f}\")\n",
        "\n",
        "print_stats(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c477d3a6463646deb7cf1de99bb0199a"
          ]
        },
        "id": "UsZLeCMeIZph",
        "outputId": "133f3502-ca28-490c-c030-f60b751dcd7b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e55eed1e35b64eb8a812c6e4cfb20199",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 499 Train Loss 0.38312511649032743 Test Loss nan\n",
            "Epoch 999 Train Loss 0.059466672574566455 Test Loss nan\n",
            "Epoch 1499 Train Loss 0.017834876413487258 Test Loss nan\n",
            "Epoch 1999 Train Loss 0.008198291490770912 Test Loss nan\n",
            "Epoch 2499 Train Loss 0.0045474221205888754 Test Loss nan\n",
            "Epoch 2999 Train Loss 0.002792479299131615 Test Loss nan\n",
            "Epoch 3499 Train Loss 0.0018253827444404303 Test Loss nan\n",
            "Epoch 3999 Train Loss 0.001243454240354598 Test Loss nan\n",
            "Epoch 4499 Train Loss 0.0008715261628919556 Test Loss nan\n",
            "Epoch 4999 Train Loss 0.000623363184824029 Test Loss nan\n",
            "Epoch 5499 Train Loss 0.00045251579865528753 Test Loss nan\n",
            "Epoch 5999 Train Loss 0.0003321229238846405 Test Loss nan\n",
            "Epoch 6499 Train Loss 0.0002457877433733275 Test Loss nan\n",
            "Epoch 6999 Train Loss 0.00018305811923903094 Test Loss nan\n",
            "Epoch 7499 Train Loss 0.00013701582525633196 Test Loss nan\n",
            "Epoch 7999 Train Loss 0.00010295625678972151 Test Loss nan\n",
            "Epoch 8499 Train Loss 7.760370500183763e-05 Test Loss nan\n",
            "Epoch 8999 Train Loss 5.8645065525021103e-05 Test Loss nan\n",
            "Epoch 9499 Train Loss 4.4407799890347315e-05 Test Loss nan\n",
            "Epoch 9999 Train Loss 3.36845194394233e-05 Test Loss nan\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_data, train_labels, test_data, test_labels, num_epochs = num_epochs, loss_target = None):\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=lr, weight_decay=wd, betas=betas\n",
        "    )\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    model_checkpoints = []\n",
        "    checkpoint_epochs = []\n",
        "    if TRAIN_MODEL:\n",
        "        for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "            train_logits = model(train_data)\n",
        "            train_loss = loss_fn(train_logits, train_labels)\n",
        "            train_loss.backward()\n",
        "            train_losses.append(train_loss.item())\n",
        "            if loss_target is not None and train_loss.item() < loss_target:\n",
        "                print(f\"Loss target {loss_target} reached with loss {train_loss.item()} at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                test_logits = model(test_data)\n",
        "                test_loss = loss_fn(test_logits, test_labels)\n",
        "                test_losses.append(test_loss.item())\n",
        "\n",
        "            if ((epoch+1)%checkpoint_every)==0:\n",
        "                checkpoint_epochs.append(epoch)\n",
        "                model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "                print_stats(model)\n",
        "                print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")\n",
        "    if TRAIN_MODEL:\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\":model.state_dict(),\n",
        "                \"config\": model.cfg,\n",
        "                \"checkpoints\": model_checkpoints,\n",
        "                \"checkpoint_epochs\": checkpoint_epochs,\n",
        "                \"test_losses\": test_losses,\n",
        "                \"train_losses\": train_losses,\n",
        "            },\n",
        "            PTH_LOCATION)\n",
        "\n",
        "train_model(\n",
        "    model, train_data, train_labels, test_data, test_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EzRKi7J7IZph"
      },
      "outputs": [],
      "source": [
        "if not TRAIN_MODEL:\n",
        "    cached_data = torch.load(PTH_LOCATION)\n",
        "    model.load_state_dict(cached_data['model'])\n",
        "    model_checkpoints = cached_data[\"checkpoints\"]\n",
        "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
        "    test_losses = cached_data['test_losses']\n",
        "    train_losses = cached_data['train_losses']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transposed Input:\n",
            " tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
            "        [0, 1, 2, 0, 1, 2, 0, 1, 2]])\n",
            "Labels:  tensor([0, 1, 2, 1, 1, 0, 2, 2, 0])\n",
            "Logits of last token:\n",
            " tensor([[ 10.4829,  -0.3025,  -2.2474],\n",
            "        [  6.5879,  17.4972, -25.4383],\n",
            "        [ 13.5779, -21.1048,  24.9873],\n",
            "        [ 11.3726,  29.8343, -42.1891],\n",
            "        [  6.9325,  32.8404, -45.7635],\n",
            "        [ 15.0433,  -1.8849,  -0.5669],\n",
            "        [  7.1881, -14.3199,  16.5910],\n",
            "        [  2.7490, -11.2691,  12.9574],\n",
            "        [ 13.9139,  -5.9930,   4.9690]], grad_fn=<SliceBackward0>)\n",
            "Unembed:\n",
            " tensor([[ 1.8803,  1.9334, -2.7273],\n",
            "        [-1.8391,  0.6233, -0.6581]])\n",
            "Last layer before unembed:\n",
            " tensor([[  1.4772,  -3.7749],\n",
            "        [  7.8878,   4.8970],\n",
            "        [ -6.2070, -13.3137],\n",
            "        [ 13.3178,   7.8469],\n",
            "        [ 13.9018,  10.8582],\n",
            "        [  1.4628,  -6.2691],\n",
            "        [ -4.4100,  -8.0023],\n",
            "        [ -3.8085,  -4.9736],\n",
            "        [ -0.2841,  -7.4411]])\n"
          ]
        }
      ],
      "source": [
        "def create_cache(model):\n",
        "    input = train_data\n",
        "    print(\"Transposed Input:\\n\", input.transpose(0, 1))\n",
        "    logits, cache = model.run_with_cache(input)\n",
        "    print(\"Labels: \", train_labels)\n",
        "    print(\"Logits of last token:\\n\", logits[:, -1, :])\n",
        "    print(\"Unembed:\\n\", model.W_U.data)\n",
        "    print(\"Last layer before unembed:\\n\", cache.cache_dict[\"blocks.0.hook_resid_post\"][:, -1, :])\n",
        "    return cache\n",
        "\n",
        "cache = create_cache(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "9.074821799999999"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "-1.3451 * -3.4785 + 0.7905 * 5.5609"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hook_embed': tensor([[[-1.9119, -1.4572],\n",
              "          [-1.9119, -1.4572]],\n",
              " \n",
              "         [[-1.9119, -1.4572],\n",
              "          [-1.3104,  1.5715]],\n",
              " \n",
              "         [[-1.9119, -1.4572],\n",
              "          [ 2.2279, -0.8822]],\n",
              " \n",
              "         [[-1.3104,  1.5715],\n",
              "          [-1.9119, -1.4572]],\n",
              " \n",
              "         [[-1.3104,  1.5715],\n",
              "          [-1.3104,  1.5715]],\n",
              " \n",
              "         [[-1.3104,  1.5715],\n",
              "          [ 2.2279, -0.8822]],\n",
              " \n",
              "         [[ 2.2279, -0.8822],\n",
              "          [-1.9119, -1.4572]],\n",
              " \n",
              "         [[ 2.2279, -0.8822],\n",
              "          [-1.3104,  1.5715]],\n",
              " \n",
              "         [[ 2.2279, -0.8822],\n",
              "          [ 2.2279, -0.8822]]]),\n",
              " 'hook_pos_embed': tensor([[[-0.3115,  0.2175],\n",
              "          [-0.7833, -1.3808]],\n",
              " \n",
              "         [[-0.3115,  0.2175],\n",
              "          [-0.7833, -1.3808]],\n",
              " \n",
              "         [[-0.3115,  0.2175],\n",
              "          [-0.7833, -1.3808]],\n",
              " \n",
              "         [[-0.3115,  0.2175],\n",
              "          [-0.7833, -1.3808]],\n",
              " \n",
              "         [[-0.3115,  0.2175],\n",
              "          [-0.7833, -1.3808]],\n",
              " \n",
              "         [[-0.3115,  0.2175],\n",
              "          [-0.7833, -1.3808]],\n",
              " \n",
              "         [[-0.3115,  0.2175],\n",
              "          [-0.7833, -1.3808]],\n",
              " \n",
              "         [[-0.3115,  0.2175],\n",
              "          [-0.7833, -1.3808]],\n",
              " \n",
              "         [[-0.3115,  0.2175],\n",
              "          [-0.7833, -1.3808]]]),\n",
              " 'blocks.0.hook_resid_pre': tensor([[[-2.2234, -1.2396],\n",
              "          [-2.6952, -2.8379]],\n",
              " \n",
              "         [[-2.2234, -1.2396],\n",
              "          [-2.0937,  0.1907]],\n",
              " \n",
              "         [[-2.2234, -1.2396],\n",
              "          [ 1.4446, -2.2630]],\n",
              " \n",
              "         [[-1.6220,  1.7890],\n",
              "          [-2.6952, -2.8379]],\n",
              " \n",
              "         [[-1.6220,  1.7890],\n",
              "          [-2.0937,  0.1907]],\n",
              " \n",
              "         [[-1.6220,  1.7890],\n",
              "          [ 1.4446, -2.2630]],\n",
              " \n",
              "         [[ 1.9163, -0.6647],\n",
              "          [-2.6952, -2.8379]],\n",
              " \n",
              "         [[ 1.9163, -0.6647],\n",
              "          [-2.0937,  0.1907]],\n",
              " \n",
              "         [[ 1.9163, -0.6647],\n",
              "          [ 1.4446, -2.2630]]]),\n",
              " 'blocks.0.attn.hook_q': tensor([[[[-3.9819,  3.6517]],\n",
              " \n",
              "          [[-6.0143,  5.8252]]],\n",
              " \n",
              " \n",
              "         [[[-3.9819,  3.6517]],\n",
              " \n",
              "          [[-2.4113,  1.8791]]],\n",
              " \n",
              " \n",
              "         [[[-3.9819,  3.6517]],\n",
              " \n",
              "          [[-1.9126,  2.7003]]],\n",
              " \n",
              " \n",
              "         [[[-0.3788, -0.2944]],\n",
              " \n",
              "          [[-6.0143,  5.8252]]],\n",
              " \n",
              " \n",
              "         [[[-0.3788, -0.2944]],\n",
              " \n",
              "          [[-2.4113,  1.8791]]],\n",
              " \n",
              " \n",
              "         [[[-0.3788, -0.2944]],\n",
              " \n",
              "          [[-1.9126,  2.7003]]],\n",
              " \n",
              " \n",
              "         [[[ 0.1198,  0.5267]],\n",
              " \n",
              "          [[-6.0143,  5.8252]]],\n",
              " \n",
              " \n",
              "         [[[ 0.1198,  0.5267]],\n",
              " \n",
              "          [[-2.4113,  1.8791]]],\n",
              " \n",
              " \n",
              "         [[[ 0.1198,  0.5267]],\n",
              " \n",
              "          [[-1.9126,  2.7003]]]]),\n",
              " 'blocks.0.attn.hook_k': tensor([[[[ 4.3516, -3.0603]],\n",
              " \n",
              "          [[ 6.4571, -4.7415]]],\n",
              " \n",
              " \n",
              "         [[[ 4.3516, -3.0603]],\n",
              " \n",
              "          [[ 2.8945, -1.8314]]],\n",
              " \n",
              " \n",
              "         [[[ 4.3516, -3.0603]],\n",
              " \n",
              "          [[-0.0997, -0.3949]]],\n",
              " \n",
              " \n",
              "         [[[ 0.7890, -0.1503]],\n",
              " \n",
              "          [[ 6.4571, -4.7415]]],\n",
              " \n",
              " \n",
              "         [[[ 0.7890, -0.1503]],\n",
              " \n",
              "          [[ 2.8945, -1.8314]]],\n",
              " \n",
              " \n",
              "         [[[ 0.7890, -0.1503]],\n",
              " \n",
              "          [[-0.0997, -0.3949]]],\n",
              " \n",
              " \n",
              "         [[[-2.2051,  1.2863]],\n",
              " \n",
              "          [[ 6.4571, -4.7415]]],\n",
              " \n",
              " \n",
              "         [[[-2.2051,  1.2863]],\n",
              " \n",
              "          [[ 2.8945, -1.8314]]],\n",
              " \n",
              " \n",
              "         [[[-2.2051,  1.2863]],\n",
              " \n",
              "          [[-0.0997, -0.3949]]]]),\n",
              " 'blocks.0.attn.hook_v': tensor([[[[ 0.5310,  2.2341]],\n",
              " \n",
              "          [[ 3.8352,  0.5333]]],\n",
              " \n",
              " \n",
              "         [[[ 0.5310,  2.2341]],\n",
              " \n",
              "          [[-2.6411,  4.0135]]],\n",
              " \n",
              " \n",
              "         [[[ 0.5310,  2.2341]],\n",
              " \n",
              "          [[ 5.5684, -2.3493]]],\n",
              " \n",
              " \n",
              "         [[[-5.9453,  5.7143]],\n",
              " \n",
              "          [[ 3.8352,  0.5333]]],\n",
              " \n",
              " \n",
              "         [[[-5.9453,  5.7143]],\n",
              " \n",
              "          [[-2.6411,  4.0135]]],\n",
              " \n",
              " \n",
              "         [[[-5.9453,  5.7143]],\n",
              " \n",
              "          [[ 5.5684, -2.3493]]],\n",
              " \n",
              " \n",
              "         [[[ 2.2643, -0.6485]],\n",
              " \n",
              "          [[ 3.8352,  0.5333]]],\n",
              " \n",
              " \n",
              "         [[[ 2.2643, -0.6485]],\n",
              " \n",
              "          [[-2.6411,  4.0135]]],\n",
              " \n",
              " \n",
              "         [[[ 2.2643, -0.6485]],\n",
              " \n",
              "          [[ 5.5684, -2.3493]]]]),\n",
              " 'blocks.0.attn.hook_attn_scores': tensor([[[[-20.1544,     -inf],\n",
              "           [-31.1117, -46.9904]]],\n",
              " \n",
              " \n",
              "         [[[-20.1544,     -inf],\n",
              "           [-11.4859,  -7.3687]]],\n",
              " \n",
              " \n",
              "         [[[-20.1544,     -inf],\n",
              "           [-11.7286,  -0.6192]]],\n",
              " \n",
              " \n",
              "         [[[ -0.1801,     -inf],\n",
              "           [ -3.9747, -46.9904]]],\n",
              " \n",
              " \n",
              "         [[[ -0.1801,     -inf],\n",
              "           [ -1.5450,  -7.3687]]],\n",
              " \n",
              " \n",
              "         [[[ -0.1801,     -inf],\n",
              "           [ -1.3541,  -0.6192]]],\n",
              " \n",
              " \n",
              "         [[[  0.2923,     -inf],\n",
              "           [ 14.6761, -46.9904]]],\n",
              " \n",
              " \n",
              "         [[[  0.2923,     -inf],\n",
              "           [  5.4689,  -7.3687]]],\n",
              " \n",
              " \n",
              "         [[[  0.2923,     -inf],\n",
              "           [  5.4383,  -0.6192]]]]),\n",
              " 'blocks.0.attn.hook_pattern': tensor([[[[1.0000e+00, 0.0000e+00],\n",
              "           [1.0000e+00, 1.2705e-07]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.6029e-02, 9.8397e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.4971e-05, 9.9998e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.0000e+00, 2.0821e-19]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [9.9705e-01, 2.9480e-03]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [3.2411e-01, 6.7589e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.0000e+00, 1.6542e-27]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.0000e+00, 2.6588e-06]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [9.9767e-01, 2.3349e-03]]]]),\n",
              " 'blocks.0.attn.hook_z': tensor([[[[ 0.5310,  2.2341]],\n",
              " \n",
              "          [[ 0.5310,  2.2341]]],\n",
              " \n",
              " \n",
              "         [[[ 0.5310,  2.2341]],\n",
              " \n",
              "          [[-2.5902,  3.9850]]],\n",
              " \n",
              " \n",
              "         [[[ 0.5310,  2.2341]],\n",
              " \n",
              "          [[ 5.5684, -2.3492]]],\n",
              " \n",
              " \n",
              "         [[[-5.9453,  5.7143]],\n",
              " \n",
              "          [[-5.9453,  5.7143]]],\n",
              " \n",
              " \n",
              "         [[[-5.9453,  5.7143]],\n",
              " \n",
              "          [[-5.9355,  5.7093]]],\n",
              " \n",
              " \n",
              "         [[[-5.9453,  5.7143]],\n",
              " \n",
              "          [[ 1.8367,  0.2642]]],\n",
              " \n",
              " \n",
              "         [[[ 2.2643, -0.6485]],\n",
              " \n",
              "          [[ 2.2643, -0.6485]]],\n",
              " \n",
              " \n",
              "         [[[ 2.2643, -0.6485]],\n",
              " \n",
              "          [[ 2.2643, -0.6485]]],\n",
              " \n",
              " \n",
              "         [[[ 2.2643, -0.6485]],\n",
              " \n",
              "          [[ 2.2720, -0.6525]]]]),\n",
              " 'blocks.0.hook_attn_out': tensor([[[  4.1723,  -0.9369],\n",
              "          [  4.1723,  -0.9369]],\n",
              " \n",
              "         [[  4.1723,  -0.9369],\n",
              "          [  9.9815,   4.7063]],\n",
              " \n",
              "         [[  4.1723,  -0.9369],\n",
              "          [ -7.6516, -11.0507]],\n",
              " \n",
              "         [[ 16.0130,  10.6848],\n",
              "          [ 16.0130,  10.6848]],\n",
              " \n",
              "         [[ 16.0130,  10.6848],\n",
              "          [ 15.9955,  10.6675]],\n",
              " \n",
              "         [[ 16.0130,  10.6848],\n",
              "          [  0.0183,  -4.0061]],\n",
              " \n",
              "         [[ -1.7149,  -5.1643],\n",
              "          [ -1.7149,  -5.1643]],\n",
              " \n",
              "         [[ -1.7149,  -5.1643],\n",
              "          [ -1.7148,  -5.1643]],\n",
              " \n",
              "         [[ -1.7149,  -5.1643],\n",
              "          [ -1.7287,  -5.1781]]]),\n",
              " 'blocks.0.hook_resid_post': tensor([[[  1.9489,  -2.1765],\n",
              "          [  1.4772,  -3.7749]],\n",
              " \n",
              "         [[  1.9489,  -2.1765],\n",
              "          [  7.8878,   4.8970]],\n",
              " \n",
              "         [[  1.9489,  -2.1765],\n",
              "          [ -6.2070, -13.3137]],\n",
              " \n",
              "         [[ 14.3910,  12.4738],\n",
              "          [ 13.3178,   7.8469]],\n",
              " \n",
              "         [[ 14.3910,  12.4738],\n",
              "          [ 13.9018,  10.8582]],\n",
              " \n",
              "         [[ 14.3910,  12.4738],\n",
              "          [  1.4628,  -6.2691]],\n",
              " \n",
              "         [[  0.2015,  -5.8290],\n",
              "          [ -4.4100,  -8.0023]],\n",
              " \n",
              "         [[  0.2015,  -5.8290],\n",
              "          [ -3.8085,  -4.9736]],\n",
              " \n",
              "         [[  0.2015,  -5.8290],\n",
              "          [ -0.2841,  -7.4411]]])}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a list of all activations stored in the cache, especially their names\n",
        "cache.cache_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Animations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "from manim import *\n",
        "\n",
        "config.media_width = \"80%\"\n",
        "config.verbosity = \"WARNING\"\n",
        "config.preview = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[tensor([ 0.8873, -1.3817])(#FFFFFF)], [tensor([ 2.7090, -0.1558])(#FFFFFF)], [tensor([6.0623, 5.5371])(#888888)]]\n"
          ]
        }
      ],
      "source": [
        "class VectorParams:\n",
        "    def __init__(self, values = [], color = WHITE, label = \"\"):\n",
        "        self.values = values\n",
        "        self.color = color\n",
        "        self.label = label\n",
        "    def __repr__(self) -> str:\n",
        "        return str(self.values) + \"(\" + str(self.color) + \")\"\n",
        "\n",
        "class Data:\n",
        "    def __init__(self):\n",
        "        self.vectors: list[list[VectorParams]] = [[]]\n",
        "        self.steps = 0\n",
        "        self.current_labels = set()\n",
        "\n",
        "    def add_vector(self, vector, color = WHITE, label = \"\"):\n",
        "        if label not in self.current_labels:\n",
        "            self.current_labels.add(label)\n",
        "            self.vectors[self.steps].append(VectorParams(values = vector, color = color, label = label))\n",
        "\n",
        "    def next_step(self):\n",
        "        self.steps += 1\n",
        "        self.vectors.append([])\n",
        "        self.current_labels = set()\n",
        "\n",
        "    def add_vectors_at_hook(self, c: ActivationCache, hook: str, color0 = WHITE, color1 = WHITE, input_labels = None, input_colors = None):\n",
        "        if input_labels is None:\n",
        "            input_labels = [\"\" for i in range(c.cache_dict[hook].shape[0])]\n",
        "        for i in range(c.cache_dict[hook].shape[0]):\n",
        "            self.add_vector(c.cache_dict[hook][i][0].cpu(), color = color0, label = input_labels[i][:1])\n",
        "            self.add_vector(\n",
        "                c.cache_dict[hook][i][1].cpu(), color=color1 if input_colors is None else input_colors[i], label=input_labels[i][:2]\n",
        "            )\n",
        "\n",
        "\n",
        "def compile_data_vectors(cache, input_labels=None, input_colors=None):\n",
        "    # Set default value as list of empty strings\n",
        "    vectors = Data()\n",
        "    vectors.add_vectors_at_hook(cache, \"hook_embed\", color1 = GRAY, input_labels=input_labels, input_colors=input_colors)\n",
        "    vectors.next_step()\n",
        "    vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_pre\", input_labels=input_labels, input_colors=input_colors)\n",
        "    vectors.next_step()\n",
        "    vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_post\", color0 = GRAY, input_labels=input_labels, input_colors=input_colors)\n",
        "    # vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_mid\")\n",
        "\n",
        "    print(vectors.vectors)\n",
        "    return vectors\n",
        "\n",
        "\n",
        "vectors = compile_data_vectors(cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def change_font_size(labeled_arrow: LabeledArrow, new_size):\n",
        "    # print(labeled_arrow, labeled_arrow.submobjects)\n",
        "    # print(labeled_arrow.submobjects[-1].font_size)\n",
        "    if not isinstance(labeled_arrow, LabeledArrow):\n",
        "        return\n",
        "    label = labeled_arrow.submobjects[-1]\n",
        "    box = labeled_arrow.submobjects[-2]\n",
        "    if not isinstance(box, BackgroundRectangle):\n",
        "        box = labeled_arrow.submobjects[-3]\n",
        "    coords = label.get_center()\n",
        "    # print(new_size)\n",
        "    labeled_arrow.submobjects[-1] = MathTex(\n",
        "        label.get_tex_string(), color=label.color, font_size=new_size\n",
        "    )\n",
        "    # print(\"size=\", labeled_arrow.submobjects[-1].font_size)\n",
        "    label = labeled_arrow.submobjects[-1]\n",
        "    label.move_to(coords)\n",
        "    box.width = label.width + 2 * box.buff\n",
        "    box.height = label.height + 2 * box.buff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "DOT_SCALE = 0.05\n",
        "class VisualizeTransformer(MovingCameraScene):\n",
        "    def construct(self):\n",
        "        print(\"v=\", vectors.vectors)\n",
        "        axes = Axes(\n",
        "            x_range = [-20, 20, 1],\n",
        "            y_range = [-20, 20, 1],\n",
        "            x_axis_config={\n",
        "                \"numbers_to_include\": np.arange(-18, 18.1, 3),\n",
        "                \"font_size\": 24\n",
        "            },\n",
        "            y_axis_config={\n",
        "                \"numbers_to_include\": np.arange(-18, 18.1, 3), \n",
        "                \"font_size\": 24            \n",
        "            },\n",
        "            x_length = 40,\n",
        "            y_length = 40,\n",
        "            axis_config={\"color\": GREEN}\n",
        "        )\n",
        "\n",
        "        scale = ValueTracker(2)\n",
        "\n",
        "        dots = VGroup()\n",
        "        def update_scale(self):\n",
        "            return\n",
        "            # TODO: Make the scaling nicer\n",
        "            self.stroke_width = 6 * scale.get_value()\n",
        "            change_font_size(self, 48 * scale.get_value())\n",
        "            # print(\"New font size: \", self.font_size)\n",
        "\n",
        "        # Embedding arrows\n",
        "        for i, t in enumerate(vectors.vectors[0]):\n",
        "            # print(t, t.numpy())\n",
        "            # arrow = LabeledArrow(\n",
        "            #     start=ORIGIN,\n",
        "            #     end=np.append(t.values.numpy(), 0),\n",
        "            #     buff = 0,\n",
        "            #     label = t.label,\n",
        "            #     label_frame = False,\n",
        "            #     label_color=YELLOW,\n",
        "            #     color = t.color,\n",
        "            #     max_stroke_width_to_length_ratio = 100,\n",
        "            # )\n",
        "\n",
        "            # arrow.add_updater(update_scale)\n",
        "            # arrows.add(arrow)\n",
        "            dot = LabeledDot(\n",
        "                point=np.append(t.values.numpy(), 0),\n",
        "                label=t.label,\n",
        "                color=t.color,\n",
        "                radius=DOT_SCALE * (len(vectors.vectors[0]) - i) + 0.2,\n",
        "            )\n",
        "\n",
        "            dot.add_updater(update_scale)\n",
        "            dots.add(dot)\n",
        "\n",
        "        # Transitioning the arrows through the model\n",
        "        self.add(axes, axes.get_axis_labels(), dots)\n",
        "        for step in range(1, len(vectors.vectors)):\n",
        "            new_dots = VGroup()\n",
        "            transition_arrows = VGroup()\n",
        "            for i, t in enumerate(vectors.vectors[step]):\n",
        "                # print(t, t.numpy())\n",
        "                # new_arrow = LabeledArrow(\n",
        "                #     start=ORIGIN,\n",
        "                #     end=np.append(t.values.numpy(), 0),\n",
        "                #     buff=0,\n",
        "                #     label=t.label,\n",
        "                #     label_frame=False,\n",
        "                #     label_color=YELLOW,\n",
        "                #     color=t.color,\n",
        "                #     max_stroke_width_to_length_ratio=100,\n",
        "                # )\n",
        "                # new_arrow.add_updater(update_scale)\n",
        "                # new_arrows.add(new_arrow)\n",
        "                new_dot = LabeledDot(\n",
        "                    point=np.append(t.values.numpy(), 0),\n",
        "                    label=t.label,\n",
        "                    color=t.color,\n",
        "                    radius = DOT_SCALE * (len(vectors.vectors[step]) - i) + 0.2,\n",
        "                )\n",
        "                new_dot.add_updater(update_scale)\n",
        "                new_dots.add(new_dot)\n",
        "\n",
        "                transition_arrow = Arrow(\n",
        "                    start=dots[i].arc_center,\n",
        "                    end=new_dots[i].arc_center,\n",
        "                    buff=0,\n",
        "                    color=RED,\n",
        "                )\n",
        "                transition_arrow.add_updater(update_scale)\n",
        "                transition_arrows.add(transition_arrow)\n",
        "\n",
        "            view = SurroundingRectangle(new_dots)\n",
        "            factor = max(\n",
        "                view.width / self.camera.frame_width,\n",
        "                view.height / self.camera.frame_height,\n",
        "            )\n",
        "            print(\n",
        "                factor,\n",
        "                self.camera.frame_width, view.width,\n",
        "                self.camera.frame_height, view.height,\n",
        "            )\n",
        "            self.wait()\n",
        "            self.play(FadeIn(transition_arrows), self.camera.auto_zoom(view, margin = 2), scale.animate.set_value(scale.get_value() * factor))\n",
        "            self.wait()\n",
        "            self.play(\n",
        "                ReplacementTransform(dots, new_dots)\n",
        "            )\n",
        "            self.wait()\n",
        "            self.play(FadeOut(transition_arrows))\n",
        "            self.wait()\n",
        "            dots = new_dots\n",
        "\n",
        "        # Unembedding Arrows\n",
        "        embedding_arrows = VGroup()\n",
        "        data = model.W_U.data\n",
        "        print(\"unembed: \", data)\n",
        "        for i in range(model.W_U.data.size()[1]):\n",
        "            embedding_arrow = LabeledArrow(\n",
        "                start=ORIGIN,\n",
        "                end=[data[0, i].item(), data[1, i].item(), 0],\n",
        "                label=str(i),\n",
        "                color=LIGHT_PINK,\n",
        "                buff=0,\n",
        "                max_stroke_width_to_length_ratio=100,\n",
        "            )\n",
        "            embedding_arrows.add(embedding_arrow)\n",
        "        self.play(FadeIn(embedding_arrows))\n",
        "        self.wait()\n",
        "\n",
        "# v = VisualizeTransformer()\n",
        "# v.construct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LabeledDot"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dot = LabeledDot(label=\"A\")\n",
        "dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [0, 3],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [1, 3],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2],\n",
            "        [2, 3],\n",
            "        [3, 0],\n",
            "        [3, 1],\n",
            "        [3, 2],\n",
            "        [3, 3]])\n",
            "tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
            "torch.Size([16, 2])\n",
            "tensor([], size=(0, 2), dtype=torch.int64)\n",
            "tensor([], dtype=torch.int64)\n",
            "torch.Size([0, 2])\n",
            "Transposed Input:\n",
            " tensor([[0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3],\n",
            "        [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]])\n",
            "Labels:  tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0])\n",
            "Logits of last token:\n",
            " tensor([[  0.5407,  -0.1525],\n",
            "        [  0.5307,  -0.1625],\n",
            "        [ -7.3317,   7.2997],\n",
            "        [-10.1159,  11.8442],\n",
            "        [  0.5406,  -0.1524],\n",
            "        [  0.5307,  -0.1625],\n",
            "        [ -7.0416,   6.9686],\n",
            "        [-10.5721,  12.3541],\n",
            "        [ -5.5939,   6.6703],\n",
            "        [ -5.5159,   6.5641],\n",
            "        [  4.4402,  -6.0507],\n",
            "        [ -9.5549,  11.0981],\n",
            "        [  0.5407,  -0.1525],\n",
            "        [  0.5307,  -0.1625],\n",
            "        [  4.8539,  -6.2788],\n",
            "        [ 10.6668, -11.4587]], grad_fn=<SliceBackward0>)\n",
            "Unembed:\n",
            " tensor([[-0.3928, -0.0660],\n",
            "        [ 1.2664, -1.6290]])\n",
            "Last layer before unembed:\n",
            " tensor([[-0.8950,  0.0618],\n",
            "        [-0.8550,  0.0663],\n",
            "        [ 3.7860, -4.7025],\n",
            "        [ 2.0998, -7.4240],\n",
            "        [-0.8949,  0.0618],\n",
            "        [-0.8550,  0.0663],\n",
            "        [ 3.7122, -4.4963],\n",
            "        [ 2.2346, -7.7424],\n",
            "        [ 0.9747, -4.2023],\n",
            "        [ 0.9851, -4.1375],\n",
            "        [ 0.6494,  3.6200],\n",
            "        [ 2.1428, -6.9677],\n",
            "        [-0.8950,  0.0618],\n",
            "        [-0.8550,  0.0663],\n",
            "        [ 0.1171,  3.7816],\n",
            "        [-3.9041,  7.1243]])\n",
            "[[tensor([-1.3298,  1.1375])(#FFFFFF), tensor([-1.3298,  1.1375])(#58C4DD), tensor([-1.3044,  1.1679])(#58C4DD), tensor([1.0662, 2.0783])(#FFFF00), tensor([-0.1918, -1.5593])(#FFFF00), tensor([-1.3044,  1.1679])(#FFFFFF), tensor([-1.3298,  1.1375])(#58C4DD), tensor([-1.3044,  1.1679])(#FFFF00), tensor([1.0662, 2.0783])(#FFFF00), tensor([-0.1918, -1.5593])(#FFFF00), tensor([1.0662, 2.0783])(#FFFFFF), tensor([-1.3298,  1.1375])(#FFFF00), tensor([-1.3044,  1.1679])(#FFFF00), tensor([1.0662, 2.0783])(#58C4DD), tensor([-0.1918, -1.5593])(#FFFF00), tensor([-0.1918, -1.5593])(#FFFFFF), tensor([-1.3298,  1.1375])(#FFFF00), tensor([-1.3044,  1.1679])(#58C4DD), tensor([1.0662, 2.0783])(#58C4DD), tensor([-0.1918, -1.5593])(#58C4DD)], [tensor([-3.6370,  0.9759])(#FFFFFF), tensor([-1.2176,  0.3486])(#58C4DD), tensor([-1.1922,  0.3789])(#58C4DD), tensor([1.1784, 1.2893])(#FFFF00), tensor([-0.0796, -2.3483])(#FFFF00), tensor([-3.6116,  1.0062])(#FFFFFF), tensor([-1.2176,  0.3486])(#58C4DD), tensor([-1.1922,  0.3789])(#FFFF00), tensor([1.1784, 1.2893])(#FFFF00), tensor([-0.0796, -2.3483])(#FFFF00), tensor([-1.2410,  1.9166])(#FFFFFF), tensor([-1.2176,  0.3486])(#FFFF00), tensor([-1.1922,  0.3789])(#FFFF00), tensor([1.1784, 1.2893])(#58C4DD), tensor([-0.0796, -2.3483])(#FFFF00), tensor([-2.4990, -1.7210])(#FFFFFF), tensor([-1.2176,  0.3486])(#FFFF00), tensor([-1.1922,  0.3789])(#58C4DD), tensor([1.1784, 1.2893])(#58C4DD), tensor([-0.0796, -2.3483])(#58C4DD)], [tensor([-0.4908, -6.4436])(#888888), tensor([-0.8950,  0.0618])(#58C4DD), tensor([-0.8550,  0.0663])(#58C4DD), tensor([ 3.7860, -4.7025])(#FFFF00), tensor([ 2.0998, -7.4240])(#FFFF00), tensor([-0.4508, -6.4391])(#888888), tensor([-0.8949,  0.0618])(#58C4DD), tensor([-0.8550,  0.0663])(#FFFF00), tensor([ 3.7122, -4.4963])(#FFFF00), tensor([ 2.2346, -7.7424])(#FFFF00), tensor([ 0.9814, -2.7032])(#888888), tensor([ 0.9747, -4.2023])(#FFFF00), tensor([ 0.9851, -4.1375])(#FFFF00), tensor([0.6494, 3.6200])(#58C4DD), tensor([ 2.1428, -6.9677])(#FFFF00), tensor([-3.5603,  0.7713])(#888888), tensor([-0.8950,  0.0618])(#FFFF00), tensor([-0.8550,  0.0663])(#58C4DD), tensor([0.1171, 3.7816])(#58C4DD), tensor([-3.9041,  7.1243])(#58C4DD)]]\n",
            "Labels:  ['00', '01', '02', '03', '10', '11', '12', '13', '20', '21', '22', '23', '30', '31', '32', '33']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Manim Community <span style=\"color: #008000; text-decoration-color: #008000\">v0.18.1</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "Manim Community \u001b[32mv0.\u001b[0m\u001b[32m18.1\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "v= [[tensor([-1.3298,  1.1375])(#FFFFFF), tensor([-1.3298,  1.1375])(#58C4DD), tensor([-1.3044,  1.1679])(#58C4DD), tensor([1.0662, 2.0783])(#FFFF00), tensor([-0.1918, -1.5593])(#FFFF00), tensor([-1.3044,  1.1679])(#FFFFFF), tensor([-1.3298,  1.1375])(#58C4DD), tensor([-1.3044,  1.1679])(#FFFF00), tensor([1.0662, 2.0783])(#FFFF00), tensor([-0.1918, -1.5593])(#FFFF00), tensor([1.0662, 2.0783])(#FFFFFF), tensor([-1.3298,  1.1375])(#FFFF00), tensor([-1.3044,  1.1679])(#FFFF00), tensor([1.0662, 2.0783])(#58C4DD), tensor([-0.1918, -1.5593])(#FFFF00), tensor([-0.1918, -1.5593])(#FFFFFF), tensor([-1.3298,  1.1375])(#FFFF00), tensor([-1.3044,  1.1679])(#58C4DD), tensor([1.0662, 2.0783])(#58C4DD), tensor([-0.1918, -1.5593])(#58C4DD)], [tensor([-3.6370,  0.9759])(#FFFFFF), tensor([-1.2176,  0.3486])(#58C4DD), tensor([-1.1922,  0.3789])(#58C4DD), tensor([1.1784, 1.2893])(#FFFF00), tensor([-0.0796, -2.3483])(#FFFF00), tensor([-3.6116,  1.0062])(#FFFFFF), tensor([-1.2176,  0.3486])(#58C4DD), tensor([-1.1922,  0.3789])(#FFFF00), tensor([1.1784, 1.2893])(#FFFF00), tensor([-0.0796, -2.3483])(#FFFF00), tensor([-1.2410,  1.9166])(#FFFFFF), tensor([-1.2176,  0.3486])(#FFFF00), tensor([-1.1922,  0.3789])(#FFFF00), tensor([1.1784, 1.2893])(#58C4DD), tensor([-0.0796, -2.3483])(#FFFF00), tensor([-2.4990, -1.7210])(#FFFFFF), tensor([-1.2176,  0.3486])(#FFFF00), tensor([-1.1922,  0.3789])(#58C4DD), tensor([1.1784, 1.2893])(#58C4DD), tensor([-0.0796, -2.3483])(#58C4DD)], [tensor([-0.4908, -6.4436])(#888888), tensor([-0.8950,  0.0618])(#58C4DD), tensor([-0.8550,  0.0663])(#58C4DD), tensor([ 3.7860, -4.7025])(#FFFF00), tensor([ 2.0998, -7.4240])(#FFFF00), tensor([-0.4508, -6.4391])(#888888), tensor([-0.8949,  0.0618])(#58C4DD), tensor([-0.8550,  0.0663])(#FFFF00), tensor([ 3.7122, -4.4963])(#FFFF00), tensor([ 2.2346, -7.7424])(#FFFF00), tensor([ 0.9814, -2.7032])(#888888), tensor([ 0.9747, -4.2023])(#FFFF00), tensor([ 0.9851, -4.1375])(#FFFF00), tensor([0.6494, 3.6200])(#58C4DD), tensor([ 2.1428, -6.9677])(#FFFF00), tensor([-3.5603,  0.7713])(#888888), tensor([-0.8950,  0.0618])(#FFFF00), tensor([-0.8550,  0.0663])(#58C4DD), tensor([0.1171, 3.7816])(#58C4DD), tensor([-3.9041,  7.1243])(#58C4DD)]]\n",
            "0.77060926258564 14.222222222222221 7.265423130989076 8.0 6.16487410068512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                            \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.967785768951384 14.515331734551324 9.190085887908936 8.16487410068512 16.06672306060791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                             \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unembed:  tensor([[-0.3928, -0.0660],\n",
            "        [ 1.2664, -1.6290]])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                              \r"
          ]
        },
        {
          "data": {
            "text/html": [
              "<video src=\"media/jupyter/Video@2025-01-15@14-04-11.mp4\" controls autoplay loop style=\"max-width: 80%;\"  >\n",
              "      Your browser does not support the <code>video</code> element.\n",
              "    </video>"
            ],
            "text/plain": [
              "<IPython.core.display.Video object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%manim -qh Video\n",
        "\n",
        "input_dim = 4\n",
        "output_dim = 2\n",
        "train_data, train_labels, test_data, test_labels = get_training_data(input_dim, output_dim, data_seed=999)\n",
        "# model = get_seeded_model(998, input_dim, output_dim)\n",
        "# train_model(model, train_data, train_labels, test_data, test_labels, num_epochs = 10000, loss_target = 1/(input_dim ** 2 * 4))\n",
        "cache = create_cache(model)\n",
        "arrow_labels = [\"\".join([str(d.item()) for d in v]) for v in train_data]\n",
        "colors = [BLUE, YELLOW, GREEN, RED]\n",
        "arrow_colors = [colors[l] for l in train_labels]\n",
        "vectors = compile_data_vectors(cache, input_labels=arrow_labels, input_colors = arrow_colors)\n",
        "print(\"Labels: \", arrow_labels)\n",
        "\n",
        "class Video(VisualizeTransformer):\n",
        "    def construct(self):\n",
        "        VisualizeTransformer.construct(self)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.9119, -1.8526, -0.0829],\n",
              "        [ 0.0683, -2.1787,  2.0396]])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.W_U.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[tensor([ 0.8166, -1.0588])(#FFFFFF),\n",
              "  tensor([ 0.8166, -1.0588])(#58C4DD),\n",
              "  tensor([ 0.8166, -1.0588])(#FFFFFF),\n",
              "  tensor([-1.3558, -1.9105])(#FFFF00),\n",
              "  tensor([ 0.8166, -1.0588])(#FFFFFF),\n",
              "  tensor([0.8130, 2.3552])(#83C167),\n",
              "  tensor([-1.3558, -1.9105])(#FFFFFF),\n",
              "  tensor([ 0.8166, -1.0588])(#FFFF00),\n",
              "  tensor([-1.3558, -1.9105])(#FFFFFF),\n",
              "  tensor([-1.3558, -1.9105])(#FFFF00),\n",
              "  tensor([-1.3558, -1.9105])(#FFFFFF),\n",
              "  tensor([0.8130, 2.3552])(#58C4DD),\n",
              "  tensor([0.8130, 2.3552])(#FFFFFF),\n",
              "  tensor([ 0.8166, -1.0588])(#83C167),\n",
              "  tensor([0.8130, 2.3552])(#FFFFFF),\n",
              "  tensor([-1.3558, -1.9105])(#83C167),\n",
              "  tensor([0.8130, 2.3552])(#FFFFFF),\n",
              "  tensor([0.8130, 2.3552])(#58C4DD)],\n",
              " [tensor([ 2.1039, -0.3284])(#FFFFFF),\n",
              "  tensor([-0.4866, -1.6315])(#58C4DD),\n",
              "  tensor([ 2.1039, -0.3284])(#FFFFFF),\n",
              "  tensor([-2.6590, -2.4832])(#FFFF00),\n",
              "  tensor([ 2.1039, -0.3284])(#FFFFFF),\n",
              "  tensor([-0.4902,  1.7825])(#83C167),\n",
              "  tensor([-0.0685, -1.1802])(#FFFFFF),\n",
              "  tensor([-0.4866, -1.6315])(#FFFF00),\n",
              "  tensor([-0.0685, -1.1802])(#FFFFFF),\n",
              "  tensor([-2.6590, -2.4832])(#FFFF00),\n",
              "  tensor([-0.0685, -1.1802])(#FFFFFF),\n",
              "  tensor([-0.4902,  1.7825])(#58C4DD),\n",
              "  tensor([2.1003, 3.0855])(#FFFFFF),\n",
              "  tensor([-0.4866, -1.6315])(#83C167),\n",
              "  tensor([2.1003, 3.0855])(#FFFFFF),\n",
              "  tensor([-2.6590, -2.4832])(#83C167),\n",
              "  tensor([2.1003, 3.0855])(#FFFFFF),\n",
              "  tensor([-0.4902,  1.7825])(#58C4DD)],\n",
              " [tensor([3.5851, 1.6986])(#888888),\n",
              "  tensor([ 1.2797, -0.5796])(#58C4DD),\n",
              "  tensor([3.5851, 1.6986])(#888888),\n",
              "  tensor([-1.1777, -0.4565])(#FFFF00),\n",
              "  tensor([3.5851, 1.6986])(#888888),\n",
              "  tensor([0.9911, 3.8093])(#83C167),\n",
              "  tensor([ 3.7505, -6.8687])(#888888),\n",
              "  tensor([ 3.4976, -7.9777])(#FFFF00),\n",
              "  tensor([ 3.7505, -6.8687])(#888888),\n",
              "  tensor([ 1.1635, -8.1835])(#FFFF00),\n",
              "  tensor([ 3.7505, -6.8687])(#888888),\n",
              "  tensor([ 3.3288, -3.9060])(#58C4DD),\n",
              "  tensor([ 2.9679, 11.1980])(#888888),\n",
              "  tensor([0.3834, 6.4700])(#83C167),\n",
              "  tensor([ 2.9679, 11.1980])(#888888),\n",
              "  tensor([-1.7914,  5.6292])(#83C167),\n",
              "  tensor([ 2.9679, 11.1980])(#888888),\n",
              "  tensor([3.2160, 0.1849])(#58C4DD)]]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectors.vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8859a5491331dba93123a91c2831400aced845b502848170e05fcb48b2c144be"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
