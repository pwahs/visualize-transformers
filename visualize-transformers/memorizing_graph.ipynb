{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Interpreting Identity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBvIU6zdIZpa"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ssx2pnTOIZpa"
      },
      "outputs": [],
      "source": [
        "TRAIN_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Jfz-lAWIZpb",
        "outputId": "48df1c71-5bc3-4a14-9a34-1af5c16786a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (25.1.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_66374/1990858556.py:9: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"load_ext autoreload\")\n",
            "/tmp/ipykernel_66374/1990858556.py:10: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"autoreload 2\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ipympl\n",
            "  Downloading ipympl-0.9.7-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: ipython<10 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipympl) (8.36.0)\n",
            "Collecting ipywidgets<9,>=7.6.0 (from ipympl)\n",
            "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting matplotlib<4,>=3.5.0 (from ipympl)\n",
            "  Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy (from ipympl)\n",
            "  Downloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting pillow (from ipympl)\n",
            "  Downloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: traitlets<6 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipympl) (5.14.3)\n",
            "Requirement already satisfied: decorator in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipython<10->ipympl) (5.2.1)\n",
            "Requirement already satisfied: exceptiongroup in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipython<10->ipympl) (1.3.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipython<10->ipympl) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipython<10->ipympl) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipython<10->ipympl) (4.9.0)\n",
            "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipython<10->ipympl) (3.0.51)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipython<10->ipympl) (2.19.1)\n",
            "Requirement already satisfied: stack_data in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipython<10->ipympl) (0.6.3)\n",
            "Requirement already satisfied: typing_extensions>=4.6 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipython<10->ipympl) (4.13.2)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.2)\n",
            "Collecting widgetsnbextension~=4.0.14 (from ipywidgets<9,>=7.6.0->ipympl)\n",
            "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets<9,>=7.6.0->ipympl)\n",
            "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib<4,>=3.5.0->ipympl)\n",
            "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib<4,>=3.5.0->ipympl)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib<4,>=3.5.0->ipympl)\n",
            "  Downloading fonttools-4.58.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (106 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib<4,>=3.5.0->ipympl)\n",
            "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from matplotlib<4,>=3.5.0->ipympl) (25.0)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib<4,>=3.5.0->ipympl)\n",
            "  Downloading pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from matplotlib<4,>=3.5.0->ipympl) (2.9.0.post0)\n",
            "Requirement already satisfied: wcwidth in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython<10->ipympl) (0.2.13)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython<10->ipympl) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython<10->ipympl) (0.7.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib<4,>=3.5.0->ipympl) (1.17.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from stack_data->ipython<10->ipympl) (2.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from stack_data->ipython<10->ipympl) (3.0.0)\n",
            "Requirement already satisfied: pure-eval in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from stack_data->ipython<10->ipympl) (0.2.3)\n",
            "Downloading ipympl-0.9.7-py3-none-any.whl (515 kB)\n",
            "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
            "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
            "Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.58.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "Installing collected packages: widgetsnbextension, pyparsing, pillow, numpy, kiwisolver, jupyterlab_widgets, fonttools, cycler, contourpy, matplotlib, ipywidgets, ipympl\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12/12\u001b[0m [ipympl] 9/12\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.1 ipympl-0.9.7 ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 kiwisolver-1.4.8 matplotlib-3.10.3 numpy-2.2.6 pillow-11.2.1 pyparsing-3.2.3 widgetsnbextension-4.0.14\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from scipy) (2.2.6)\n",
            "Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "Successfully installed scipy-1.15.3\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting manim\n",
            "  Downloading manim-0.19.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from manim) (11.2.1)\n",
            "Requirement already satisfied: Pygments>=2.0.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from manim) (2.19.1)\n",
            "Collecting av<14.0.0,>=9.0.0 (from manim)\n",
            "  Downloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting beautifulsoup4>=4.12 (from manim)\n",
            "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting click>=8.0 (from manim)\n",
            "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting cloup>=2.0.0 (from manim)\n",
            "  Downloading cloup-3.0.7-py2.py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: decorator>=4.3.2 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from manim) (5.2.1)\n",
            "Collecting isosurfaces>=0.1.0 (from manim)\n",
            "  Using cached isosurfaces-0.1.2-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting manimpango<1.0.0,>=0.5.0 (from manim)\n",
            "  Using cached ManimPango-0.6.0-cp310-cp310-linux_x86_64.whl\n",
            "Collecting mapbox-earcut>=1.0.0 (from manim)\n",
            "  Using cached mapbox_earcut-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting moderngl<6.0.0,>=5.0.0 (from manim)\n",
            "  Using cached moderngl-5.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting moderngl-window>=2.0.0 (from manim)\n",
            "  Downloading moderngl_window-3.1.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting networkx>=2.6 (from manim)\n",
            "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numpy>=2.1 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from manim) (2.2.6)\n",
            "Collecting pycairo<2.0.0,>=1.13 (from manim)\n",
            "  Downloading pycairo-1.28.0.tar.gz (662 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m662.5/662.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting pydub>=0.20.0 (from manim)\n",
            "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting rich>=12.0.0 (from manim)\n",
            "  Downloading rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from manim) (1.15.3)\n",
            "Collecting screeninfo>=0.7 (from manim)\n",
            "  Using cached screeninfo-0.8.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting skia-pathops>=0.7.0 (from manim)\n",
            "  Using cached skia_pathops-0.8.0.post2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting srt>=3.0.0 (from manim)\n",
            "  Using cached srt-3.5.3-py3-none-any.whl\n",
            "Collecting svgelements>=1.8.0 (from manim)\n",
            "  Using cached svgelements-1.9.6-py2.py3-none-any.whl.metadata (44 kB)\n",
            "Collecting tqdm>=4.0.0 (from manim)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from manim) (4.13.2)\n",
            "Collecting watchdog>=2.0.0 (from manim)\n",
            "  Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Collecting glcontext>=3.0.0 (from moderngl<6.0.0,>=5.0.0->manim)\n",
            "  Using cached glcontext-3.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
            "Collecting soupsieve>1.2 (from beautifulsoup4>=4.12->manim)\n",
            "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pyglet>=2.0.0 (from moderngl-window>=2.0.0->manim)\n",
            "  Downloading pyglet-2.1.6-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting pyglm<3,>=2.7.0 (from moderngl-window>=2.0.0->manim)\n",
            "  Downloading pyglm-2.8.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=12.0.0->manim)\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=12.0.0->manim)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading manim-0.19.0-py3-none-any.whl (625 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m625.9/625.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-13.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hUsing cached moderngl-5.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n",
            "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
            "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
            "Downloading cloup-3.0.7-py2.py3-none-any.whl (54 kB)\n",
            "Using cached glcontext-3.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51 kB)\n",
            "Using cached isosurfaces-0.1.2-py3-none-any.whl (11 kB)\n",
            "Using cached mapbox_earcut-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (95 kB)\n",
            "Downloading moderngl_window-3.1.1-py3-none-any.whl (382 kB)\n",
            "Downloading pyglm-2.8.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading pyglet-2.1.6-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m984.0/984.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rich-14.0.0-py3-none-any.whl (243 kB)\n",
            "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Using cached screeninfo-0.8.1-py3-none-any.whl (12 kB)\n",
            "Using cached skia_pathops-0.8.0.post2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
            "Using cached svgelements-1.9.6-py2.py3-none-any.whl (137 kB)\n",
            "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "Building wheels for collected packages: pycairo\n",
            "  Building wheel for pycairo (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for pycairo: filename=pycairo-1.28.0-cp310-cp310-linux_x86_64.whl size=129008 sha256=0d6e1e09c9c5a9c5f59d15a1a4383db6fe4f5a00ece6f8876699199f7e889fac\n",
            "  Stored in directory: /home/philippw/.cache/pip/wheels/9e/f9/1a/21072105191b3a713c81dd89521caa2323e7566839ec4a1829\n",
            "Successfully built pycairo\n",
            "Installing collected packages: svgelements, pyglm, pydub, glcontext, watchdog, tqdm, srt, soupsieve, skia-pathops, screeninfo, pyglet, pycairo, networkx, moderngl, mdurl, mapbox-earcut, manimpango, isosurfaces, click, av, moderngl-window, markdown-it-py, cloup, beautifulsoup4, rich, manim\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26/26\u001b[0m [manim]m25/26\u001b[0m [manim]gl-window]\n",
            "\u001b[1A\u001b[2KSuccessfully installed av-13.1.0 beautifulsoup4-4.13.4 click-8.2.1 cloup-3.0.7 glcontext-3.0.0 isosurfaces-0.1.2 manim-0.19.0 manimpango-0.6.0 mapbox-earcut-1.0.3 markdown-it-py-3.0.0 mdurl-0.1.2 moderngl-5.12.0 moderngl-window-3.1.1 networkx-3.4.2 pycairo-1.28.0 pydub-0.25.1 pyglet-2.1.6 pyglm-2.8.2 rich-14.0.0 screeninfo-0.8.1 skia-pathops-0.8.0.post2 soupsieve-2.7 srt-3.5.3 svgelements-1.9.6 tqdm-4.67.1 watchdog-6.0.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting torch\n",
            "  Downloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting filelock (from torch)\n",
            "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch) (3.4.2)\n",
            "Collecting jinja2 (from torch)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.0 (from torch)\n",
            "  Downloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from triton==3.3.0->torch) (59.6.0)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
            "  Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Downloading torch-2.7.0-cp310-cp310-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.3.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hUsing cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, MarkupSafe, fsspec, filelock, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/22\u001b[0m [torch]m21/22\u001b[0m [torch]-cusolver-cu12]2]\n",
            "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 filelock-3.18.0 fsspec-2025.5.1 jinja2-3.1.6 mpmath-1.3.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.0 triton-3.3.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "/bin/bash: Zeile 1: 2: Datei oder Verzeichnis nicht gefunden\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting einops\n",
            "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Installing collected packages: einops\n",
            "Successfully installed einops-0.8.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Collecting transformer_lens\n",
            "  Downloading transformer_lens-2.15.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting accelerate>=0.23.0 (from transformer_lens)\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer_lens)\n",
            "  Using cached beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer_lens)\n",
            "  Using cached better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting datasets>=2.7.1 (from transformer_lens)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from transformer_lens) (0.8.1)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer_lens)\n",
            "  Using cached fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer_lens)\n",
            "  Downloading jaxtyping-0.3.2-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from transformer_lens) (2.2.6)\n",
            "Collecting pandas>=1.1.5 (from transformer_lens)\n",
            "  Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Requirement already satisfied: rich>=12.6.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from transformer_lens) (14.0.0)\n",
            "Collecting sentencepiece (from transformer_lens)\n",
            "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: torch>=2.2 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from transformer_lens) (2.7.0)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from transformer_lens) (4.67.1)\n",
            "Collecting transformers>=4.43 (from transformer_lens)\n",
            "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
            "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer_lens)\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting typeguard<5.0,>=4.2 (from transformer_lens)\n",
            "  Downloading typeguard-4.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from transformer_lens) (4.13.2)\n",
            "Collecting wandb>=0.13.5 (from transformer_lens)\n",
            "  Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (25.0)\n",
            "Requirement already satisfied: psutil in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from accelerate>=0.23.0->transformer_lens) (7.0.0)\n",
            "Collecting pyyaml (from accelerate>=0.23.0->transformer_lens)\n",
            "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting huggingface-hub>=0.21.0 (from accelerate>=0.23.0->transformer_lens)\n",
            "  Downloading huggingface_hub-0.32.3-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting safetensors>=0.4.3 (from accelerate>=0.23.0->transformer_lens)\n",
            "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: filelock in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from datasets>=2.7.1->transformer_lens) (3.18.0)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.7.1->transformer_lens)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting requests>=2.32.2 (from datasets>=2.7.1->transformer_lens)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting xxhash (from datasets>=2.7.1->transformer_lens)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=2.7.1->transformer_lens)\n",
            "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading aiohttp-3.12.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting attrs>=17.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading multidict-6.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
            "Collecting idna>=2.0 (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub>=0.21.0->accelerate>=0.23.0->transformer_lens)\n",
            "  Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer_lens)\n",
            "  Downloading wadler_lindig-0.1.6-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas>=1.1.5->transformer_lens)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas>=1.1.5->transformer_lens)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens) (1.17.0)\n",
            "Collecting charset-normalizer<4,>=2 (from requests>=2.32.2->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests>=2.32.2->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests>=2.32.2->datasets>=2.7.1->transformer_lens)\n",
            "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from rich>=12.6.0->transformer_lens) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (1.14.0)\n",
            "Requirement already satisfied: networkx in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from torch>=2.2->transformer_lens) (3.3.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from triton==3.3.0->torch>=2.2->transformer_lens) (59.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.2->transformer_lens) (1.3.0)\n",
            "Collecting regex!=2019.12.17 (from transformers>=4.43->transformer_lens)\n",
            "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers>=4.43->transformer_lens)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (8.2.1)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: platformdirs in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from wandb>=0.13.5->transformer_lens) (4.3.8)\n",
            "Collecting protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting pydantic<3 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading pydantic-2.11.5-py3-none-any.whl.metadata (67 kB)\n",
            "Collecting sentry-sdk>=2.0.0 (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading sentry_sdk-2.29.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer_lens)\n",
            "  Downloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3->wandb>=0.13.5->transformer_lens)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3->wandb>=0.13.5->transformer_lens)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens)\n",
            "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/philippw/Stuff/programming/mechint/visualize_transformers/visualize-transformers/.venv/lib/python3.10/site-packages (from jinja2->torch>=2.2->transformer_lens) (3.0.2)\n",
            "Downloading transformer_lens-2.15.4-py3-none-any.whl (189 kB)\n",
            "Using cached beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "Using cached better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Downloading typeguard-4.4.2-py3-none-any.whl (35 kB)\n",
            "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "Downloading aiohttp-3.12.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading multidict-6.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
            "Downloading yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
            "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
            "Using cached fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "Downloading huggingface_hub-0.32.3-py3-none-any.whl (512 kB)\n",
            "Downloading hf_xet-1.1.2-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading jaxtyping-0.3.2-py3-none-any.whl (55 kB)\n",
            "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
            "Downloading pyarrow-20.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
            "Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
            "Downloading transformers-4.52.3-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "Downloading wadler_lindig-0.1.6-py3-none-any.whl (20 kB)\n",
            "Downloading wandb-0.19.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.31.1-cp39-abi3-manylinux2014_x86_64.whl (321 kB)\n",
            "Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\n",
            "Downloading pydantic_core-2.33.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
            "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Downloading sentry_sdk-2.29.1-py2.py3-none-any.whl (341 kB)\n",
            "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setproctitle-1.3.6-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "Building wheels for collected packages: transformers-stream-generator\n",
            "  Building wheel for transformers-stream-generator (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for transformers-stream-generator: filename=transformers_stream_generator-0.0.5-py3-none-any.whl size=12525 sha256=bb7ee3a2d614fb010d9b378c553f10e15687f831620e44bb4aed103415463e5c\n",
            "  Stored in directory: /home/philippw/.cache/pip/wheels/95/4a/90/140f7b67d125906f6a165f38aad212ecb4a695ad0d87582437\n",
            "Successfully built transformers-stream-generator\n",
            "Installing collected packages: sentencepiece, pytz, better-abc, xxhash, wadler-lindig, urllib3, tzdata, typing-inspection, typeguard, smmap, setproctitle, safetensors, regex, pyyaml, pydantic-core, pyarrow, protobuf, propcache, multidict, idna, hf-xet, fsspec, frozenlist, fancy-einsum, docker-pycreds, dill, charset-normalizer, certifi, beartype, attrs, async-timeout, annotated-types, aiohappyeyeballs, yarl, sentry-sdk, requests, pydantic, pandas, multiprocess, jaxtyping, gitdb, aiosignal, huggingface-hub, gitpython, aiohttp, wandb, tokenizers, accelerate, transformers, datasets, transformers-stream-generator, transformer_lens\n",
            "\u001b[2K  Attempting uninstall: fsspec0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/52\u001b[0m [hf-xet]f]\n",
            "\u001b[2K    Found existing installation: fsspec 2025.5.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/52\u001b[0m [hf-xet]\n",
            "\u001b[2K    Uninstalling fsspec-2025.5.1:\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/52\u001b[0m [hf-xet]\n",
            "\u001b[2K      Successfully uninstalled fsspec-2025.5.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/52\u001b[0m [hf-xet]\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52/52\u001b[0m [transformer_lens]transformers-stream-generator]\n",
            "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.4 aiosignal-1.3.2 annotated-types-0.7.0 async-timeout-5.0.1 attrs-25.3.0 beartype-0.14.1 better-abc-0.0.3 certifi-2025.4.26 charset-normalizer-3.4.2 datasets-3.6.0 dill-0.3.8 docker-pycreds-0.4.0 fancy-einsum-0.0.3 frozenlist-1.6.0 fsspec-2025.3.0 gitdb-4.0.12 gitpython-3.1.44 hf-xet-1.1.2 huggingface-hub-0.32.3 idna-3.10 jaxtyping-0.3.2 multidict-6.4.4 multiprocess-0.70.16 pandas-2.2.3 propcache-0.3.1 protobuf-6.31.1 pyarrow-20.0.0 pydantic-2.11.5 pydantic-core-2.33.2 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 sentencepiece-0.2.0 sentry-sdk-2.29.1 setproctitle-1.3.6 smmap-5.0.2 tokenizers-0.21.1 transformer_lens-2.15.4 transformers-4.52.3 transformers-stream-generator-0.0.5 typeguard-4.4.2 typing-inspection-0.4.1 tzdata-2025.2 urllib3-2.4.0 wadler-lindig-0.1.6 wandb-0.19.11 xxhash-3.5.0 yarl-1.20.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "DEVELOPMENT_MODE = True\n",
        "# Upgrade pip\n",
        "%pip install --upgrade pip\n",
        "\n",
        "from IPython import get_ipython\n",
        "\n",
        "ipython = get_ipython()\n",
        "# Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "ipython.magic(\"load_ext autoreload\")\n",
        "ipython.magic(\"autoreload 2\")\n",
        "ipython.run_line_magic(\"pip\", \"install ipympl\")\n",
        "ipython.run_line_magic(\"pip\", \"install scipy\")\n",
        "ipython.run_line_magic(\"pip\", \"install manim\")\n",
        "ipython.run_line_magic(\"pip\", \"install torch\")\n",
        "ipython.run_line_magic(\"pip\", \"install numpy<2\")\n",
        "ipython.run_line_magic(\"pip\", \"install einops\")\n",
        "ipython.run_line_magic(\"pip\", \"install transformer_lens\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i8GCNEdpIZpc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "import copy\n",
        "\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, ActivationCache\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)\n",
        "# Define the location to save the model, using a relative path\n",
        "PTH_LOCATION = \"workspace/_scratch/identity.pth\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M2Nzu7EIZpe"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOckI2-GIZpe"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_2OXlPG-IZpe"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = 3\n",
        "OUTPUT_DIM = 3\n",
        "frac_train = 1\n",
        "\n",
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1e-2\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "num_epochs = 10000\n",
        "checkpoint_every = 500\n",
        "\n",
        "DATA_SEED = 599"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT7xY9WsIZpe"
      },
      "source": [
        "## Define Task\n",
        "* Define random function\n",
        "* Define the dataset & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOJMbA26IZpe"
      },
      "source": [
        "Input format:\n",
        "|a|b|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I8Af2zLZIZpf",
        "outputId": "5075c607-e865-4b8c-90df-d2685d17ebb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2]], device='cuda:0')\n",
            "tensor([2, 2, 1, 0, 2, 2, 0, 0, 2], device='cuda:0')\n",
            "torch.Size([9, 2])\n",
            "tensor([], device='cuda:0', size=(0, 2), dtype=torch.int64)\n",
            "tensor([], device='cuda:0', dtype=torch.int64)\n",
            "torch.Size([0, 2])\n"
          ]
        }
      ],
      "source": [
        "def get_training_data(input_dim = INPUT_DIM, output_dim = OUTPUT_DIM, data_seed = DATA_SEED):\n",
        "    torch.manual_seed(data_seed)\n",
        "    a_vector = torch.arange(input_dim)\n",
        "    dataset = torch.cartesian_prod(a_vector, a_vector).to(device)\n",
        "\n",
        "    labels = torch.randint(0, output_dim, (dataset.shape[0],), device=device)\n",
        "    train_data = dataset\n",
        "    train_labels = labels\n",
        "    # For now no test data\n",
        "    test_data = dataset[0:0]\n",
        "    test_labels = labels[0:0]\n",
        "    print(train_data)\n",
        "    print(train_labels)\n",
        "    print(train_data.shape)\n",
        "    print(test_data[:5])\n",
        "    print(test_labels[:5])\n",
        "    print(test_data.shape)\n",
        "    return train_data, train_labels, test_data, test_labels\n",
        "\n",
        "train_data, train_labels, test_data, test_labels = get_training_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_83i1bUkIZpf"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "UL7gVZ9WIZpf"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_seeded_model(seed = 999, input_dim = INPUT_DIM, output_dim = OUTPUT_DIM):\n",
        "    cfg = HookedTransformerConfig(\n",
        "        n_layers = 1,\n",
        "        n_heads = 1,\n",
        "        d_model = 2,\n",
        "        d_head = 2,\n",
        "        d_mlp = 4,\n",
        "        attn_only=False,\n",
        "        act_fn = \"relu\",\n",
        "        normalization_type=None,\n",
        "        d_vocab=input_dim,\n",
        "        d_vocab_out=output_dim,\n",
        "        n_ctx=2,\n",
        "        init_weights=True,\n",
        "        device=device,\n",
        "        seed = seed,\n",
        "    )\n",
        "    model = HookedTransformer(cfg)\n",
        "    # Biases are enabled by default\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"mlp.b_out\" in name:\n",
        "            param.requires_grad = False\n",
        "    return model\n",
        "\n",
        "model = get_seeded_model(seed = 993)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lctx1jAtIZpg"
      },
      "source": [
        "## Define Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2]], device='cuda:0')\n",
            "tensor([[[ 0.1324,  0.1346,  0.1444],\n",
            "         [-0.1453,  0.5954,  0.4067]],\n",
            "\n",
            "        [[ 0.1324,  0.1346,  0.1444],\n",
            "         [ 0.0021,  0.2391,  0.1825]],\n",
            "\n",
            "        [[ 0.1324,  0.1346,  0.1444],\n",
            "         [-0.3875,  0.3479,  0.1415]],\n",
            "\n",
            "        [[ 0.2610,  0.0929,  0.1536],\n",
            "         [-0.1282,  0.5130,  0.3495]],\n",
            "\n",
            "        [[ 0.2610,  0.0929,  0.1536],\n",
            "         [ 0.0193,  0.1562,  0.1249]],\n",
            "\n",
            "        [[ 0.2610,  0.0929,  0.1536],\n",
            "         [-0.3633,  0.2177,  0.0502]],\n",
            "\n",
            "        [[-0.0580, -0.1071, -0.0999],\n",
            "         [-0.1084,  0.7052,  0.5020]],\n",
            "\n",
            "        [[-0.0580, -0.1071, -0.0999],\n",
            "         [ 0.0397,  0.3555,  0.2830]],\n",
            "\n",
            "        [[-0.0580, -0.1071, -0.0999],\n",
            "         [-0.3493,  0.5255,  0.2887]]], device='cuda:0',\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(train_data)\n",
        "print(model(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dJORAU_PIZpg",
        "outputId": "1c2b9f25-056e-4e0d-defc-0bd11586136d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.1659, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "tensor(nan, device='cuda:0', dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "Uniform loss:\n",
            "2.1972245773362196\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)\n",
        "def loss_fn(logits, labels):\n",
        "    if len(logits.shape)==3:\n",
        "        logits = logits[:, -1]\n",
        "    logits = logits.to(torch.float64)\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
        "    return -correct_log_probs.mean()\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(train_loss)\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(test_loss)\n",
        "print(\"Uniform loss:\")\n",
        "print(np.log(INPUT_DIM * INPUT_DIM))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9Q4bIcIZpg"
      },
      "source": [
        "## Actually Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj4h2LIRIZpg"
      },
      "source": [
        "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_stats(model):\n",
        "    # Extract the p 2-dimensional tensors, vector i is vec[:, i]\n",
        "    vec = model.W_U.data\n",
        "\n",
        "    # Function to compute the angle between two vectors\n",
        "    def compute_angle(v1, v2):\n",
        "        cos_theta = torch.dot(v1, v2) / (torch.norm(v1) * torch.norm(v2))\n",
        "        angle = torch.acos(cos_theta) * (180.0 / np.pi)\n",
        "        return angle\n",
        "\n",
        "    # Compute pairwise angles\n",
        "    # for i in range(vec.shape[1]):\n",
        "    #     for j in range(i+1, vec.shape[1]):\n",
        "    #         angle = compute_angle(vec[:, i], vec[:, j])\n",
        "    #         print(f\"Angle between {i} and {j}: {angle.item():.2f}°\")\n",
        "    #     print(f\"Norm of vector {i}: {torch.norm(vec[:, i]):.2f}\")\n",
        "\n",
        "print_stats(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c477d3a6463646deb7cf1de99bb0199a"
          ]
        },
        "id": "UsZLeCMeIZph",
        "outputId": "133f3502-ca28-490c-c030-f60b751dcd7b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e7d92d2d236400fa11b12bf24521551",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 499 Train Loss 0.4106060732104831 Test Loss nan\n",
            "Epoch 999 Train Loss 0.1807328558316352 Test Loss nan\n",
            "Epoch 1499 Train Loss 0.16043439349428762 Test Loss nan\n",
            "Epoch 1999 Train Loss 0.15647240053487907 Test Loss nan\n",
            "Epoch 2499 Train Loss 0.15523836767544205 Test Loss nan\n",
            "Epoch 2999 Train Loss 0.15471887337163706 Test Loss nan\n",
            "Epoch 3499 Train Loss 0.1544579820204664 Test Loss nan\n",
            "Epoch 3999 Train Loss 0.15431172330198398 Test Loss nan\n",
            "Epoch 4499 Train Loss 0.15422296296427562 Test Loss nan\n",
            "Epoch 4999 Train Loss 0.15416575042857242 Test Loss nan\n",
            "Epoch 5499 Train Loss 0.15412773918838138 Test Loss nan\n",
            "Epoch 5999 Train Loss 0.15410142859328083 Test Loss nan\n",
            "Epoch 6499 Train Loss 0.1540828876843727 Test Loss nan\n",
            "Epoch 6999 Train Loss 0.15406962154348441 Test Loss nan\n",
            "Epoch 7499 Train Loss 0.15406035555606504 Test Loss nan\n",
            "Epoch 7999 Train Loss 0.15405372086043706 Test Loss nan\n",
            "Epoch 8499 Train Loss 0.1540489450639284 Test Loss nan\n",
            "Epoch 8999 Train Loss 0.15404571483857057 Test Loss nan\n",
            "Epoch 9499 Train Loss 0.15404297395914548 Test Loss nan\n",
            "Epoch 9999 Train Loss 0.15404118345250087 Test Loss nan\n"
          ]
        }
      ],
      "source": [
        "def train_model(model, train_data, train_labels, test_data, test_labels, num_epochs = num_epochs, loss_target = None):\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=lr, weight_decay=wd, betas=betas\n",
        "    )\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    model_checkpoints = []\n",
        "    checkpoint_epochs = []\n",
        "    if TRAIN_MODEL:\n",
        "        for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "            train_logits = model(train_data)\n",
        "            train_loss = loss_fn(train_logits, train_labels)\n",
        "            train_loss.backward()\n",
        "            train_losses.append(train_loss.item())\n",
        "            if loss_target is not None and train_loss.item() < loss_target:\n",
        "                print(f\"Loss target {loss_target} reached with loss {train_loss.item()} at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.inference_mode():\n",
        "                test_logits = model(test_data)\n",
        "                test_loss = loss_fn(test_logits, test_labels)\n",
        "                test_losses.append(test_loss.item())\n",
        "\n",
        "            if ((epoch+1)%checkpoint_every)==0:\n",
        "                checkpoint_epochs.append(epoch)\n",
        "                model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "                print_stats(model)\n",
        "                print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")\n",
        "    if TRAIN_MODEL:\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model\":model.state_dict(),\n",
        "                \"config\": model.cfg,\n",
        "                \"checkpoints\": model_checkpoints,\n",
        "                \"checkpoint_epochs\": checkpoint_epochs,\n",
        "                \"test_losses\": test_losses,\n",
        "                \"train_losses\": train_losses,\n",
        "            },\n",
        "            PTH_LOCATION)\n",
        "\n",
        "train_model(\n",
        "    model, train_data, train_labels, test_data, test_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EzRKi7J7IZph"
      },
      "outputs": [],
      "source": [
        "if not TRAIN_MODEL:\n",
        "    cached_data = torch.load(PTH_LOCATION)\n",
        "    model.load_state_dict(cached_data['model'])\n",
        "    model_checkpoints = cached_data[\"checkpoints\"]\n",
        "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
        "    test_losses = cached_data['test_losses']\n",
        "    train_losses = cached_data['train_losses']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transposed Input:\n",
            " tensor([[0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
            "        [0, 1, 2, 0, 1, 2, 0, 1, 2]], device='cuda:0')\n",
            "Labels:  tensor([2, 2, 1, 0, 2, 2, 0, 0, 2], device='cuda:0')\n",
            "Logits of last token:\n",
            " tensor([[ 28.1620, -19.2949,  41.1054],\n",
            "        [ 31.0093, -21.3608,  52.8604],\n",
            "        [-17.7300,  11.5763,  11.5747],\n",
            "        [ 21.5631, -14.4316,   8.8870],\n",
            "        [ 30.3038, -20.8193,  47.9917],\n",
            "        [-17.7300,  11.5763,  11.5747],\n",
            "        [ 19.9547, -13.3544,   8.1595],\n",
            "        [ 20.4123, -13.6608,   8.3665],\n",
            "        [ -0.4468,   0.1360,  10.4339]], device='cuda:0',\n",
            "       grad_fn=<SliceBackward0>)\n",
            "Unembed:\n",
            " tensor([[-1.0749,  0.7303, -1.1674],\n",
            "        [ 1.5622, -1.0085, -1.7878]], device='cuda:0')\n",
            "Last layer before unembed:\n",
            " tensor([[-30.2508,  -3.1116],\n",
            "        [-36.5129,  -5.5977],\n",
            "        [  3.9719,  -8.9408],\n",
            "        [-13.6627,   4.0780],\n",
            "        [-34.1454,  -4.4203],\n",
            "        [  3.9719,  -8.9408],\n",
            "        [-12.5916,   3.7855],\n",
            "        [-12.8963,   3.8687],\n",
            "        [ -3.8020,  -3.2263]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "def create_cache(model):\n",
        "    input = train_data\n",
        "    print(\"Transposed Input:\\n\", input.transpose(0, 1))\n",
        "    logits, cache = model.run_with_cache(input)\n",
        "    print(\"Labels: \", train_labels)\n",
        "    print(\"Logits of last token:\\n\", logits[:, -1, :])\n",
        "    print(\"Unembed:\\n\", model.W_U.data)\n",
        "    print(\"Last layer before unembed:\\n\", cache.cache_dict[\"blocks.0.hook_resid_post\"][:, -1, :])\n",
        "    return cache\n",
        "\n",
        "cache = create_cache(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hook_embed': tensor([[[-0.7916,  0.5167],\n",
              "          [-0.7916,  0.5167]],\n",
              " \n",
              "         [[-0.7916,  0.5167],\n",
              "          [-1.0963,  0.5999]],\n",
              " \n",
              "         [[-0.7916,  0.5167],\n",
              "          [ 0.4704, -0.9617]],\n",
              " \n",
              "         [[-1.0963,  0.5999],\n",
              "          [-0.7916,  0.5167]],\n",
              " \n",
              "         [[-1.0963,  0.5999],\n",
              "          [-1.0963,  0.5999]],\n",
              " \n",
              "         [[-1.0963,  0.5999],\n",
              "          [ 0.4704, -0.9617]],\n",
              " \n",
              "         [[ 0.4704, -0.9617],\n",
              "          [-0.7916,  0.5167]],\n",
              " \n",
              "         [[ 0.4704, -0.9617],\n",
              "          [-1.0963,  0.5999]],\n",
              " \n",
              "         [[ 0.4704, -0.9617],\n",
              "          [ 0.4704, -0.9617]]], device='cuda:0'),\n",
              " 'hook_pos_embed': tensor([[[-0.8347,  1.5906],\n",
              "          [-1.1737, -1.0081]],\n",
              " \n",
              "         [[-0.8347,  1.5906],\n",
              "          [-1.1737, -1.0081]],\n",
              " \n",
              "         [[-0.8347,  1.5906],\n",
              "          [-1.1737, -1.0081]],\n",
              " \n",
              "         [[-0.8347,  1.5906],\n",
              "          [-1.1737, -1.0081]],\n",
              " \n",
              "         [[-0.8347,  1.5906],\n",
              "          [-1.1737, -1.0081]],\n",
              " \n",
              "         [[-0.8347,  1.5906],\n",
              "          [-1.1737, -1.0081]],\n",
              " \n",
              "         [[-0.8347,  1.5906],\n",
              "          [-1.1737, -1.0081]],\n",
              " \n",
              "         [[-0.8347,  1.5906],\n",
              "          [-1.1737, -1.0081]],\n",
              " \n",
              "         [[-0.8347,  1.5906],\n",
              "          [-1.1737, -1.0081]]], device='cuda:0'),\n",
              " 'blocks.0.hook_resid_pre': tensor([[[-1.6263,  2.1073],\n",
              "          [-1.9652, -0.4914]],\n",
              " \n",
              "         [[-1.6263,  2.1073],\n",
              "          [-2.2700, -0.4082]],\n",
              " \n",
              "         [[-1.6263,  2.1073],\n",
              "          [-0.7033, -1.9698]],\n",
              " \n",
              "         [[-1.9310,  2.1905],\n",
              "          [-1.9652, -0.4914]],\n",
              " \n",
              "         [[-1.9310,  2.1905],\n",
              "          [-2.2700, -0.4082]],\n",
              " \n",
              "         [[-1.9310,  2.1905],\n",
              "          [-0.7033, -1.9698]],\n",
              " \n",
              "         [[-0.3643,  0.6289],\n",
              "          [-1.9652, -0.4914]],\n",
              " \n",
              "         [[-0.3643,  0.6289],\n",
              "          [-2.2700, -0.4082]],\n",
              " \n",
              "         [[-0.3643,  0.6289],\n",
              "          [-0.7033, -1.9698]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_q': tensor([[[[-1.0015, -1.8015]],\n",
              " \n",
              "          [[-3.9981,  2.3349]]],\n",
              " \n",
              " \n",
              "         [[[-1.0015, -1.8015]],\n",
              " \n",
              "          [[-4.3128,  2.3078]]],\n",
              " \n",
              " \n",
              "         [[[-1.0015, -1.8015]],\n",
              " \n",
              "          [[-3.8095,  4.2028]]],\n",
              " \n",
              " \n",
              "         [[[-1.3163, -1.8286]],\n",
              " \n",
              "          [[-3.9981,  2.3349]]],\n",
              " \n",
              " \n",
              "         [[[-1.3163, -1.8286]],\n",
              " \n",
              "          [[-4.3128,  2.3078]]],\n",
              " \n",
              " \n",
              "         [[[-1.3163, -1.8286]],\n",
              " \n",
              "          [[-3.8095,  4.2028]]],\n",
              " \n",
              " \n",
              "         [[[-0.8129,  0.0664]],\n",
              " \n",
              "          [[-3.9981,  2.3349]]],\n",
              " \n",
              " \n",
              "         [[[-0.8129,  0.0664]],\n",
              " \n",
              "          [[-4.3128,  2.3078]]],\n",
              " \n",
              " \n",
              "         [[[-0.8129,  0.0664]],\n",
              " \n",
              "          [[-3.8095,  4.2028]]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_k': tensor([[[[ 2.7117, -3.1289]],\n",
              " \n",
              "          [[ 3.6670, -2.2739]]],\n",
              " \n",
              " \n",
              "         [[[ 2.7117, -3.1289]],\n",
              " \n",
              "          [[ 4.3094, -2.8019]]],\n",
              " \n",
              " \n",
              "         [[[ 2.7117, -3.1289]],\n",
              " \n",
              "          [[ 1.1079,  0.5206]]],\n",
              " \n",
              " \n",
              "         [[[ 3.3541, -3.6570]],\n",
              " \n",
              "          [[ 3.6670, -2.2739]]],\n",
              " \n",
              " \n",
              "         [[[ 3.3541, -3.6570]],\n",
              " \n",
              "          [[ 4.3094, -2.8019]]],\n",
              " \n",
              " \n",
              "         [[[ 3.3541, -3.6570]],\n",
              " \n",
              "          [[ 1.1079,  0.5206]]],\n",
              " \n",
              " \n",
              "         [[[ 0.1526, -0.3345]],\n",
              " \n",
              "          [[ 3.6670, -2.2739]]],\n",
              " \n",
              " \n",
              "         [[[ 0.1526, -0.3345]],\n",
              " \n",
              "          [[ 4.3094, -2.8019]]],\n",
              " \n",
              " \n",
              "         [[[ 0.1526, -0.3345]],\n",
              " \n",
              "          [[ 1.1079,  0.5206]]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_v': tensor([[[[-6.5405, -6.0172]],\n",
              " \n",
              "          [[-0.7535, -2.5331]]],\n",
              " \n",
              " \n",
              "         [[[-6.5405, -6.0172]],\n",
              " \n",
              "          [[-1.2270, -2.8914]]],\n",
              " \n",
              " \n",
              "         [[[-6.5405, -6.0172]],\n",
              " \n",
              "          [[ 3.8670,  0.5867]]],\n",
              " \n",
              " \n",
              "         [[[-7.0139, -6.3756]],\n",
              " \n",
              "          [[-0.7535, -2.5331]]],\n",
              " \n",
              " \n",
              "         [[[-7.0139, -6.3756]],\n",
              " \n",
              "          [[-1.2270, -2.8914]]],\n",
              " \n",
              " \n",
              "         [[[-7.0139, -6.3756]],\n",
              " \n",
              "          [[ 3.8670,  0.5867]]],\n",
              " \n",
              " \n",
              "         [[[-1.9200, -2.8975]],\n",
              " \n",
              "          [[-0.7535, -2.5331]]],\n",
              " \n",
              " \n",
              "         [[[-1.9200, -2.8975]],\n",
              " \n",
              "          [[-1.2270, -2.8914]]],\n",
              " \n",
              " \n",
              "         [[[-1.9200, -2.8975]],\n",
              " \n",
              "          [[ 3.8670,  0.5867]]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_attn_scores': tensor([[[[  2.0654,     -inf],\n",
              "           [-12.8322, -14.1211]]],\n",
              " \n",
              " \n",
              "         [[[  2.0654,     -inf],\n",
              "           [-13.3757, -17.7145]]],\n",
              " \n",
              " \n",
              "         [[[  2.0654,     -inf],\n",
              "           [-16.6031,  -1.4374]]],\n",
              " \n",
              " \n",
              "         [[[  1.6067,     -inf],\n",
              "           [-15.5202, -14.1211]]],\n",
              " \n",
              " \n",
              "         [[[  1.6067,     -inf],\n",
              "           [-16.1965, -17.7145]]],\n",
              " \n",
              " \n",
              "         [[[  1.6067,     -inf],\n",
              "           [-19.9029,  -1.4374]]],\n",
              " \n",
              " \n",
              "         [[[ -0.1034,     -inf],\n",
              "           [ -0.9838, -14.1211]]],\n",
              " \n",
              " \n",
              "         [[[ -0.1034,     -inf],\n",
              "           [ -1.0113, -17.7145]]],\n",
              " \n",
              " \n",
              "         [[[ -0.1034,     -inf],\n",
              "           [ -1.4052,  -1.4374]]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_pattern': tensor([[[[1.0000e+00, 0.0000e+00],\n",
              "           [7.8397e-01, 2.1603e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [9.8712e-01, 1.2883e-02]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [2.5917e-07, 1.0000e+00]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.9796e-01, 8.0204e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [8.2024e-01, 1.7976e-01]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [9.5612e-09, 1.0000e+00]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.0000e+00, 1.9702e-06]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [1.0000e+00, 5.5704e-08]]],\n",
              " \n",
              " \n",
              "         [[[1.0000e+00, 0.0000e+00],\n",
              "           [5.0805e-01, 4.9195e-01]]]], device='cuda:0'),\n",
              " 'blocks.0.attn.hook_z': tensor([[[[-6.5405, -6.0172]],\n",
              " \n",
              "          [[-5.2903, -5.2645]]],\n",
              " \n",
              " \n",
              "         [[[-6.5405, -6.0172]],\n",
              " \n",
              "          [[-6.4720, -5.9770]]],\n",
              " \n",
              " \n",
              "         [[[-6.5405, -6.0172]],\n",
              " \n",
              "          [[ 3.8670,  0.5867]]],\n",
              " \n",
              " \n",
              "         [[[-7.0139, -6.3756]],\n",
              " \n",
              "          [[-1.9928, -3.2937]]],\n",
              " \n",
              " \n",
              "         [[[-7.0139, -6.3756]],\n",
              " \n",
              "          [[-5.9736, -5.7493]]],\n",
              " \n",
              " \n",
              "         [[[-7.0139, -6.3756]],\n",
              " \n",
              "          [[ 3.8670,  0.5867]]],\n",
              " \n",
              " \n",
              "         [[[-1.9200, -2.8975]],\n",
              " \n",
              "          [[-1.9200, -2.8975]]],\n",
              " \n",
              " \n",
              "         [[[-1.9200, -2.8975]],\n",
              " \n",
              "          [[-1.9200, -2.8975]]],\n",
              " \n",
              " \n",
              "         [[[-1.9200, -2.8975]],\n",
              " \n",
              "          [[ 0.9269, -1.1834]]]], device='cuda:0'),\n",
              " 'blocks.0.hook_attn_out': tensor([[[-23.6860,  13.4023],\n",
              "          [-20.3803,  10.9724]],\n",
              " \n",
              "         [[-23.6860,  13.4023],\n",
              "          [-23.5073,  13.2697]],\n",
              " \n",
              "         [[-23.6860,  13.4023],\n",
              "          [  4.6752,  -6.9710]],\n",
              " \n",
              "         [[-25.1205,  14.3539],\n",
              "          [-11.6975,   4.5694]],\n",
              " \n",
              "         [[-25.1205,  14.3539],\n",
              "          [-22.3699,  12.3320]],\n",
              " \n",
              "         [[-25.1205,  14.3539],\n",
              "          [  4.6752,  -6.9710]],\n",
              " \n",
              "         [[-10.6263,   4.2769],\n",
              "          [-10.6263,   4.2769]],\n",
              " \n",
              "         [[-10.6263,   4.2769],\n",
              "          [-10.6263,   4.2769]],\n",
              " \n",
              "         [[-10.6263,   4.2769],\n",
              "          [ -3.0987,  -1.2565]]], device='cuda:0'),\n",
              " 'blocks.0.hook_resid_mid': tensor([[[-25.3122,  15.5097],\n",
              "          [-22.3455,  10.4810]],\n",
              " \n",
              "         [[-25.3122,  15.5097],\n",
              "          [-25.7773,  12.8615]],\n",
              " \n",
              "         [[-25.3122,  15.5097],\n",
              "          [  3.9719,  -8.9408]],\n",
              " \n",
              "         [[-27.0515,  16.5445],\n",
              "          [-13.6627,   4.0780]],\n",
              " \n",
              "         [[-27.0515,  16.5445],\n",
              "          [-24.6399,  11.9238]],\n",
              " \n",
              "         [[-27.0515,  16.5445],\n",
              "          [  3.9719,  -8.9408]],\n",
              " \n",
              "         [[-10.9906,   4.9059],\n",
              "          [-12.5916,   3.7855]],\n",
              " \n",
              "         [[-10.9906,   4.9059],\n",
              "          [-12.8963,   3.8687]],\n",
              " \n",
              "         [[-10.9906,   4.9059],\n",
              "          [ -3.8020,  -3.2263]]], device='cuda:0'),\n",
              " 'blocks.0.mlp.hook_pre': tensor([[[ 1.6657e+01],\n",
              "          [ 8.1354e+00]],\n",
              " \n",
              "         [[ 1.6657e+01],\n",
              "          [ 1.1048e+01]],\n",
              " \n",
              "         [[ 1.6657e+01],\n",
              "          [-1.6559e+01]],\n",
              " \n",
              "         [[ 1.7787e+01],\n",
              "          [-2.1842e-03]],\n",
              " \n",
              "         [[ 1.7787e+01],\n",
              "          [ 9.7823e+00]],\n",
              " \n",
              "         [[ 1.7787e+01],\n",
              "          [-1.6559e+01]],\n",
              " \n",
              "         [[ 3.1489e+00],\n",
              "          [-8.8573e-04]],\n",
              " \n",
              "         [[ 3.1489e+00],\n",
              "          [-1.2825e-03]],\n",
              " \n",
              "         [[ 3.1489e+00],\n",
              "          [-9.3099e+00]]], device='cuda:0'),\n",
              " 'blocks.0.mlp.hook_post': tensor([[[16.6574],\n",
              "          [ 8.1354]],\n",
              " \n",
              "         [[16.6574],\n",
              "          [11.0482]],\n",
              " \n",
              "         [[16.6574],\n",
              "          [ 0.0000]],\n",
              " \n",
              "         [[17.7867],\n",
              "          [ 0.0000]],\n",
              " \n",
              "         [[17.7867],\n",
              "          [ 9.7823]],\n",
              " \n",
              "         [[17.7867],\n",
              "          [ 0.0000]],\n",
              " \n",
              "         [[ 3.1489],\n",
              "          [ 0.0000]],\n",
              " \n",
              "         [[ 3.1489],\n",
              "          [ 0.0000]],\n",
              " \n",
              "         [[ 3.1489],\n",
              "          [ 0.0000]]], device='cuda:0'),\n",
              " 'blocks.0.hook_mlp_out': tensor([[[-16.1862, -27.8308],\n",
              "          [ -7.9053, -13.5925]],\n",
              " \n",
              "         [[-16.1862, -27.8308],\n",
              "          [-10.7356, -18.4591]],\n",
              " \n",
              "         [[-16.1862, -27.8308],\n",
              "          [  0.0000,   0.0000]],\n",
              " \n",
              "         [[-17.2835, -29.7176],\n",
              "          [  0.0000,   0.0000]],\n",
              " \n",
              "         [[-17.2835, -29.7176],\n",
              "          [ -9.5055, -16.3440]],\n",
              " \n",
              "         [[-17.2835, -29.7176],\n",
              "          [  0.0000,   0.0000]],\n",
              " \n",
              "         [[ -3.0598,  -5.2611],\n",
              "          [  0.0000,   0.0000]],\n",
              " \n",
              "         [[ -3.0598,  -5.2611],\n",
              "          [  0.0000,   0.0000]],\n",
              " \n",
              "         [[ -3.0598,  -5.2611],\n",
              "          [  0.0000,   0.0000]]], device='cuda:0'),\n",
              " 'blocks.0.hook_resid_post': tensor([[[-41.4984, -12.3212],\n",
              "          [-30.2508,  -3.1116]],\n",
              " \n",
              "         [[-41.4984, -12.3212],\n",
              "          [-36.5129,  -5.5977]],\n",
              " \n",
              "         [[-41.4984, -12.3212],\n",
              "          [  3.9719,  -8.9408]],\n",
              " \n",
              "         [[-44.3350, -13.1732],\n",
              "          [-13.6627,   4.0780]],\n",
              " \n",
              "         [[-44.3350, -13.1732],\n",
              "          [-34.1454,  -4.4203]],\n",
              " \n",
              "         [[-44.3350, -13.1732],\n",
              "          [  3.9719,  -8.9408]],\n",
              " \n",
              "         [[-14.0504,  -0.3552],\n",
              "          [-12.5916,   3.7855]],\n",
              " \n",
              "         [[-14.0504,  -0.3552],\n",
              "          [-12.8963,   3.8687]],\n",
              " \n",
              "         [[-14.0504,  -0.3552],\n",
              "          [ -3.8020,  -3.2263]]], device='cuda:0')}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get a list of all activations stored in the cache, especially their names\n",
        "cache.cache_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Animations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from manim import *\n",
        "\n",
        "config.media_width = \"80%\"\n",
        "config.verbosity = \"WARNING\"\n",
        "config.preview = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "blocks.0.mlp.W_in Parameter containing:\n",
            "tensor([[-1.0536],\n",
            "        [-1.0663]], device='cuda:0', requires_grad=True)\n",
            "blocks.0.mlp.b_in Parameter containing:\n",
            "tensor([-0.0978], device='cuda:0', requires_grad=True)\n",
            "blocks.0.mlp.W_out Parameter containing:\n",
            "tensor([[-1.0305,  1.1713]], device='cuda:0', requires_grad=True)\n",
            "blocks.0.mlp.b_out Parameter containing:\n",
            "tensor([0., 0.], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "for a, b in model.named_parameters():\n",
        "    if a.startswith(\"blocks.0.mlp.\"):\n",
        "        print(a, b)\n",
        "normal = model.get_parameter(\"blocks.0.mlp.W_in\")\n",
        "bias = model.get_parameter(\"blocks.0.mlp.b_in\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 3, 0])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normal_numpy = normal\n",
        "b = [2, 3]\n",
        "b_numpy = np.array(b)\n",
        "np.append(b, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gathering Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[tensor([-0.5821,  0.9190])(#FFFFFF)], [tensor([-1.2485,  0.7991])(#FFFFFF)], [tensor([ 0.2676, -1.6296])(#888888)], [tensor([ 0.2676, -1.6296])(#888888)]]\n"
          ]
        }
      ],
      "source": [
        "class VectorParams:\n",
        "    def __init__(self, values = [], color = WHITE, label = \"\"):\n",
        "        self.values = values\n",
        "        self.color = color\n",
        "        self.label = label\n",
        "    def __repr__(self) -> str:\n",
        "        return str(self.values) + \"(\" + str(self.color) + \")\"\n",
        "\n",
        "class Data:\n",
        "    def __init__(self):\n",
        "        # for each step, store a list of all vectors (one per input)\n",
        "        self.vectors: list[list[VectorParams]] = [[]]\n",
        "        # for each step, store some supplementory lines that should be drawn, e.g. zero-lines for MLPs\n",
        "        self.supp_lines: list[list[tuple[list[float], list[float]]]] = [[]]\n",
        "        self.steps = 0\n",
        "        self.current_labels = set()\n",
        "\n",
        "    def add_vector(self, vector, color = WHITE, label = \"\"):\n",
        "        if label not in self.current_labels:\n",
        "            self.current_labels.add(label)\n",
        "            self.vectors[self.steps].append(VectorParams(values = vector, color = color, label = label))\n",
        "\n",
        "    def next_step(self):\n",
        "        self.steps += 1\n",
        "        self.vectors.append([])\n",
        "        self.current_labels = set()\n",
        "        self.supp_lines.append([])\n",
        "\n",
        "    def add_vectors_at_hook(self, c: ActivationCache, hook: str, color0 = WHITE, color1 = WHITE, input_labels = None, input_colors = None):\n",
        "        if input_labels is None:\n",
        "            input_labels = [\"\" for i in range(c.cache_dict[hook].shape[0])]\n",
        "        for i in range(c.cache_dict[hook].shape[0]):\n",
        "            self.add_vector(c.cache_dict[hook][i][0].cpu(), color = color0, label = input_labels[i][:1])\n",
        "            self.add_vector(\n",
        "                c.cache_dict[hook][i][1].cpu(), color=color1 if input_colors is None else input_colors[i], label=input_labels[i][:2]\n",
        "            )\n",
        "\n",
        "    def add_mlp_lines(self, model, W_in, b_in):\n",
        "        normals = model.get_parameter(W_in).transpose(0, 1)\n",
        "        biases = model.get_parameter(b_in)\n",
        "        for i, normal in enumerate(normals):\n",
        "            bias = biases[i]\n",
        "            # To draw the line, we need to compute the start and end points A, B such that\n",
        "            # A * normal + bias = 0 and B * normal + bias = 0, and A and B are far away in different directions\n",
        "            if normal[0].item() == 0:\n",
        "                # If the first component is zero, we can draw a horizontal line\n",
        "                A = [-10, - bias.item() / normal[1].item()]\n",
        "                B = [+10, - bias.item() / normal[1].item()]\n",
        "            else:\n",
        "                # Pick Y-coordinates -10 and 10 and compute the corresponding X-coordinates:\n",
        "                A = [(- bias.item() + 10 * normal[1].item()) / normal[0].item(), -10]\n",
        "                B = [(- bias.item() - 10 * normal[1].item()) / normal[0].item(), +10]\n",
        "            self.supp_lines[self.steps].append((A, B))\n",
        "\n",
        "\n",
        "def compile_data_vectors(model, cache, input_labels=None, input_colors=None):\n",
        "    # Set default value as list of empty strings\n",
        "    vectors = Data()\n",
        "    vectors.add_vectors_at_hook(cache, \"hook_embed\", color1 = GRAY, input_labels=input_labels, input_colors=input_colors)\n",
        "    vectors.next_step()\n",
        "    vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_pre\", input_labels=input_labels, input_colors=input_colors)\n",
        "    vectors.next_step()\n",
        "    vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_mid\", color0 = GRAY, input_labels=input_labels, input_colors=input_colors)\n",
        "    vectors.next_step()\n",
        "    vectors.add_vectors_at_hook(cache, \"blocks.0.hook_resid_post\", color0 = GRAY, input_labels=input_labels, input_colors=input_colors)\n",
        "    vectors.add_mlp_lines(model, \"blocks.0.mlp.W_in\", \"blocks.0.mlp.b_in\")\n",
        "\n",
        "    print(vectors.vectors)\n",
        "    return vectors\n",
        "\n",
        "\n",
        "vectors = compile_data_vectors(model, cache)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "def change_font_size(labeled_arrow: LabeledArrow, new_size):\n",
        "    # print(labeled_arrow, labeled_arrow.submobjects)\n",
        "    # print(labeled_arrow.submobjects[-1].font_size)\n",
        "    if not isinstance(labeled_arrow, LabeledArrow):\n",
        "        return\n",
        "    label = labeled_arrow.submobjects[-1]\n",
        "    box = labeled_arrow.submobjects[-2]\n",
        "    if not isinstance(box, BackgroundRectangle):\n",
        "        box = labeled_arrow.submobjects[-3]\n",
        "    coords = label.get_center()\n",
        "    # print(new_size)\n",
        "    labeled_arrow.submobjects[-1] = MathTex(\n",
        "        label.get_tex_string(), color=label.color, font_size=new_size\n",
        "    )\n",
        "    # print(\"size=\", labeled_arrow.submobjects[-1].font_size)\n",
        "    label = labeled_arrow.submobjects[-1]\n",
        "    label.move_to(coords)\n",
        "    box.width = label.width + 2 * box.buff\n",
        "    box.height = label.height + 2 * box.buff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Scene Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "DOT_SCALE = 0.01\n",
        "class VisualizeTransformer(MovingCameraScene):\n",
        "    def construct(self):\n",
        "        print(\"v=\", vectors.vectors)\n",
        "        axes = Axes(\n",
        "            x_range = [-20, 20, 1],\n",
        "            y_range = [-20, 20, 1],\n",
        "            x_axis_config={\n",
        "                \"numbers_to_include\": np.arange(-18, 18.1, 3),\n",
        "                \"font_size\": 24\n",
        "            },\n",
        "            y_axis_config={\n",
        "                \"numbers_to_include\": np.arange(-18, 18.1, 3), \n",
        "                \"font_size\": 24            \n",
        "            },\n",
        "            x_length = 40,\n",
        "            y_length = 40,\n",
        "            axis_config={\"color\": GREEN}\n",
        "        )\n",
        "\n",
        "        scale = ValueTracker(2)\n",
        "\n",
        "        dots = VGroup()\n",
        "        def update_scale(self):\n",
        "            return\n",
        "            # TODO: Make the scaling nicer\n",
        "            self.stroke_width = 6 * scale.get_value()\n",
        "            change_font_size(self, 48 * scale.get_value())\n",
        "            # print(\"New font size: \", self.font_size)\n",
        "\n",
        "        # Embedding arrows\n",
        "        for i, t in enumerate(vectors.vectors[0]):\n",
        "            # print(t, t.numpy())\n",
        "            # arrow = LabeledArrow(\n",
        "            #     start=ORIGIN,\n",
        "            #     end=np.append(t.values.numpy(), 0),\n",
        "            #     buff = 0,\n",
        "            #     label = t.label,\n",
        "            #     label_frame = False,\n",
        "            #     label_color=YELLOW,\n",
        "            #     color = t.color,\n",
        "            #     max_stroke_width_to_length_ratio = 100,\n",
        "            # )\n",
        "\n",
        "            # arrow.add_updater(update_scale)\n",
        "            # arrows.add(arrow)\n",
        "            dot = LabeledDot(\n",
        "                point=np.append(t.values.numpy(), 0),\n",
        "                label=t.label,\n",
        "                color=t.color,\n",
        "                radius=DOT_SCALE * (len(vectors.vectors[0]) - i) + 0.2,\n",
        "            )\n",
        "\n",
        "            dot.set_opacity(0.5)\n",
        "\n",
        "            dot.add_updater(update_scale)\n",
        "            dots.add(dot)\n",
        "\n",
        "        # Transitioning the arrows through the model\n",
        "        self.add(axes, axes.get_axis_labels(), dots)\n",
        "        for step in range(1, len(vectors.vectors)):\n",
        "            new_dots = VGroup()\n",
        "            transition_arrows = VGroup()\n",
        "            for i, t in enumerate(vectors.vectors[step]):\n",
        "                # print(t, t.numpy())\n",
        "                # new_arrow = LabeledArrow(\n",
        "                #     start=ORIGIN,\n",
        "                #     end=np.append(t.values.numpy(), 0),\n",
        "                #     buff=0,\n",
        "                #     label=t.label,\n",
        "                #     label_frame=False,\n",
        "                #     label_color=YELLOW,\n",
        "                #     color=t.color,\n",
        "                #     max_stroke_width_to_length_ratio=100,\n",
        "                # )\n",
        "                # new_arrow.add_updater(update_scale)\n",
        "                # new_arrows.add(new_arrow)\n",
        "                new_dot = LabeledDot(\n",
        "                    point=np.append(t.values.numpy(), 0),\n",
        "                    label=t.label,\n",
        "                    color=t.color,\n",
        "                    radius = DOT_SCALE * (len(vectors.vectors[step]) - i) + 0.2,\n",
        "                )\n",
        "                new_dot.set_opacity(0.5)\n",
        "                new_dot.add_updater(update_scale)\n",
        "                new_dots.add(new_dot)\n",
        "\n",
        "                transition_arrow = Arrow(\n",
        "                    start=dots[i].arc_center,\n",
        "                    end=new_dots[i].arc_center,\n",
        "                    buff=0,\n",
        "                    color=RED,\n",
        "                )\n",
        "                transition_arrow.add_updater(update_scale)\n",
        "                transition_arrows.add(transition_arrow)\n",
        "            \n",
        "            mlp_lines = VGroup()\n",
        "            for i, t in enumerate(vectors.supp_lines[step]):\n",
        "                # print(t)\n",
        "                mlp_line = Line(\n",
        "                    start=np.append(t[0],0),\n",
        "                    end=np.append(t[1],0),\n",
        "                    buff=0,\n",
        "                    color=BLUE,\n",
        "                )\n",
        "                mlp_line.add_updater(update_scale)\n",
        "                mlp_lines.add(mlp_line)\n",
        "\n",
        "            view = SurroundingRectangle(new_dots)\n",
        "            factor = max(\n",
        "                view.width / self.camera.frame_width,\n",
        "                view.height / self.camera.frame_height,\n",
        "            )\n",
        "            print(\n",
        "                factor,\n",
        "                self.camera.frame_width, view.width,\n",
        "                self.camera.frame_height, view.height,\n",
        "            )\n",
        "            self.wait()\n",
        "            self.play(FadeIn(transition_arrows), FadeIn(mlp_lines), self.camera.auto_zoom(view, margin = 2), scale.animate.set_value(scale.get_value() * factor))\n",
        "            self.wait()\n",
        "            self.play(\n",
        "                ReplacementTransform(dots, new_dots)\n",
        "            )\n",
        "            self.wait()\n",
        "            self.play(FadeOut(transition_arrows), FadeOut(mlp_lines))\n",
        "            self.wait()\n",
        "            dots = new_dots\n",
        "\n",
        "        # Unembedding Arrows\n",
        "        embedding_arrows = VGroup()\n",
        "        data = model.W_U.data\n",
        "        print(\"unembed: \", data)\n",
        "        for i in range(data.size()[1]):\n",
        "            embedding_arrow = LabeledArrow(\n",
        "                start=ORIGIN,\n",
        "                end=[data[0, i].item(), data[1, i].item(), 0],\n",
        "                label=str(i),\n",
        "                color=LIGHT_PINK,\n",
        "                buff=0,\n",
        "                max_stroke_width_to_length_ratio=100,\n",
        "            )\n",
        "            embedding_arrows.add(embedding_arrow)\n",
        "        self.play(FadeIn(embedding_arrows))\n",
        "        self.wait()\n",
        "\n",
        "# v = VisualizeTransformer()\n",
        "# v.construct()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n",
            "env: TORCH_USE_CUDA_DSA=1\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "%env TORCH_USE_CUDA_DSA=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-65.82968603798591, -10] [63.45951549715765, 10]\n",
            "tensor([[-0.3886],\n",
            "        [ 2.5119]], device='cuda:0') 0.46049827337265015\n",
            "tensor([0.4605], device='cuda:0', grad_fn=<AddBackward0>) tensor([0.4605], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ],
      "source": [
        "A, B = vectors.supp_lines[3][0]\n",
        "print(A, B)\n",
        "W, bias = model.get_parameter(\"blocks.0.mlp.W_in\"), model.get_parameter(\"blocks.0.mlp.b_in\")\n",
        "print(W.data, bias.item())\n",
        "def dot(a, b):\n",
        "    return a[0] * b[0] + a[1] * b[1]\n",
        "print(dot(A, W), dot(B, W))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Video Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [0, 1],\n",
            "        [0, 2],\n",
            "        [0, 3],\n",
            "        [0, 4],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [1, 2],\n",
            "        [1, 3],\n",
            "        [1, 4],\n",
            "        [2, 0],\n",
            "        [2, 1],\n",
            "        [2, 2],\n",
            "        [2, 3],\n",
            "        [2, 4],\n",
            "        [3, 0],\n",
            "        [3, 1],\n",
            "        [3, 2],\n",
            "        [3, 3],\n",
            "        [3, 4],\n",
            "        [4, 0],\n",
            "        [4, 1],\n",
            "        [4, 2],\n",
            "        [4, 3],\n",
            "        [4, 4]], device='cuda:0')\n",
            "tensor([1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,\n",
            "        1], device='cuda:0')\n",
            "torch.Size([25, 2])\n",
            "tensor([], device='cuda:0', size=(0, 2), dtype=torch.int64)\n",
            "tensor([], device='cuda:0', dtype=torch.int64)\n",
            "torch.Size([0, 2])\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "42f99e0899994b36aa677933982ce975",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%manim -qh Video\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "\n",
        "input_dim = 5\n",
        "output_dim = 2\n",
        "train_data, train_labels, test_data, test_labels = get_training_data(input_dim, output_dim, data_seed=997)\n",
        "model = get_seeded_model(997, input_dim, output_dim)\n",
        "train_model(model, train_data, train_labels, test_data, test_labels, num_epochs = 10000, loss_target = 1/(input_dim ** 2 * 4))\n",
        "cache = create_cache(model)\n",
        "arrow_labels = [\"\".join([str(d.item()) for d in v]) for v in train_data]\n",
        "colors = [BLUE, YELLOW, GREEN, RED]\n",
        "arrow_colors = [colors[l] for l in train_labels]\n",
        "vectors = compile_data_vectors(model, cache, input_labels=arrow_labels, input_colors = arrow_colors)\n",
        "print(\"Labels: \", arrow_labels)\n",
        "\n",
        "class Video(VisualizeTransformer):\n",
        "    def construct(self):\n",
        "        VisualizeTransformer.construct(self)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
