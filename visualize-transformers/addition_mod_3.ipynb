{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4DVAjijIZpX"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/Grokking_Demo.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DARnrM3XIZpZ"
      },
      "source": [
        "# Interpreting Addition Mod 3 Map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBvIU6zdIZpa"
      },
      "source": [
        "# Setup\n",
        "(No need to read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ssx2pnTOIZpa"
      },
      "outputs": [],
      "source": [
        "TRAIN_MODEL = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7Jfz-lAWIZpb",
        "outputId": "48df1c71-5bc3-4a14-9a34-1af5c16786a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running as a Jupyter notebook - intended for development only!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_794282/1878011034.py:22: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"load_ext autoreload\")\n",
            "/tmp/ipykernel_794282/1878011034.py:23: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"autoreload 2\")\n",
            "/tmp/ipykernel_794282/1878011034.py:24: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"pip install ipympl\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ipympl in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (0.9.4)\n",
            "Requirement already satisfied: ipython-genutils in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipympl) (0.2.0)\n",
            "Requirement already satisfied: ipython<9 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipympl) (8.22.1)\n",
            "Requirement already satisfied: ipywidgets<9,>=7.6.0 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipympl) (8.1.5)\n",
            "Requirement already satisfied: matplotlib<4,>=3.4.0 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipympl) (3.8.3)\n",
            "Requirement already satisfied: numpy in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipympl) (1.26.4)\n",
            "Requirement already satisfied: pillow in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipympl) (10.2.0)\n",
            "Requirement already satisfied: traitlets<6 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipympl) (5.14.1)\n",
            "Requirement already satisfied: decorator in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (0.19.1)\n",
            "Requirement already satisfied: matplotlib-inline in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (2.17.2)\n",
            "Requirement already satisfied: stack-data in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (0.6.3)\n",
            "Requirement already satisfied: exceptiongroup in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (1.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipython<9->ipympl) (4.9.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (0.2.1)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (4.0.13)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from ipywidgets<9,>=7.6.0->ipympl) (3.0.13)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from matplotlib<4,>=3.4.0->ipympl) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from matplotlib<4,>=3.4.0->ipympl) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from matplotlib<4,>=3.4.0->ipympl) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from matplotlib<4,>=3.4.0->ipympl) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from matplotlib<4,>=3.4.0->ipympl) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from matplotlib<4,>=3.4.0->ipympl) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from matplotlib<4,>=3.4.0->ipympl) (2.8.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from jedi>=0.16->ipython<9->ipympl) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from pexpect>4.3->ipython<9->ipympl) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython<9->ipympl) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib<4,>=3.4.0->ipympl) (1.16.0)\n",
            "Requirement already satisfied: executing>=1.2.0 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from stack-data->ipython<9->ipympl) (2.0.1)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from stack-data->ipython<9->ipympl) (2.4.1)\n",
            "Requirement already satisfied: pure-eval in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from stack-data->ipython<9->ipympl) (0.2.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_794282/1878011034.py:25: DeprecationWarning: `magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "  ipython.magic(\"pip install scipy\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (1.14.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.23.5 in /home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages (from scipy) (1.26.4)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "import os\n",
        "\n",
        "DEVELOPMENT_MODE = True\n",
        "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "\n",
        "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
        "    # # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")\n",
        "    ipython.magic(\"pip install ipympl\")\n",
        "    ipython.magic(\"pip install scipy\")\n",
        "\n",
        "if IN_COLAB or IN_GITHUB:\n",
        "    %pip install transformer_lens\n",
        "    %pip install circuitsvis\n",
        "    %pip install ipympl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "caNlrFAnIZpc",
        "outputId": "b116f2ef-8c45-4d6b-a408-b86670d5b807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using renderer: notebook_connected\n"
          ]
        }
      ],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bakRjAcIIZpc"
      },
      "outputs": [],
      "source": [
        "pio.templates['plotly'].layout.xaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.yaxis.title.font.size = 20\n",
        "pio.templates['plotly'].layout.title.font.size = 30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i8GCNEdpIZpc"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import os\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KxZBfCTlIZpd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/philippw/Stuff/programming/mechint/getting_started/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning:\n",
            "\n",
            "CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iC52U60IZpd"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hvpC2mPgIZpd"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dFCGZobRIZpe"
      },
      "outputs": [],
      "source": [
        "# Define the location to save the model, using a relative path\n",
        "PTH_LOCATION = \"workspace/_scratch/local_optimimum_mod_3.pth\"\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(Path(PTH_LOCATION).parent, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M2Nzu7EIZpe"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOckI2-GIZpe"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_2OXlPG-IZpe"
      },
      "outputs": [],
      "source": [
        "p = 3\n",
        "frac_train = 1\n",
        "\n",
        "# Optimizer config\n",
        "lr = 1e-3\n",
        "wd = 1e-2\n",
        "betas = (0.9, 0.999)\n",
        "\n",
        "num_epochs = 20000\n",
        "checkpoint_every = 200\n",
        "\n",
        "DATA_SEED = 598"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT7xY9WsIZpe"
      },
      "source": [
        "## Define Task\n",
        "* Define modular addition\n",
        "* Define the dataset & labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOJMbA26IZpe"
      },
      "source": [
        "Input format:\n",
        "|a|b|=|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HGFlE125IZpe"
      },
      "outputs": [],
      "source": [
        "a_vector = einops.repeat(torch.arange(p), \"i -> (i j)\", j=p)\n",
        "b_vector = einops.repeat(torch.arange(p), \"j -> (i j)\", i=p)\n",
        "equals_vector = einops.repeat(torch.tensor(p), \" -> (i j)\", i=p, j=p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "6JOHNIJDIZpe",
        "outputId": "a35e475f-c388-4568-9c29-228adf72fc44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 0, 3],\n",
            "        [0, 1, 3],\n",
            "        [0, 2, 3],\n",
            "        [1, 0, 3],\n",
            "        [1, 1, 3],\n",
            "        [1, 2, 3],\n",
            "        [2, 0, 3],\n",
            "        [2, 1, 3],\n",
            "        [2, 2, 3]])\n",
            "torch.Size([9, 3])\n"
          ]
        }
      ],
      "source": [
        "dataset = torch.stack([a_vector, b_vector, equals_vector], dim=1).to(device)\n",
        "print(dataset)\n",
        "print(dataset.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "cjUu4BTsIZpf",
        "outputId": "b94f7dbb-e7ad-46fa-addb-f904a37c3ab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([9])\n",
            "tensor([0, 1, 2, 1, 2, 0, 2, 0, 1])\n"
          ]
        }
      ],
      "source": [
        "labels = (dataset[:, 0] + dataset[:, 1]) % p\n",
        "print(labels.shape)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Ad6m8BIZpf"
      },
      "source": [
        "Convert this to a train + test set - 30% in the training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "I8Af2zLZIZpf",
        "outputId": "5075c607-e865-4b8c-90df-d2685d17ebb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 2, 3],\n",
            "        [2, 2, 3],\n",
            "        [1, 0, 3],\n",
            "        [0, 0, 3],\n",
            "        [1, 2, 3],\n",
            "        [0, 1, 3],\n",
            "        [2, 1, 3],\n",
            "        [1, 1, 3],\n",
            "        [2, 0, 3]])\n",
            "tensor([2, 1, 1, 0, 0, 1, 0, 2, 2])\n",
            "torch.Size([9, 3])\n",
            "tensor([], size=(0, 3), dtype=torch.int64)\n",
            "tensor([], dtype=torch.int64)\n",
            "torch.Size([0, 3])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(DATA_SEED)\n",
        "indices = torch.randperm(p*p)\n",
        "cutoff = int(p*p*frac_train)\n",
        "train_indices = indices[:cutoff]\n",
        "test_indices = indices[cutoff:]\n",
        "\n",
        "train_data = dataset[train_indices]\n",
        "train_labels = labels[train_indices]\n",
        "test_data = dataset[test_indices]\n",
        "test_labels = labels[test_indices]\n",
        "print(train_data)\n",
        "print(train_labels)\n",
        "print(train_data.shape)\n",
        "print(test_data[:5])\n",
        "print(test_labels[:5])\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_83i1bUkIZpf"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "UL7gVZ9WIZpf"
      },
      "outputs": [],
      "source": [
        "cfg = HookedTransformerConfig(\n",
        "    n_layers = 1,\n",
        "    n_heads = 2,\n",
        "    d_model = 3,\n",
        "    d_head = 3,\n",
        "    d_mlp = 0,\n",
        "    act_fn = \"relu\",\n",
        "    normalization_type=None,\n",
        "    d_vocab=p+1,\n",
        "    d_vocab_out=p,\n",
        "    n_ctx=3,\n",
        "    init_weights=True,\n",
        "    device=device,\n",
        "    seed = 997,\n",
        ")\n",
        "model = HookedTransformer(cfg)\n",
        "for name, param in model.named_parameters():\n",
        "    if \"b_\" in name:\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H61FscDoIZpg"
      },
      "source": [
        "Disable the biases, as we don't need them for this task and it makes things easier to interpret."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lctx1jAtIZpg"
      },
      "source": [
        "## Define Optimizer + Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "dJORAU_PIZpg",
        "outputId": "1c2b9f25-056e-4e0d-defc-0bd11586136d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.1591, dtype=torch.float64, grad_fn=<NegBackward0>)\n",
            "tensor(nan, dtype=torch.float64, grad_fn=<NegBackward0>)\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)\n",
        "def loss_fn(logits, labels):\n",
        "    if len(logits.shape)==3:\n",
        "        logits = logits[:, -1]\n",
        "    logits = logits.to(torch.float64)\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels[:, None])[:, 0]\n",
        "    return -correct_log_probs.mean()\n",
        "train_logits = model(train_data)\n",
        "train_loss = loss_fn(train_logits, train_labels)\n",
        "print(train_loss)\n",
        "test_logits = model(test_data)\n",
        "test_loss = loss_fn(test_logits, test_labels)\n",
        "print(test_loss)\n",
        "print(\"Uniform loss:\")\n",
        "print(np.log(p))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW9Q4bIcIZpg"
      },
      "source": [
        "## Actually Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj4h2LIRIZpg"
      },
      "source": [
        "**Weird Decision:** Training the model with full batch training rather than stochastic gradient descent. We do this so to make training smoother and reduce the number of slingshots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c477d3a6463646deb7cf1de99bb0199a"
          ]
        },
        "id": "UsZLeCMeIZph",
        "outputId": "133f3502-ca28-490c-c030-f60b751dcd7b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f6e0a3a1ddd4565bf83411aa0c50b20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/20000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 199 Train Loss 8.44302405715638e-11 Test Loss nan\n",
            "Epoch 399 Train Loss 8.439357854013343e-11 Test Loss nan\n",
            "Epoch 599 Train Loss 8.435760731414049e-11 Test Loss nan\n",
            "Epoch 799 Train Loss 8.432935830607325e-11 Test Loss nan\n",
            "Epoch 999 Train Loss 8.430335441567806e-11 Test Loss nan\n",
            "Epoch 1199 Train Loss 8.427670906309164e-11 Test Loss nan\n",
            "Epoch 1399 Train Loss 8.424648632520385e-11 Test Loss nan\n",
            "Epoch 1599 Train Loss 8.423222612724569e-11 Test Loss nan\n",
            "Epoch 1799 Train Loss 8.422830333922623e-11 Test Loss nan\n",
            "Epoch 1999 Train Loss 8.422632960940564e-11 Test Loss nan\n",
            "Epoch 2199 Train Loss 8.422677369861628e-11 Test Loss nan\n",
            "Epoch 2399 Train Loss 8.423244817185363e-11 Test Loss nan\n",
            "Epoch 2599 Train Loss 8.423627227338336e-11 Test Loss nan\n",
            "Epoch 2799 Train Loss 8.424017038978108e-11 Test Loss nan\n",
            "Epoch 2999 Train Loss 8.42381226450924e-11 Test Loss nan\n",
            "Epoch 3199 Train Loss 8.42260828931826e-11 Test Loss nan\n",
            "Epoch 3399 Train Loss 8.422294959709201e-11 Test Loss nan\n",
            "Epoch 3599 Train Loss 8.423032641229982e-11 Test Loss nan\n",
            "Epoch 3799 Train Loss 8.423442190167953e-11 Test Loss nan\n",
            "Epoch 3999 Train Loss 8.423664234772941e-11 Test Loss nan\n",
            "Epoch 4199 Train Loss 8.423691373558076e-11 Test Loss nan\n",
            "Epoch 4399 Train Loss 8.424414252105212e-11 Test Loss nan\n",
            "Epoch 4599 Train Loss 8.425633030269965e-11 Test Loss nan\n",
            "Epoch 4799 Train Loss 8.426444726659054e-11 Test Loss nan\n",
            "Epoch 4999 Train Loss 8.42730576629367e-11 Test Loss nan\n",
            "Epoch 5199 Train Loss 8.429207948409007e-11 Test Loss nan\n",
            "Epoch 5399 Train Loss 8.431110130524398e-11 Test Loss nan\n",
            "Epoch 5599 Train Loss 8.432945699258328e-11 Test Loss nan\n",
            "Epoch 5799 Train Loss 8.435277167609845e-11 Test Loss nan\n",
            "Epoch 5999 Train Loss 8.436671114296215e-11 Test Loss nan\n",
            "Epoch 6199 Train Loss 8.43749267933436e-11 Test Loss nan\n",
            "Epoch 6399 Train Loss 8.439315912256848e-11 Test Loss nan\n",
            "Epoch 6599 Train Loss 8.441329116674639e-11 Test Loss nan\n",
            "Epoch 6799 Train Loss 8.442802012553831e-11 Test Loss nan\n",
            "Epoch 6999 Train Loss 8.444343988976799e-11 Test Loss nan\n",
            "Epoch 7199 Train Loss 8.445602241737984e-11 Test Loss nan\n",
            "Epoch 7399 Train Loss 8.446853093012338e-11 Test Loss nan\n",
            "Epoch 7599 Train Loss 8.447835023598537e-11 Test Loss nan\n",
            "Epoch 7799 Train Loss 8.448735537829579e-11 Test Loss nan\n",
            "Epoch 7999 Train Loss 8.449907439911042e-11 Test Loss nan\n",
            "Epoch 8199 Train Loss 8.451989724872617e-11 Test Loss nan\n",
            "Epoch 8399 Train Loss 8.453852432391524e-11 Test Loss nan\n",
            "Epoch 8599 Train Loss 8.454846698789073e-11 Test Loss nan\n",
            "Epoch 8799 Train Loss 8.455451153546886e-11 Test Loss nan\n",
            "Epoch 8999 Train Loss 8.456640325764307e-11 Test Loss nan\n",
            "Epoch 9199 Train Loss 8.457790023385269e-11 Test Loss nan\n",
            "Epoch 9399 Train Loss 8.458658464506678e-11 Test Loss nan\n",
            "Epoch 9599 Train Loss 8.459475095220303e-11 Test Loss nan\n",
            "Epoch 9799 Train Loss 8.460484164591587e-11 Test Loss nan\n",
            "Epoch 9999 Train Loss 8.461562314506614e-11 Test Loss nan\n",
            "Epoch 10199 Train Loss 8.462455427250908e-11 Test Loss nan\n",
            "Epoch 10399 Train Loss 8.462709544965531e-11 Test Loss nan\n",
            "Epoch 10599 Train Loss 8.463679139740385e-11 Test Loss nan\n",
            "Epoch 10799 Train Loss 8.464258922875509e-11 Test Loss nan\n",
            "Epoch 10999 Train Loss 8.464532777888253e-11 Test Loss nan\n",
            "Epoch 11199 Train Loss 8.466040214039383e-11 Test Loss nan\n",
            "Epoch 11399 Train Loss 8.468768895517487e-11 Test Loss nan\n",
            "Epoch 11599 Train Loss 8.470949866970147e-11 Test Loss nan\n",
            "Epoch 11799 Train Loss 8.473034619093992e-11 Test Loss nan\n",
            "Epoch 11999 Train Loss 8.474171980903674e-11 Test Loss nan\n",
            "Epoch 12199 Train Loss 8.475528920155951e-11 Test Loss nan\n",
            "Epoch 12399 Train Loss 8.476160513698834e-11 Test Loss nan\n",
            "Epoch 12599 Train Loss 8.476690953588411e-11 Test Loss nan\n",
            "Epoch 12799 Train Loss 8.476816778864588e-11 Test Loss nan\n",
            "Epoch 12999 Train Loss 8.477347218754125e-11 Test Loss nan\n",
            "Epoch 13199 Train Loss 8.478417967182273e-11 Test Loss nan\n",
            "Epoch 13399 Train Loss 8.479232130733676e-11 Test Loss nan\n",
            "Epoch 13599 Train Loss 8.479562730478851e-11 Test Loss nan\n",
            "Epoch 13799 Train Loss 8.479752701974237e-11 Test Loss nan\n",
            "Epoch 13999 Train Loss 8.482096506137125e-11 Test Loss nan\n",
            "Epoch 14199 Train Loss 8.484990487487758e-11 Test Loss nan\n",
            "Epoch 14399 Train Loss 8.487425576654944e-11 Test Loss nan\n",
            "Epoch 14599 Train Loss 8.489971688124554e-11 Test Loss nan\n",
            "Epoch 14799 Train Loss 8.491414978056467e-11 Test Loss nan\n",
            "Epoch 14999 Train Loss 8.491817125507612e-11 Test Loss nan\n",
            "Epoch 15199 Train Loss 8.492347565397151e-11 Test Loss nan\n",
            "Epoch 15399 Train Loss 8.493457788421677e-11 Test Loss nan\n",
            "Epoch 15599 Train Loss 8.494237411701144e-11 Test Loss nan\n",
            "Epoch 15799 Train Loss 8.494992363357844e-11 Test Loss nan\n",
            "Epoch 15999 Train Loss 8.495394510809008e-11 Test Loss nan\n",
            "Epoch 16199 Train Loss 8.495438919730089e-11 Test Loss nan\n",
            "Epoch 16399 Train Loss 8.496048308812531e-11 Test Loss nan\n",
            "Epoch 16599 Train Loss 8.496413448829564e-11 Test Loss nan\n",
            "Epoch 16799 Train Loss 8.497281889951049e-11 Test Loss nan\n",
            "Epoch 16999 Train Loss 8.498251484725863e-11 Test Loss nan\n",
            "Epoch 17199 Train Loss 8.499943958047757e-11 Test Loss nan\n",
            "Epoch 17399 Train Loss 8.501429189738395e-11 Test Loss nan\n",
            "Epoch 17599 Train Loss 8.502820669262522e-11 Test Loss nan\n",
            "Epoch 17799 Train Loss 8.50459455893953e-11 Test Loss nan\n",
            "Epoch 17999 Train Loss 8.506188345770317e-11 Test Loss nan\n",
            "Epoch 18199 Train Loss 8.50762916853995e-11 Test Loss nan\n",
            "Epoch 18399 Train Loss 8.508823275081932e-11 Test Loss nan\n",
            "Epoch 18599 Train Loss 8.509622635659619e-11 Test Loss nan\n",
            "Epoch 18799 Train Loss 8.510639106517685e-11 Test Loss nan\n",
            "Epoch 18999 Train Loss 8.511418729797186e-11 Test Loss nan\n",
            "Epoch 19199 Train Loss 8.51160870129256e-11 Test Loss nan\n",
            "Epoch 19399 Train Loss 8.512770734724911e-11 Test Loss nan\n",
            "Epoch 19599 Train Loss 8.513375189482788e-11 Test Loss nan\n",
            "Epoch 19799 Train Loss 8.514260900740173e-11 Test Loss nan\n",
            "Epoch 19999 Train Loss 8.51429544101217e-11 Test Loss nan\n"
          ]
        }
      ],
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "model_checkpoints = []\n",
        "checkpoint_epochs = []\n",
        "if TRAIN_MODEL:\n",
        "    for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "        train_logits = model(train_data)\n",
        "        train_loss = loss_fn(train_logits, train_labels)\n",
        "        train_loss.backward()\n",
        "        train_losses.append(train_loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            test_logits = model(test_data)\n",
        "            test_loss = loss_fn(test_logits, test_labels)\n",
        "            test_losses.append(test_loss.item())\n",
        "\n",
        "        if ((epoch+1)%checkpoint_every)==0:\n",
        "            checkpoint_epochs.append(epoch)\n",
        "            model_checkpoints.append(copy.deepcopy(model.state_dict()))\n",
        "            print(f\"Epoch {epoch} Train Loss {train_loss.item()} Test Loss {test_loss.item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "YhTuoGc5IZph"
      },
      "outputs": [],
      "source": [
        "torch.save(\n",
        "    {\n",
        "        \"model\":model.state_dict(),\n",
        "        \"config\": model.cfg,\n",
        "        \"checkpoints\": model_checkpoints,\n",
        "        \"checkpoint_epochs\": checkpoint_epochs,\n",
        "        \"test_losses\": test_losses,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"train_indices\": train_indices,\n",
        "        \"test_indices\": test_indices,\n",
        "    },\n",
        "    PTH_LOCATION)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "EzRKi7J7IZph"
      },
      "outputs": [],
      "source": [
        "if not TRAIN_MODEL:\n",
        "    cached_data = torch.load(PTH_LOCATION)\n",
        "    model.load_state_dict(cached_data['model'])\n",
        "    model_checkpoints = cached_data[\"checkpoints\"]\n",
        "    checkpoint_epochs = cached_data[\"checkpoint_epochs\"]\n",
        "    test_losses = cached_data['test_losses']\n",
        "    train_losses = cached_data['train_losses']\n",
        "    train_indices = cached_data[\"train_indices\"]\n",
        "    test_indices = cached_data[\"test_indices\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Look at weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Embedding weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.4255,  0.5214,  1.2031],\n",
            "        [ 1.5470,  0.0481, -1.3921],\n",
            "        [-2.2277,  0.0171,  0.2355],\n",
            "        [-0.1093, -1.4294,  1.3221]])\n",
            "torch.Size([4, 3])\n",
            "Token 0: 1.37850821018219\n",
            "Token 1: 2.081744432449341\n",
            "Token 2: 2.240163564682007\n",
            "Token 3: 1.9501159191131592\n",
            "[[ 0.4254886   1.5470358  -2.227684   -0.10925768]\n",
            " [ 0.5213902   0.04806257  0.01714477 -1.4293535 ]\n",
            " [ 1.2030779  -1.3921316   0.23550649  1.3221059 ]]\n",
            "Q:  [[-0.30865872  0.9055637   0.29100567]\n",
            " [-0.3782279   0.16386488 -0.91109383]\n",
            " [-0.8727391  -0.39128345  0.2919311 ]]\n",
            "R:  [[-1.3785081   0.719283    0.47557402 -0.5795088 ]\n",
            " [ 0.          1.9535332  -2.1066506  -0.85047877]\n",
            " [ 0.          0.         -0.5951376   1.6564443 ]]\n",
            "[[ 0.42548853  1.5470358  -2.227685   -0.10925758]\n",
            " [ 0.5213902   0.04806259  0.0171448  -1.4293534 ]\n",
            " [ 1.2030779  -1.3921316   0.2355063   1.3221059 ]]\n",
            "0.7988577 72.71251367741992\n",
            "0.79885757 72.71251367741992\n"
          ]
        }
      ],
      "source": [
        "from scipy.linalg import qr\n",
        "\n",
        "print(model.embed.W_E.data)\n",
        "print(model.embed.W_E.data.shape)\n",
        "\n",
        "tensor = model.embed.W_E.data.cpu().numpy()\n",
        "for i in range(tensor.shape[0]):\n",
        "    print(f\"Token {i}: {np.linalg.norm(tensor[i])}\")\n",
        "\n",
        "# Create a matrix with tensor[0] as the first column\n",
        "ttr = tensor.transpose()\n",
        "print(ttr)\n",
        "\n",
        "# Perform QR decomposition to obtain an orthonormal matrix\n",
        "Q, R = qr(ttr)\n",
        "\n",
        "print(\"Q: \", Q)\n",
        "print(\"R: \", R)\n",
        "print(np.matmul(Q, R))\n",
        "\n",
        "def get_angle(a, b):\n",
        "    return np.arccos(\n",
        "        np.dot(a, b)\n",
        "        / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "    ) / np.pi * 180\n",
        "\n",
        "dot_product = np.dot(tensor[0], tensor[3])\n",
        "print(dot_product, get_angle(tensor[0], tensor[3]))\n",
        "product2 = np.dot(R[:, 0], R[:, 3])\n",
        "print(product2, get_angle(R[:, 0], R[:, 3]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39membed\u001b[38;5;241m.\u001b[39mW_E\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_tensor\u001b[39m(tensor):\n\u001b[1;32m      9\u001b[0m     fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as n\n",
        "\n",
        "%matplotlib widget\n",
        "\n",
        "tensor = model.embed.W_E.data.cpu().numpy()\n",
        "\n",
        "def plot_tensor(tensor):\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Define the origin\n",
        "    origin = np.zeros((4, 3))\n",
        "\n",
        "    # Plot the arrows\n",
        "    for i in range(tensor.shape[0]):\n",
        "        ax.quiver(origin[i, 0], origin[i, 1], origin[i, 2], \n",
        "                tensor[i, 0], tensor[i, 1], tensor[i, 2])\n",
        "        ax.text(tensor[i, 0], tensor[i, 1], tensor[i, 2], f\"Embedding {i}\", color='red')\n",
        "\n",
        "    # Set labels\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "    ax.set_xlim(tensor[:, 0].min(), tensor[:, 0].max())\n",
        "    ax.set_ylim(tensor[:, 1].min(), tensor[:, 1].max())\n",
        "    ax.set_zlim(tensor[:, 2].min(), tensor[:, 2].max())\n",
        "\n",
        "plot_tensor(tensor)\n",
        "plot_tensor(R.transpose())\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8859a5491331dba93123a91c2831400aced845b502848170e05fcb48b2c144be"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
