{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training memorization:\n",
    "\n",
    "Vocabulary: 0, 1, 2 (, ..., N)\n",
    "\n",
    "Output Vocab: 0, 1, 2 (, ..., M)\n",
    "\n",
    "Task: Memorize 00, 01, 02, 10, 11, 12, 20, 21, 22 with random output\n",
    "\n",
    "* Use one attention head to copy the first token to the second, then use one MLP-layer to compute the memorized value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations:\n",
    "* I thought it is about getting all \"positive\" points into one halfspace and the others into the other, but in the case of direct loss,\n",
    "it is the difference between two scalar products that matters. (replacing \"halfspace\" by \"convex set\", possibly \"unbounded convex set\" seems still likely. Investigate shape of points that have positive difference between two vectors. (a,x) - (b, x) = (a-b, x). There, it is still a halfspace, with the vector a-b giving the normal vector of the separating hyperplane.)\n",
    "* With multiple labels, the sets need to be pairwise separated by hyperplane, but there doesn't need to be a \"global\" separating hyperplane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open questions:\n",
    "\n",
    "* What capacity is there, depending on various dimensions? When does the training loss become limited?\n",
    "\n",
    "* Can we visualize my folding theory? (one MLP hidden neuron = 1 fold and stretch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "* Train with target loss, not fixed epoch number"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
